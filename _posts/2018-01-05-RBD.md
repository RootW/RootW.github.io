---
layout: post
title: Rados Block Device
date: 2018-01-05 
tags: ceph
---

&emsp;&emsp;ceph作为流行的SDS(Software Define Storage)开源实现备受业界关注，本篇博文从它提供块服务的视角对ceph进行一步步深入分析。

### 什么是Rados Block Device?

#### **什么是ceph?**

&emsp;&emsp;看一下官方的解释：“Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.” 这里说的是，ceph是一个**统一**分布式存储系统(功能)；另外，它也具有极佳的性能、可靠性和扩展性。功能和DFx兼具，很完美。

&emsp;&emsp;之所以说ceph是一个统一存储，是因为用户从功能上看，会认为ceph同时支持对象、块和文件三种形态(如下图所示)。其中块设备形态，就是Rados Block Device，简称RBD。

<div align="center">
<img src="/images/posts/i440fx/rbd_1.jpg" height="200" width="400">  
</div> 


#### **如何使用Rados Block Device?**

&emsp;&emsp;为了使大家对ceph(RBD)有一个更直观的理解，下面我们来看看ceph(RBD)的实际使用方法：

**1.创建ceph集群**

&emsp;&emsp;ceph是一个分布式系统，内部包含许多计算、存储和网络节点，这里我们暂且把它当做一个黑盒，仅需了解使用ceph之前需要搭建这样一个集群。具体搭建方法我们会在深入分析时介绍。

**2.创建pool**

&emsp;&emsp;pool是ceph中比较重要的一个概念，一个个对象(块、文件最终也转换成对象存储)均放在pool中，管理员针对不同的pool可以采用不同的配置策略。默认情况下，一个ceph集群搭建完成后，就会有一个名为"rbd"的pool，其中专门用来存放RDB对象。我们可以用ceph集群管理命令rados来查询当前已有的pools：

> [root@ceph-client]# **rados lspools**  
> rbd  

&emsp;&emsp;这里我们手动建一个新的pool，然后再查询结果：

> [root@ceph-client]# **rados mkpool** wbpool  
> successfully created pool wbpool  
> [root@ceph-client]# **rados lspools**  
> rbd  
> wbpool  

**3.创建块设备**

&emsp;&emsp;接下来我们使用rbd命令在wbpool池中新建一个1G大小的块设备，取名wb：

> [root@ceph-client]# **rbd create** wb **\-\-pool** wbpool **\-\-size** 1G  
> [root@ceph-client]# **rbd ls \-\-pool** wbpool  
> wb  

**4.映射块设备**

&emsp;&emsp;创建完块设备之后，我们需要在使用RBD的主机上将它映射成一个可使用的设备：

> [root@ceph-client]# **rbd map** wb **\-\-pool** wbpool  
> /dev/rbd0  

**5.使用块设备**

&emsp;&emsp;通过映射，我们获知RBD设备的本地访问设备为/dev/rbd0，那么我们就可以像使用本地块设备一样使用RBD块设备，例如将其格式化成文件系统并挂载使用：
> [root@ceph-client]# **mkfs.ext4** /dev/rbd0  
> [root@ceph-client]# **mount** /dev/rbd0 /mnt  

&emsp;&emsp;



### 为什么需要Rados Block Device?

&emsp;&emsp;ceph兼具对象、块、文件三种存储形态，支持PB级超大存储容量，同时具备较好的性能、可靠性和扩展性，因此非常适合企业级应用存储需求，公有云、私有云都有ceph成功部署的案例。Redhat将其收购后，更是进一步加速了它的应用和推广。

&emsp;&emsp;三种存储形态中，对象和块相对更稳定一些；块设备方式可以完全兼容已有应用，因此使用范围更为广泛。

### 如何实现Rados Block Device?

#### **1. ceph集群结构**

&emsp;&emsp;我们已经知道ceph是一个集群系统，那么从物理视角深入看，集群内部有哪些部件？他们又是如何相互协作对外提供服务的？下图是ceph官方给出的系统部署图：

<div align="center">
<img src="/images/posts/i440fx/rbd_2.jpg" height="250" width="600">  
</div> 

&emsp;&emsp;从图中我们可以看到这些组件(简单地说，可见将组件视为部署了不同ceph程序的服务器)：

>* **Ceph Mon(Monitor)**，集群的管理者，负责维护集群状态图(如monitor map, OSD map, CRUSH map)，并对客户端提供管理服务(客户端通过Monitor获取全局存储信息并建立访问连接)；集群中通常有多个Monitor，以提升系统可靠性
>* **Ceph OSD(Object Storage Daemon)**，对象存储服务提供者，每个OSD提供数据存储空间、处理数据复制、恢复、均衡并通过彼此间的心跳机制为Monitor提供监控信息；一台配置了单独数据硬件的服务器即可视为一个OSD节点；系统中通常也有多个OSD节点
>* **Ceph MDS(MetaData Server)**，为文件服务提供元数据管理功能；可选组件，在RBD中不涉及
>* **Ceph Client(客户端)**，数据访问端点，其上部署了各种软件驱动(或库)，最终为应用提供对象、块、文件服务
>* **Public and Cluster Network**，集群内部网络平面，Public平面提供客户端到集群的数据访问通信，Cluster平面为OSD之间的心跳和数据复制提供通信，采用两个独立的网络平面可以避免相互影响，提升系统整体服务性能

&emsp;&emsp;通过组件的协作，正常的数据流大体如下：
>* 客户端通过驱动首先与Monitor建立网络连接，并完成认证，最后Monitor返回系统中所有的OSD状态图
>* 客户端基于OSD状态图、访问对象和CRUSH算法计算出存放对象的主备OSD
>* 客户端与主OSD建立连接并发起数据访问请求
>* 主OSD执行客户端访问请求，对于写请求会将请求复制到其它备OSD，待其它OSD返回结果后再对客户端返回执行结果

#### **2. ceph功能结构**

&emsp;&emsp;切换到功能视角，我们也可以看到ceph整体的软件栈架构：底部是由Monitor、MDS、OSD实现的Rados集群功能；客户端通过librados库为应用提供多语言的对象访问支持，或者通过RADOSGW实现兼客S3和Swift的对象服务，或者通过librbd为应用(如虚拟化程序)提供块服务，或者通过文件驱动为应用提供兼容posix标准的文件服务。注，RADOSGW和librbd基于librados构建，但内核块服务和文件服务直接在内核中实现，其中涵盖了librados的功能。

<div align="center">
<img src="/images/posts/i440fx/rbd_3.jpg" height="400" width="600">  
</div> 


#### **3. 客户端内核rbd驱动分析**

&emsp;&emsp;下面我们通过分析客户端内核驱动，而将服务端(Monitor，OSD)当作黑盒来理解整个ceph系统原理。如果按软件栈分层，linux内核中的rbd驱动分为上下两层：上层是在内核块层中的rbd驱动(rbd.ko)；下层是在内核网络层中的libceph驱动(libceph.ko)。rbd.ko为客户端提供可访问的块设备，libceph.ko为rbd.ko提供网络服务接口。

##### **3.1 rbd块设备创建**

&emsp;&emsp;回顾开篇介绍rbd块设备的使用方法，分为创建、映射和使用三步。创建过程主要是用户态rbd工具与rados集群通信完成的，不涉及内核rbd驱动；但问题是rados集群中的OSD提供的是对象服务(我们可以把对象简单理解成key/value对)，如何来表达一个rbd块设备？

&emsp;&emsp;我们通过rados ls命令来查看一下wbpool中创建了一个名为wb的rbd设备后都生成了哪些对象：

> [root@ceph-client]# **rados ls \-\-pool** wbpool  
> rbd_header.371e643c9869  
> rbd_directory    
> rbd_object_map.371e643c9869  
> rbd_id.wb  
> rbd_info  

&emsp;&emsp;这些对象里到底包含了哪些信息呢？我们可以用rados get命令获取对象内容到文件中并打开文件进行查看。先看看rbd_id.wb对象内容：

> [root@ceph-client]# **rados get** rbd_id.wb ./result.txt **\-\-pool** wbpool; **hexdump -Cv** ./result.txt  
> 00000000  0c 00 00 00 33 37 31 65  36 34 33 63 39 38 36 39 |....371e643c9869|  
> 00000010  

&emsp;&emsp;从文件内容看，rbd_id.wb中保存的主要是个id信息，为"371e643c9869"。知道了wb块设备的id，我们再看来看看rbd_head.371e643c9869对象中的内容：

> [root@ceph-client]# rados get rbd_header.371e643c9869 ./result.txt --pool wbpool; hexdump -Cv ./result.txt  
>  

&emsp;&emsp;rbd_header.371e643c9869对象内容为空，怎么回事呢？我们再来看看该对象相关的key/value map值(可以用来描述对象的元数据信息)：

> [root@ceph-client]# **rados listomapvals** rbd_header.371e643c9869 **\-\-pool** wbpool  
> create_timestamp  
> value (8 bytes) :  
> 00000000  63 38 26 5a 84 c1 71 08 |c8&Z..q.|  
> 00000008  
>  
> features  
> value (8 bytes) :  
> 00000000  01 00 00 00 00 00 00 00 |........|  
> 00000008  
>  
> flags  
> value (8 bytes) :  
> 00000000  00 00 00 00 00 00 00 00 |........|  
> 00000008  
>  
> object_prefix  
> value (25 bytes) :  
> 00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 33 37 31 |....rbd_data.371|  
> 00000010  65 36 34 33 63 39 38 36  39 |e643c9869|  
> 00000019  
> 
> order 
> value (1 bytes) :  
> 00000000  16                                                |.|  
> 00000001  
>  
> size  
> value (8 bytes) :  
> 00000000  00 00 00 40 00 00 00 00 |...@....|  
> 00000008  
>  
> snap_seq  
> value (8 bytes) :  
> 00000000  00 00 00 00 00 00 00 00 |........|  
> 00000008  

&emsp;&emsp;这样我们看到rbd_header.371e643c9869对象的map中保存与rbd相关的元数据信息，如rbd数据存放对象前缀为"rbd_data.371e643c9869"；order为0x16，表示以4M单位来划分rbd块设备，每4M对应一个对象，对象名为"rbd_data.371e643c9869.偏移"；size为0x40000000，即1G。

&emsp;&emsp;最后我们来看看另外两个与具体rbd设备无关的对象rbd_info和rbd_directory：

> [root@ceph-client]# **rados get** rbd_info ./result.txt **\-\-pool** wbpool; **hexdump -Cv** ./result.txt  
> 00000000  6f 76 65 72 77 72 69 74  65 20 76 61 6c 69 64 61 |overwrite valida|  
> 00000010  74 65 64                                          |ted|  
> 00000013  
>  
> [root@ceph-client]# **rados get** rbd_directory ./result.txt **\-\-pool** wbpool; **hexdump -Cv** ./result.txt  
> [root@ceph-client]# **rados listomapvals** rbd_directory **\-\-pool** wbpool  
> id_371e643c9869  
> value (6 bytes) :  
> 00000000  02 00 00 00 77 62                                 |....wb|  
> 00000006  
>  
> name_wb  
> value (16 bytes) :  
> 00000000  0c 00 00 00 33 37 31 65  36 34 33 63 39 38 36 39 |....371e643c9869|  
> 00000010  

&emsp;&emsp;从结果我们看出rbd_info中只是保存了一个提示字符串。rbd_directory对象的map中保存了所有rbd块设备的名字和id的对应关系，可实现双向查找。

##### **3.2 rbd块设备映射流程分析**

##### **3.3 rbd块设备IO流程分析**

#### **4. ceph Monitor原理分析**

#### **5. ceph OSD原理分析**


<br>
转载请注明：[吴斌的博客](https://rootw.github.io) » [Rados Block Device](https://rootw.github.io/2018/01/RBD/) 
