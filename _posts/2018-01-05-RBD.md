---
layout: post
title: Rados Block Device之一－概述
date: 2018-01-05 
tags: ceph
---

&emsp;&emsp;ceph作为流行的SDS(Software Define Storage)开源实现备受业界关注，从本篇博文开始我们将从它提供的块服务(Rados Block Device)视角对ceph进行一步步深入分析。

### 什么是Rados Block Device?

#### **什么是ceph?**

&emsp;&emsp;看一下官方的解释："Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability." 这里说的是，ceph是一个**统一**分布式存储系统(功能)；另外，它也具有极佳的性能、可靠性和扩展性。功能和DFx兼具，很完美。

&emsp;&emsp;之所以说ceph是一个统一存储，是因为用户从功能上看，会认为ceph同时支持对象、块和文件三种形态(如下图所示)。其中块设备形态，就是Rados Block Device，简称RBD。

<div align="center">
<img src="/images/posts/i440fx/rbd_1.jpg" height="200" width="400">  
</div> 


#### **如何使用Rados Block Device?**

&emsp;&emsp;为了使大家对ceph(RBD)有一个更直观的理解，下面我们来看看ceph(RBD)的实际使用方法：

**1.创建ceph集群**

&emsp;&emsp;ceph是一个分布式系统，内部包含许多计算、存储和网络节点，这里我们暂且把它当做一个黑盒，仅需了解使用ceph之前需要搭建这样一个集群。具体搭建方法我们会在深入分析时介绍。

**2.创建pool**

&emsp;&emsp;pool是ceph中比较重要的一个概念，一个个对象(块、文件最终也转换成对象存储)均放在pool中，管理员针对不同的pool可以采用不同的配置策略。默认情况下，一个ceph集群搭建完成后，就会有一个名为"rbd"的pool，其中专门用来存放RDB对象。我们可以用ceph集群管理命令rados来查询当前已有的pools：

> [root@ceph-client]# **rados lspools**  
> rbd  

&emsp;&emsp;这里我们手动建一个新的pool，然后再查询结果：

> [root@ceph-client]# **rados mkpool** wbpool  
> successfully created pool wbpool  
> [root@ceph-client]# **rados lspools**  
> rbd  
> wbpool  

**3.创建块设备**

&emsp;&emsp;接下来我们使用rbd命令在wbpool池中新建一个1G大小的块设备，取名wb：

> [root@ceph-client]# **rbd create** wb **\-\-pool** wbpool **\-\-size** 1G  
> [root@ceph-client]# **rbd ls \-\-pool** wbpool  
> wb  

**4.映射块设备**

&emsp;&emsp;创建完块设备之后，我们需要在使用RBD的主机上将它映射成一个可使用的设备：

> [root@ceph-client]# **rbd map** wb **\-\-pool** wbpool  
> /dev/rbd0  

**5.使用块设备**

&emsp;&emsp;通过映射，我们获知RBD设备的本地访问设备为/dev/rbd0，那么我们就可以像使用本地块设备一样使用RBD块设备，例如将其格式化成文件系统并挂载使用：
> [root@ceph-client]# **mkfs.ext4** /dev/rbd0  
> [root@ceph-client]# **mount** /dev/rbd0 /mnt  

&emsp;&emsp;



### 为什么需要Rados Block Device?

&emsp;&emsp;ceph兼具对象、块、文件三种存储形态，支持PB级超大存储容量，同时具备较好的性能、可靠性和扩展性，因此非常适合企业级应用存储需求，公有云、私有云都有ceph成功部署的案例。Redhat将其收购后，更是进一步加速了它的应用和推广。

&emsp;&emsp;三种存储形态中，对象和块相对更稳定一些；块设备方式可以完全兼容已有应用，因此使用范围更为广泛。

### 如何实现Rados Block Device?

#### **1. ceph集群结构**

&emsp;&emsp;我们已经知道ceph是一个集群系统，那么从物理视角深入看，集群内部有哪些部件？他们又是如何相互协作对外提供服务的？下图是ceph官方给出的系统部署图：

<div align="center">
<img src="/images/posts/i440fx/rbd_2.jpg" height="250" width="600">  
</div> 

&emsp;&emsp;从图中我们可以看到这些组件(简单地说，可见将组件视为部署了不同ceph程序的服务器)：

>* **Ceph Mon(Monitor)**，集群的管理者，负责维护集群状态图(如monitor map, OSD map, CRUSH map)，并对客户端提供管理服务(客户端通过Monitor获取全局存储信息并建立访问连接)；集群中通常有多个Monitor，以提升系统可靠性
>* **Ceph OSD(Object Storage Daemon)**，对象存储服务提供者，每个OSD提供数据存储空间、处理数据复制、恢复、均衡并通过彼此间的心跳机制为Monitor提供监控信息；一台配置了单独数据硬盘的服务器即可视为一个OSD节点；系统中通常也有多个OSD节点
>* **Ceph MDS(MetaData Server)**，为文件服务提供元数据管理功能；可选组件，在RBD中不涉及
>* **Ceph Client(客户端)**，数据访问端点，其上部署了各种软件驱动(或库)，最终为应用提供对象、块、文件服务
>* **Public and Cluster Network**，集群内部网络平面，Public平面提供客户端到集群的数据访问通信，Cluster平面为OSD之间的心跳和数据复制提供通信，采用两个独立的网络平面可以避免相互影响，提升系统整体服务性能

&emsp;&emsp;通过组件的协作，正常的数据流大体如下：
>* 客户端通过驱动首先与Monitor建立网络连接，并完成认证，最后Monitor返回系统中所有的OSD状态图
>* 客户端基于OSD状态图、访问对象和CRUSH算法计算出存放对象的主备OSD
>* 客户端与主OSD建立连接并发起数据访问请求
>* 主OSD执行客户端访问请求，对于写请求会将请求复制到其它备OSD，待其它OSD返回结果后再对客户端返回执行结果

#### **2. ceph功能结构**

&emsp;&emsp;切换到功能视角，我们也可以看到ceph整体的软件栈架构：底部是由Monitor、MDS、OSD实现的Rados集群功能；客户端通过librados库为应用提供多语言的对象访问支持，或者通过RADOSGW实现兼客S3和Swift的对象服务，或者通过librbd为应用(如虚拟化程序)提供块服务，或者通过文件驱动为应用提供兼容posix标准的文件服务。注，RADOSGW和librbd基于librados构建，但内核块服务和文件服务直接在内核中实现，其中涵盖了librados的功能。

<div align="center">
<img src="/images/posts/i440fx/rbd_3.jpg" height="400" width="600">  
</div> 


#### **3. 客户端内核RBD驱动分析**
    
&emsp;&emsp;参见[Rados Block Device之二－客户端内核RBD驱动分析](https://rootw.github.io/2018/01/RBD-client/)

#### **4. ceph Monitor原理分析**

#### **5. ceph OSD原理分析**


<br>
转载请注明：[吴斌的博客](https://rootw.github.io) » [ Rados Block Device之一－概述](https://rootw.github.io/2018/01/RBD/) 
