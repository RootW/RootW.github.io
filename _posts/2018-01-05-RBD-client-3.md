---
layout: post
title: 【Rados Block Device】内核RBD驱动分析－设备IO流程
date: 2018-01-05 
tags: ceph
---

&emsp;&emsp;内核RBD驱动整体软件栈如下：

<div align="center">
<img src="/images/posts/i440fx/rbd_4.jpg" height="400" width="600">  
</div> 

### 3. RBD块设备IO流程分析

&emsp;&emsp;我们针对IO流程同样沿着从上至下的顺序进行深入分析，对于和上节重复部分我们将不再展开，重点讨论有差异的部分。 IO流程可分为请求下发和响应返回两个阶段。

#### **3.1. IO下发：rbd_request_fn**

```
linux/drivers/block/rbd.c:

/*当应用向RBD块设备下发读写请求时，最终会调用该函数；
  q代表请求所在的请求队列*/
static void rbd_request_fn(struct request_queue *q)
__releases(q->queue_lock) __acquires(q->queue_lock)
{
    struct rbd_device *rbd_dev = q->queuedata; /*队列私有数据，初始化时指定，代表rbd_dev对象*/
    struct request *rq;
    int result;

    /*通过一个大循环，不断调用blk_fetch_request从请求队列中取出待处理的请求rq*/
    while ((rq = blk_fetch_request(q))) {
        bool write_request = rq_data_dir(rq) == WRITE; /*判断请求读写类型*/
        struct rbd_img_request *img_request;
        u64 offset;
        u64 length;

        ...
        offset = (u64) blk_rq_pos(rq) << SECTOR_SHIFT; /*计算请求起始位置，从扇区转换成字节*/
        length = (u64) blk_rq_bytes(rq); /*获取请求大小*/
        ...

        result = -ENOMEM;
        /*针对每个rq，这里会生成一个img_request与之对应；其内部会记录请求起始位置、长度和读写类型等信息*/
        img_request = rbd_img_request_create(rbd_dev, offset, length, write_request);
        if (!img_request)
            goto end_request;
        
        /*绑定img_request和rq之间的关系*/
        img_request->rq = rq;

        /*向img_request中填入bio信息，下面将展开分析*/
        result = rbd_img_request_fill(img_request, OBJ_REQUEST_BIO, rq->bio);
        /*将img_request递交给底层libceph过行网络发送，下面将展开分析*/
        if (!result)
            result = rbd_img_request_submit(img_request);
        ...
    }
}
```

##### **3.1.1. rbd_img_request_fill**

&emsp;&emsp;由于每个RBD设备会以4M为粒度划分成一个个对象，所以对每个RBD设备的请求(image request)会转换成若干对象请求(object request)。rbd_img_request_fill的主要作用就是完成image request到object request的转换：

```
linux/drivers/block/rbd.c:

static int rbd_img_request_fill(struct rbd_img_request *img_request,
                        enum obj_request_type type,void *data_desc)
{
    struct rbd_device *rbd_dev = img_request->rbd_dev;
    struct rbd_obj_request *obj_request = NULL;
    struct rbd_obj_request *next_obj_request;
    bool write_request = img_request_write_test(img_request); /*是否为写请求*/
    struct bio *bio_list = 0;
    unsigned int bio_offset = 0;
    struct page **pages = 0;
    u64 img_offset;
    u64 resid;
    u16 opcode;

    
    opcode = write_request ? CEPH_OSD_OP_WRITE : CEPH_OSD_OP_READ;
    img_offset = img_request->offset; /*image request对应的请求偏移*/
    resid = img_request->length; /*image request对应的请求长度*/
    rbd_assert(resid > 0);

    if (type == OBJ_REQUEST_BIO) {/*这里主要考虑bio类型的请求*/
        bio_list = data_desc;
        rbd_assert(img_offset == bio_list->bi_sector << SECTOR_SHIFT);
    } else {
        ...
    }

    while (resid) {
        struct ceph_osd_request *osd_req;
        const char *object_name;
        u64 offset;
        u64 length;

        /*根据image request偏移位置计算对象名称*/
        object_name = rbd_segment_name(rbd_dev, img_offset);
        ...
        /*4M对象内的偏移*/
        offset = rbd_segment_offset(rbd_dev, img_offset);
        /*4M对象内的长度*/
        length = rbd_segment_length(rbd_dev, img_offset, resid);
        /*新建一个object request*/
        obj_request = rbd_obj_request_create(object_name, offset, length, type);
        /* object request has its own copy of the object name */
        rbd_segment_name_free(object_name);
        if (!obj_request)
            goto out_unwind;
        /*
         * set obj_request->img_request before creating the
         * osd_request so that it gets the right snapc
         */
        rbd_img_obj_request_add(img_request, obj_request);

        if (type == OBJ_REQUEST_BIO) {
            unsigned int clone_size;

            rbd_assert(length <= (u64)UINT_MAX);
            clone_size = (unsigned int)length;
            /*克隆object request对应的bio段*/
            obj_request->bio_list =
                bio_chain_clone_range(&bio_list,
                                    &bio_offset,
                                    clone_size,
                                    GFP_ATOMIC);
            ...
        } else {
            ...
        }

        /*针对object request创建底层的osd request*/
        osd_req = rbd_osd_req_create(rbd_dev, write_request, obj_request);
        ...
        obj_request->osd_req = osd_req;
        obj_request->callback = rbd_img_obj_callback; /*请求处理完成时的回调*/
        rbd_img_request_get(img_request);

        /*将object request的bio信息填入到osd request的extent中*/
        osd_req_op_extent_init(osd_req, 0, opcode, offset, length, 0, 0);
        if (type == OBJ_REQUEST_BIO)
            osd_req_op_extent_osd_data_bio(osd_req, 0, obj_request->bio_list, length);
        else
            ...

        if (write_request)
            rbd_osd_req_format_write(obj_request);
        else
            rbd_osd_req_format_read(obj_request);

        obj_request->img_offset = img_offset;

        /*更新偏移和长度，并继续循环*/
        img_offset += length; 
        resid -= length;
    }

    return 0;

    ...
}
```

##### **3.1.2. rbd_img_request_submit**

&emsp;&emsp;完成object request对象的成生后，rbd_image_request_submit调用rbd_obj_request_submit，最终来到libceph中的ceph_osdc_start_request，它的主要作用就是根据访问对象、OSDMap和CRUSH算法计算出对象所在PG对应的主OSD节点，并将该object request对应的osd request通过网络发送给该OSD处理：

```
linux/net/ceph/osd_client.c:

int ceph_osdc_start_request(struct ceph_osd_client *osdc,
            struct ceph_osd_request *req, bool nofail)
{
    int rc = 0;

    down_read(&osdc->map_sem);
    mutex_lock(&osdc->request_mutex);
    /*将该osd请求添加到osdc中*/
    __register_request(osdc, req);
    req->r_sent = 0;
    req->r_got_reply = 0;
    /*通过CRUSH算法及OSDMap映射当前osd请求到OSD节点，有关CRUSH的原理可以参考相关论文*/
    rc = __map_request(osdc, req, 0);
    if (rc < 0) {
        ...
    }
    if (req->r_osd == NULL) {
        dout("send_request %p no up osds in pg\n", req);
        ceph_monc_request_next_osdmap(&osdc->client->monc);
    } else {
        /*将该请求发送给OSD节点*/
        __send_queued(osdc);
    }
    rc = 0;
out_unlock:
    mutex_unlock(&osdc->request_mutex);
    up_read(&osdc->map_sem);
    return rc;
}
```

#### **3.2. IO返回**

&emsp;&emsp;IO请求通过网络发送给OSD节点后，OSD节点在处理完成后会回复响应，响应的处理逻辑在客户端创建OSD对象时指定：

```
linux/net/ceph/osd_client.c:

static struct ceph_osd *create_osd(struct ceph_osd_client *osdc, int onum)
{
    struct ceph_osd *osd;

    osd = kzalloc(sizeof(*osd), GFP_NOFS);
    if (!osd)
        return NULL;

    atomic_set(&osd->o_ref, 1);
    osd->o_osdc = osdc;
    osd->o_osd = onum;
    RB_CLEAR_NODE(&osd->o_node);
    INIT_LIST_HEAD(&osd->o_requests);
    INIT_LIST_HEAD(&osd->o_linger_requests);
    INIT_LIST_HEAD(&osd->o_osd_lru);
    osd->o_incarnation = 1;

    ceph_con_init(&osd->o_con, osd, &osd_con_ops, &osdc->client->msgr);

    INIT_LIST_HEAD(&osd->o_keepalive_item);
    return osd;
}

static const struct ceph_connection_operations osd_con_ops = {
    .get = get_osd_con,
    .put = put_osd_con,
    .dispatch = dispatch, /*消息处理逻辑*/
    .get_authorizer = get_authorizer,
    .verify_authorizer_reply = verify_authorizer_reply,
    .invalidate_authorizer = invalidate_authorizer,
    .alloc_msg = alloc_msg,
    .fault = osd_reset,
};

static void dispatch(struct ceph_connection *con, struct ceph_msg *msg)
{
    struct ceph_osd *osd = con->private;
    struct ceph_osd_client *osdc;
    int type = le16_to_cpu(msg->hdr.type);

    if (!osd)
        goto out;
    osdc = osd->o_osdc;

    switch (type) {
    case CEPH_MSG_OSD_MAP:
        ceph_osdc_handle_map(osdc, msg);
        break;
    case CEPH_MSG_OSD_OPREPLY:
        handle_reply(osdc, msg, con);
        break;
    case CEPH_MSG_WATCH_NOTIFY:
        handle_watch_notify(osdc, msg);
        break;

    default:
        pr_err("received unknown message type %d %s\n", type,
        ceph_msg_type_name(type));
    }
out:
    ceph_msg_put(msg);
}
```

&emsp;&emsp;在handle_reply中会从osd request到object request，再到image request，层层调用回调并处理已经结束的IO请求。

<br>
转载请注明：[吴斌的博客](https://rootw.github.io) » [【Rados Block Device】内核RBD驱动分析－设备IO流程](https://rootw.github.io/2018/01/RBD-client-3/) 
