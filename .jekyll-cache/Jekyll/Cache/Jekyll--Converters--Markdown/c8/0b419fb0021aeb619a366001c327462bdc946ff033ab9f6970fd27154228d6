I"$<p>  随着越来越多公有云服务提供商采用SPDK技术作为其高性能云存储的核心技术之一，intel推出的SPDK技术备受业界关注。本篇博文就和大家一起探索SPDK。</p>

<h3 id="什么是spdk为什么需要它">什么是SPDK？为什么需要它？</h3>

<p>  SPDK(全称Storage Performance Development Kit)，提供了一整套工具和库，以实现高性能、扩展性强、全用户态的存储应用程序。它是继DPDK之后，intel在存储领域推出的又一项颠覆性技术，旨在大幅缩减存储IO栈的软件开销，从而提升存储性能，可以说它就是为了存储性能而生。</p>

<p>  为便于大家理解，我们先介绍一下SPDK在虚拟化场景下的使用方法，以给大家一些直观的认识。</p>

<h4 id="1-dpdk的编译与安装"><strong>1. DPDK的编译与安装</strong></h4>

<p>  SPDK使用了DPDK中一些通用的功能和机制，因此首先需要下载DPDK的源码并完成编译和安装：</p>

<blockquote>
  <p>[root@linux:~/DPDK]# <strong>make config T=x86_64-native-linuxapp-gcc</strong><br />
[root@linux:~/DPDK]# <strong>make</strong><br />
[root@linux:~/DPDK]# <strong>make install</strong> (默认安装到/usr/local，包括.a库文件和头文件)</p>
</blockquote>

<h4 id="2-spdk的编译"><strong>2. SPDK的编译</strong></h4>

<blockquote>
  <p>[root@linux:~/SPDK]# <strong>./configure –with-dpdk=</strong>/usr/local<br />
[root@linux:~/SPDK]# <strong>make</strong></p>
</blockquote>

<p>  编译成功后，我们在spdk/app/vhost目录下可以看到一个名为vhost的可执行文件，它就是SPDK在虚拟化场景下为虚拟机模拟程序qemu提供的存储转发服务，借此为虚拟机用户带来高性能的虚拟磁盘。</p>

<h4 id="3-大页内存配置"><strong>3. 大页内存配置</strong></h4>

<p>  SPDK vhost进程和qemu进程通过大页共享虚拟机可见内存，因此需要进行一些大页的配置和调整：</p>

<blockquote>
  <ul>
    <li>可通过设置/sys/kernel/mm/hugepages/hugepages-xxx/nr_hugepages来调整大页数量(xxx通常为2M或1G)</li>
    <li>qemu使用挂载到/dev/hugepages目录下的hugetlbfs来使用大页内存，可在挂载参数中指定大页大小，如mount -t hugetlbfs -o pagesize=1G nodev /dev/hugepages</li>
  </ul>
</blockquote>

<h4 id="4-vhost配置与启动"><strong>4. vhost配置与启动</strong></h4>

<blockquote>
  <p>[root@linux:~/SPDK]# <strong>HUGEMEM=</strong>4096 <strong>scripts/setup.sh</strong><br />
[root@linux:~/SPDK]# <strong>app/vhost/vhost -S</strong> /var/tmp <strong>-m</strong> 0x3 <strong>-c</strong> etc/spdk/rootw.conf</p>
</blockquote>

<p>  vhost命令执行过程中，是一个常驻的服务进程；-S参数指定了socket文件的生成的目录，每个虚拟磁盘(vhost-blk)或虚拟存储控制器(vhost-scsi)都会在该目录下产生一个socket文件，以便qemu程序与vhost进程建立连接；-m参数指定了vhost进程中的轮循线程所绑定的物理CPU核，例如0x3代表在0号和1号核上各绑定一个轮循线程；-c参数指定了vhost进程所需的配置文件，例如这里我通过内存设备(SPDK中称之为Malloc设备)提供了一个vhost-blk磁盘：</p>

<blockquote>
  <p>[root@linux:~/SPDK]# <strong>cat</strong> etc/spdk/rootw.conf <br />
[root@linux:~/SPDK]#  <br />
<strong>[Malloc]</strong>  <br />
NumberOfLuns 1   #创建一个内存设备，默认名称为Malloc0  <br />
LunSizeInMB 128  #该内存设备大小为128M  <br />
BlockSize 4096   #该内存设备块大小为4096字节  <br />
<strong>[VhostBlk0]</strong>  <br />
Name vhost.2     #创建一个vhost-blk设备，名称为vhost.2  <br />
Dev Malloc0      #该设备后端对应的物理设备为Malloc0  <br />
Cpumask 0x1      #将该设备绑定到0号核的轮循线程上</p>
</blockquote>

<h4 id="5-虚拟机启动与验证"><strong>5. 虚拟机启动与验证</strong></h4>

<p>  vhost进程启动后，我们就可以拉起qemu进程来启动一个新虚拟机，qemu进程的命令行参数如下(重点关注与SPDK vhost相关部分)：</p>

<blockquote>
  <p>[root@linux:~/qemu]# <strong>./x86_64-softmmu/qemu-system-x86_64</strong> -name rootw-vm -machine pc-i440fx-2.6,accel=kvm \  <br />
<strong>-m 1G -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem</strong> \  <br />
-drive file=/mnt/centos.qcow2,format=qcow2,id=virtio-disk0,cache=none,aio=native -device virtio-blk-pci,drive=virtio-disk0,id=blk0 \  <br />
<strong>-chardev socket,id=char_rootw,path=/var/tmp/vhost.2 -device vhost-user-blk-pci,id=blk_rootw,chardev=char_rootw</strong> \  <br />
-vnc 0.0.0.0:0</p>
</blockquote>

<p>  通过上述启动参数，我们可以看出：</p>
<blockquote>
  <ul>
    <li>vhost进程和qemu进程通过大页方式共享虚拟机可见的所有内存(原因我们将在深入分析时讨论)</li>
    <li>qemu在配置vhost-user-blk-pci设备时，只需要指定vhost生成的socket文件即可(-S参数指定的路径后拼接上设备名称)</li>
  </ul>
</blockquote>

<p>  虚拟机启动成功后，我们通过vnc工具登陆虚拟机，执行lsblk命令可以查看到vda和vdb两个virtio-blk块设备，表明vhost后端已成功生效。这里要说明一下，qemu中配置的virtio-blk-pci设备、vhost-user-blk-pci设备或vhost-blk-pci设备，在虚拟机内部均呈现为virtio-blk-pci设备，因此在虚拟机中采用相同的virtio-blk-pci和virtio-blk驱动进行使能，如此一来不同的后端实现技术在虚拟机内部均采用一套驱动，可以减少驱动的开发和维护工作量。</p>

<h3 id="如何实现spdk">如何实现SPDK?</h3>

<p>  SPDK能实现高性能，得益于以下三个关键技术：</p>

<blockquote>
  <ul>
    <li><strong>全用户态</strong>，它把所有必要的驱动全部移到了用户态，避免了系统调用的开销并真正实现内存零拷贝</li>
    <li><strong>轮循模式</strong>，针对高速物理存储设备，采用轮循的方式而非中断通知方式判断请求完成，大大降低时延并减少性能波动</li>
    <li><strong>无锁机制</strong>，在IO路径上避免采用任何锁机制进行同步，降低时延并提升吞吐量</li>
  </ul>
</blockquote>

<p>  下面我们将深入到SPDK的实现细节，去看看这些关键点分别是如何提升性能的。</p>

<h4 id="1-整体架构"><strong>1. 整体架构</strong></h4>

<p>  首先，我们来了解一下SPDK内部的整体组件架构：</p>

<div align="center">
<img src="/images/posts/spdk/arch.png" height="300" width="550" />  
</div>

<p>  SPDK整体分为三层：</p>

<blockquote>
  <ul>
    <li>存储协议层(Storage Protocols)，指SPDK支持存储应用类型。iSCSI Target对外提供iSCSI服务，用户可以将运行SPDK服务的主机当前标准的iSCSI存储设备来使用；vhost-scsi或vhost-blk对qemu提供后端存储服务，qemu可以基于SPDK提供的后端存储为虚拟机挂载virtio-scsi或virtio-blk磁盘；NVMF对外提供基于NVMe协议的存储服务端。注意，图中vhost-blk在spdk-18.04版本中已实现，后面我们主要基于此版本进行代码分析。</li>
    <li>存储服务层(Storage Services)，该层实现了对块和文件的抽象。目前来说，SPDK主要在块层实现了QoS特性，这一层整体上还是非常薄的。</li>
    <li>驱动层(drivers)，这一层实现了存储服务层定义的抽象接口，以对接不同的存储类型，如NVMe，RBD，virtio，aio等等。图中把驱动细分成两层，和块设备强相关的放到了存储服务层，而把和硬件强相关部分放到了驱动层。</li>
  </ul>
</blockquote>

<h4 id="2-深入数据面"><strong>2. 深入数据面</strong></h4>

<p>  接下来我们将以SPDK前端配置成vhost-blk、后端配置成NVMe SSD场景为例，来分析整个数据面流程。我们将分两部分完成数据面的分析：</p>

<ul>
  <li><a href="https://rootw.github.io/2018/05/SPDK-iostack/">IO栈对比与线程模型</a></li>
  <li><a href="https://rootw.github.io/2018/05/SPDK-ioanalyze/">IO流程代码解析</a></li>
</ul>

<h4 id="3-深入管理面"><strong>3. 深入管理面</strong></h4>

<p>  管理面流程比数据面要复杂得多，也无趣得多。因此我们在分析完数据面流程之后，再回头看看数据面中涉及的各个对象分别是如何被创建和初始化的，这样更利于我们理解这样做的目的，也不会一下子就被这些复杂的流程吓住而无法坚持往下分析。</p>

<p>  整个管理面功能包含vhost启动初始化和通过rpc动态管理两个部分，这里我们主要讨化启动初始化，根据启动时的先后顺序，分为</p>

<ul>
  <li><a href="https://rootw.github.io/2018/05/SPDK-reactors-init/">reactor线程初始化</a></li>
  <li><a href="https://rootw.github.io/2018/05/SPDK-subsys-bdev/">bdev子系统初始化</a></li>
  <li><a href="https://rootw.github.io/2018/05/SPDK-subsys-vhost/">vhost子系统初始化</a></li>
  <li><a href="https://rootw.github.io/2018/05/SPDK-vhost-msg-handle">vhost客户端(qemu)连接请求处理</a></li>
</ul>

<p><br />
转载请注明：<a href="https://rootw.github.io">吴斌的博客</a> » <a href="https://rootw.github.io/2018/05/SPDK-all/">【SPDK】一、概述</a></p>
:ET