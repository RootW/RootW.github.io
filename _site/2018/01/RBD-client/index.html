<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Rados Block Device之二－客户端内核RBD驱动分析</title>
  <meta name="description" content="  下面我们通过分析客户端内核驱动，而将服务端(Monitor，OSD)当作黑盒来理解整个ceph系统原理。如果按软件栈分层，linux内核中的RBD驱动涉及上中下两层：上层是在内核块层中的RBD驱动(rbd.ko)；中层是在内核网络层中的libceph驱动(libceph.ko)；下层内核网络socket通信。...">
  <meta name="author" content="leopardpan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Rados Block Device之二－客户端内核RBD驱动分析">
  <meta name="twitter:description" content="  下面我们通过分析客户端内核驱动，而将服务端(Monitor，OSD)当作黑盒来理解整个ceph系统原理。如果按软件栈分层，linux内核中的RBD驱动涉及上中下两层：上层是在内核块层中的RBD驱动(rbd.ko)；中层是在内核网络层中的libceph驱动(libceph.ko)；下层内核网络socket通信。...">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Rados Block Device之二－客户端内核RBD驱动分析">
  <meta property="og:description" content="  下面我们通过分析客户端内核驱动，而将服务端(Monitor，OSD)当作黑盒来理解整个ceph系统原理。如果按软件栈分层，linux内核中的RBD驱动涉及上中下两层：上层是在内核块层中的RBD驱动(rbd.ko)；中层是在内核网络层中的libceph驱动(libceph.ko)；下层内核网络socket通信。...">
  
  <link rel="icon" type="image/png" href="/images/favicon.png" />
  <link href="/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/2018/01/RBD-client/">
  <link rel="alternate" type="application/rss+xml" title="吴斌" href="http://localhost:4000/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

<!-- 站点统计 -->
  <script 
  async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>  

<!-- 百度统计 -->
  

<!-- google 统计 -->
  

</head>


  <body>

    <span class="mobile btn-mobile-menu">        
      <div class="nav_container">
         <nav class="nav-menu-item" style = "float:right">
            <i class="nav-menu-item">
              <a href="/#blog" title="" class="blog-button">  博客主页
              </a>
            </i>
            
                <i class="nav-menu-item">

                  <a href="/archive" title="archive" class="btn-mobile-menu__icon">
                      所有文章
                  </a>
                </i>
            
                <i class="nav-menu-item">

                  <a href="/tags" title="tags" class="btn-mobile-menu__icon">
                      标签
                  </a>
                </i>
            
                <i class="nav-menu-item">

                  <a href="/about" title="about" class="btn-mobile-menu__icon">
                      关于我
                  </a>
                </i>
            
          </nav>
      </div>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">
        <!-- 头像效果-start -->
        <div class="ih-item circle effect right_to_left">            
            <a href="/#blog" title="前往 吴斌 的主页" class="blog-button">
                <div class="img"><img src="/images/avatar.jpg" alt="img"></div>
                <div class="info">
                    <div class="info-back">
                        <h2> 
                            
                                DigDeeply
                            
                        </h2>
                        <p>
                           
                                系统编程<br>内核/虚拟化
                            
                        </p>
                    </div>
                </div>
            </a>
        </div>
        <!-- 头像效果-end -->
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for 吴斌" class="blog-button">吴斌</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">个人主页</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">欢迎来到我的技术博客~</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        

        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">博客主页</a></li>
                
                  <li class="navigation__item"><a href="/archive" title="archive">所有文章</a></li>
                
                  <li class="navigation__item"><a href="/tags" title="tags">标签</a></li>
                
                  <li class="navigation__item"><a href="/about" title="about">关于我</a></li>
                
              </ul>
            </nav>
          </div>          
        </div>


        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-clear"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title">Rados Block Device之二－客户端内核RBD驱动分析</h1>
    <div class="post-meta">
      <img src="/images/calendar.png" width="20px"/> 
      <time datetime="2018-01-05 00:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2018-01-05</time>  
         
      <span id="busuanzi_container_page_pv"> | 阅读：<span id="busuanzi_value_page_pv"></span>次</span>
    </p>
    </div>
  </header>

  <section class="post">
    <p>  下面我们通过分析客户端内核驱动，而将服务端(Monitor，OSD)当作黑盒来理解整个ceph系统原理。如果按软件栈分层，linux内核中的RBD驱动涉及上中下两层：上层是在内核块层中的RBD驱动(rbd.ko)；中层是在内核网络层中的libceph驱动(libceph.ko)；下层内核网络socket通信。rbd.ko为客户端提供可访问的块设备，libceph.ko为rbd.ko提供对象存储接口，socket则提供最底层的网络通信，软件栈及其核心对象概览如下：</p>

<div align="center">
<img src="/images/posts/i440fx/rbd_4.jpg" height="400" width="600" />  
</div>

<h3 id="1-rbd块设备创建">1. RBD块设备创建</h3>

<p>  回顾开篇介绍RBD块设备的使用方法，分为创建、映射和使用三步。创建过程主要是用户态rbd工具与rados集群通信完成的，不涉及内核RBD驱动；但问题是rados集群中的OSD提供的是对象服务(我们可以把对象简单理解成key/value对)，如何来表达一个RBD块设备？</p>

<p>  我们通过rados ls命令来查看一下wbpool中创建了一个名为wb的RBD设备后都生成了哪些对象：</p>

<blockquote>
  <p>[root@ceph-client]# <strong>rados ls --pool</strong> wbpool<br />
rbd_header.371e643c9869<br />
rbd_directory  <br />
rbd_object_map.371e643c9869<br />
rbd_id.wb<br />
rbd_info</p>
</blockquote>

<p>  这些对象里到底包含了哪些信息呢？我们可以用rados get命令获取对象内容到文件中并打开文件进行查看。先看看rbd_id.wb对象内容：</p>

<blockquote>
  <p>[root@ceph-client]# <strong>rados get</strong> rbd_id.wb ./result.txt <strong>--pool</strong> wbpool; <strong>hexdump -Cv</strong> ./result.txt<br />
00000000  0c 00 00 00 33 37 31 65  36 34 33 63 39 38 36 39 |….371e643c9869|<br />
00000010</p>
</blockquote>

<p>  从文件内容看，rbd_id.wb中保存的主要是个id信息，为”371e643c9869”。知道了wb块设备的id，我们再看来看看rbd_head.371e643c9869对象中的内容：</p>

<blockquote>
  <p>[root@ceph-client]# rados get rbd_header.371e643c9869 ./result.txt –pool wbpool; hexdump -Cv ./result.txt</p>

</blockquote>

<p>  rbd_header.371e643c9869对象内容为空，怎么回事呢？我们再来看看该对象相关的key/value map值(可以用来描述对象的元数据信息)：</p>

<blockquote>
  <p>[root@ceph-client]# <strong>rados listomapvals</strong> rbd_header.371e643c9869 <strong>--pool</strong> wbpool<br />
create_timestamp<br />
value (8 bytes) :<br />
00000000  63 38 26 5a 84 c1 71 08 |c8&amp;Z..q.|<br />
00000008</p>

  <p>features<br />
value (8 bytes) :<br />
00000000  01 00 00 00 00 00 00 00 |……..|<br />
00000008</p>

  <p>flags<br />
value (8 bytes) :<br />
00000000  00 00 00 00 00 00 00 00 |……..|<br />
00000008</p>

  <p>object_prefix<br />
value (25 bytes) :<br />
00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 33 37 31 |….rbd_data.371|<br />
00000010  65 36 34 33 63 39 38 36  39 |e643c9869|<br />
00000019</p>

  <p>order 
value (1 bytes) :<br />
00000000  16                                                |.|<br />
00000001</p>

  <p>size<br />
value (8 bytes) :<br />
00000000  00 00 00 40 00 00 00 00 |…@….|<br />
00000008</p>

  <p>snap_seq<br />
value (8 bytes) :<br />
00000000  00 00 00 00 00 00 00 00 |……..|<br />
00000008</p>
</blockquote>

<p>  这样我们看到rbd_header.371e643c9869对象的map中保存与RBD相关的元数据信息，如RBD数据存放对象前缀为”rbd_data.371e643c9869”；order为0x16，表示以4M单位来划分RBD块设备，每4M对应一个对象，对象名为”rbd_data.371e643c9869.偏移”；size为0x40000000，即1G。</p>

<p>  最后我们来看看另外两个与具体rbd设备无关的对象rbd_info和rbd_directory：</p>

<blockquote>
  <p>[root@ceph-client]# <strong>rados get</strong> rbd_info ./result.txt <strong>--pool</strong> wbpool; <strong>hexdump -Cv</strong> ./result.txt<br />
00000000  6f 76 65 72 77 72 69 74  65 20 76 61 6c 69 64 61 |overwrite valida|<br />
00000010  74 65 64                                          |ted|<br />
00000013</p>

  <p>[root@ceph-client]# <strong>rados get</strong> rbd_directory ./result.txt <strong>--pool</strong> wbpool; <strong>hexdump -Cv</strong> ./result.txt<br />
[root@ceph-client]# <strong>rados listomapvals</strong> rbd_directory <strong>--pool</strong> wbpool<br />
id_371e643c9869<br />
value (6 bytes) :<br />
00000000  02 00 00 00 77 62                                 |….wb|<br />
00000006</p>

  <p>name_wb<br />
value (16 bytes) :<br />
00000000  0c 00 00 00 33 37 31 65  36 34 33 63 39 38 36 39 |….371e643c9869|<br />
00000010</p>
</blockquote>

<p>  从结果我们看出rbd_info中只是保存了一个提示字符串”overwirte validated”。rbd_directory对象的map中保存了所有RBD块设备的名字和id的对应关系，可实现双向查找。</p>

<p>  综上所述，ceph中也是利用了对象存储的方式来保存RBD信息：”rbd_id.块设备名”对象中保存对应块设备的id；”rbd_header.块设备id”的omap中保存了块设备的各种元数据信息(这里没有直接保存的对象内容中，通过omap更易解析)；”rbd_data.块设备id.偏移”对象用来保存块设备对应偏移处的数据；rbd_directory用来保存pool中所有的RBD设备的名称和id。</p>

<h3 id="2-rbd块设备映射流程分析">2. RBD块设备映射流程分析</h3>

<p>  rbd工具通过map子命令映射产生一个可使用的内核块设备，其本质原理是由rbd工具往sysfs(“/sys/bus/rbd/add”)内核文件接口中写入待创建块设备的信息(例如前文实例写入的内容为”9.22.115.154:6789 name=amdin,key=client, wbpool wb -“，其中9.22.115.154为主monitor的IP)，内核在处理这个写入请求时会调用由RBD驱动事先注册好的处理函数来生成一个内核块设备对象。</p>

<p>  我们先来看看rbd模块加载初始化时完成了哪些初始化动作：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/drivers/block/rbd.c:

static int __init rbd_init(void)
{
    int rc;

    /*首先检查rbd驱动依赖的底层libceph驱动的兼容性*/
    if (!libceph_compatible(NULL)) {
        rbd_warn(NULL, "libceph incompatibility (quitting)");

        return -EINVAL;
    }

    /*初始化rbd驱动中使用的内存分配器*/
    rc = rbd_slab_init();
    if (rc)
        return rc;

    /*sysfs文件接口初始化，为用户态rbd工具暴露访问接口*/
    rc = rbd_sysfs_init();
    if (rc)
        rbd_slab_exit();
    else
    pr_info("loaded " RBD_DRV_NAME_LONG "\n");

    return rc;
}

module_init(rbd_init);
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/drivers/block/rbd.c:

/*
 * create control files in sysfs
 * /sys/bus/rbd/...
 */
static int rbd_sysfs_init(void)
{
    int ret;

    ret = device_register(&amp;rbd_root_dev);
    if (ret &lt; 0)
        return ret;

    /*根据rbd_bus_type定义的信息在/sys/bus/下生成子目录*/
    ret = bus_register(&amp;rbd_bus_type);
    if (ret &lt; 0)
        device_unregister(&amp;rbd_root_dev);

    return ret;
}

static struct bus_attribute rbd_bus_attrs[] = {
    __ATTR(add, S_IWUSR, NULL, rbd_add), /*对rbd目录下的add文件进行写操作时将调用rbd_add*/
    __ATTR(remove, S_IWUSR, NULL, rbd_remove), /*对rbd目录下的remove文件进行写操作时将调用rbd_remove*/
    __ATTR_NULL
};

static struct bus_type rbd_bus_type = {
    .name		= "rbd", /* /sys/bus/成生的子目录名 */
    .bus_attrs	= rbd_bus_attrs,
};

</code></pre>
</div>

<p>  下面我们顺着本文开头给出的软件栈从上至下(rbd-&gt;libceph)深入分析rbd_add函数，看看内核RBD驱动是如何进行块设备的生成的：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/drivers/block/rbd.c:

/*用户态工具往/sys/bus/rbd/add中写入RBD设备信息，内核在处理该写入请求时就会
  调用这里的rbd_add，其中bus就是/sys/bus/rbd对象，buf中存放用户态工具写入的
  字符串，如"9.22.115.154:6789 name=amdin,key=client, wbpool wb -"，
  count为写入的字符串长度*/

static ssize_t rbd_add(struct bus_type *bus,
                const char *buf, size_t count)
{
    struct rbd_device *rbd_dev = NULL;
    struct ceph_options *ceph_opts = NULL;
    struct rbd_options *rbd_opts = NULL;
    struct rbd_spec *spec = NULL;
    struct rbd_client *rbdc;
    struct ceph_osd_client *osdc;
    bool read_only;
    int rc = -ENOMEM;

    ...

    /*首先对用户态写入的字符串进行解析，结果返回到ceph_opts，rbd_opt和spec中：
      (1)ceph_opts中保存ceph集群相关信息，针对前文示例，ceph_opts.mon_addr中将保存
         "9.22.115.154:6789"；ceph_opts.name为"admin"，代表访问ceph集群的角色名称；
         ceph_opts.key对应系统配置中的"ceph.client.admin.keyring"，为登陆密钥。
      (2)rbd_opt中包含当前rbd设备是否只读，针对前文示例，rbd_opt.read_only为false。
      (3)spec中包含当前rbd设备的详细描述信息，针对示例，spec.pool_name为"wb_pool"，
         spec.image_name为"wb"，spec.snap_name为"-"，表示无快照*/

    /* parse add command */
    rc = rbd_add_parse_args(buf, &amp;ceph_opts, &amp;rbd_opts, &amp;spec);
    if (rc &lt; 0)
        goto err_out_module;
    read_only = rbd_opts-&gt;read_only;
    kfree(rbd_opts);
    rbd_opts = NULL;	/* done with this */

    /*接着根据ceph集群配置ceph_opts在当前客户端查找是否已生成rbd_client(对应一个ceph_client，
      内部包含mon_client和osd_client)；如果未找到则生成一个新的rbd_client，并建立会话(完成认证、获
      取monmap和osdmap等一系统初始化动作)*/
    rbdc = rbd_get_client(ceph_opts);
    if (IS_ERR(rbdc)) {
        rc = PTR_ERR(rbdc);
        goto err_out_args;
    }

    /*从ceph集群返回的信息中获取当前pool对应的id*/
    /* pick the pool */
    osdc = &amp;rbdc-&gt;client-&gt;osdc;
    rc = ceph_pg_poolid_by_name(osdc-&gt;osdmap, spec-&gt;pool_name);
    if (rc &lt; 0)
        goto err_out_client;
    spec-&gt;pool_id = (u64)rc;

    ...

    /*创建内核中的rbd_dev对象*/
    rbd_dev = rbd_dev_create(rbdc, spec);
    if (!rbd_dev)
        goto err_out_client;
    rbdc = NULL;		/* rbd_dev now owns this */
    spec = NULL;		/* rbd_dev now owns this */

    /*从ceph集群中查询当前rbd设备(image)的元数据信息，即从rbd_id.[rbd设备名]中获知image id，并从
      rbd_header.[image id]的omap中获得设备大小，分片大小等信息*/
    rc = rbd_dev_image_probe(rbd_dev, true);
    if (rc &lt; 0)
        goto err_out_rbd_dev;

    ...

    /*根据ceph集群返回的rbd信息，完成对rbd_dev的设置，并通过内核块层接口生成新的块设备对象*/
    rc = rbd_dev_device_setup(rbd_dev);
    if (rc) {
        rbd_dev_image_release(rbd_dev);
        goto err_out_module;
    }

    return count;

    ...
}

</code></pre>
</div>

<h4 id="21-rbd_get_client"><strong>2.1. rbd_get_client</strong></h4>

<p>  针对同一个rados集群，每个客户端节点都会在内核中生成一个rbd_client对象，用来和该rados集群进行会话；在一个客户端节点上，如果针对同一个rados集群创建了多个RBD设备，那么这些RBD设备会共用同一个rbd_client。下面我们看看rbd_get_client的内部实现：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/drivers/block/rbd.c:

/*
 * Get a ceph client with specific addr and configuration, if one does
 * not exist create it.  Either way, ceph_opts is consumed by this
 * function.
 */
static struct rbd_client *rbd_get_client(struct ceph_options *ceph_opts)
{
    struct rbd_client *rbdc;

    rbdc = rbd_client_find(ceph_opts); /*在全局链表rbd_client_list中根据ceph_opts的集群信息查找rbd_client*/
    if (rbdc)	/* using an existing client */
        ceph_destroy_options(ceph_opts); /*如果找到相同的rbd_client(对应同一个rados集群)，则复用该对象，同时销毁ceph_opts*/
    else
        rbdc = rbd_client_create(ceph_opts); /*如果没有找到相同的rbd_client，则新建一个并添加到rbd_client_list中*/

    return rbdc;
}
</code></pre>
</div>

<p>  接着看rbd_client_create：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/drivers/block/rbd.c:

/*rbd_client代表rbd客户端实例，多个rbd设备可共用一个客户端实例；其内部主要包含一个ceph_client对象*/
/*
 * an instance of the client.  multiple devices may share an rbd client.
 */
struct rbd_client {
    struct ceph_client	*client;
    struct kref		kref;
    struct list_head	node;
};

/*
 * Initialize an rbd client instance.  Success or not, this function
 * consumes ceph_opts.
 */
static struct rbd_client *rbd_client_create(struct ceph_options *ceph_opts)
{
    struct rbd_client *rbdc;
    int ret = -ENOMEM;

    rbdc = kmalloc(sizeof(struct rbd_client), GFP_KERNEL);
    ...

    /*调用libceph.ko中的函数创建ceph_client对象*/
    rbdc-&gt;client = ceph_create_client(ceph_opts, rbdc, 0, 0);
    ...

    /*调用libceph.ko中的函数打开ceph_client对象的会话，基于该会话可以和rados集群(monitor or OSD)进行通信*/
    ret = ceph_open_session(rbdc-&gt;client);
    ...

    spin_lock(&amp;rbd_client_list_lock);
    list_add_tail(&amp;rbdc-&gt;node, &amp;rbd_client_list);
    spin_unlock(&amp;rbd_client_list_lock);

    ...

    return rbdc;

    ...
}
</code></pre>
</div>

<p>  下面我们再深入一层，看看libceph中ceph_client的相关实现，但不会深入到messenger模块中(复杂性较高)，我们在2.1.3节会总结一下对messenger模块的使用方法(API)。对于messenger的分析我们将放到本篇博文最后一节。</p>

<h5 id="211-ceph_create_client"><strong>2.1.1. ceph_create_client</strong></h5>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/include/linux/ceph/libceph.h:

/*
 * per client state
 *
 * possibly shared by multiple mount points, if they are
 * mounting the same ceph filesystem/cluster.
 */
struct ceph_client {
    ...

    /*每个ceph_client包含一个messenger实例、一个mon_client实例和一个osd_client实例：
      (1)messenger实例描述了与网络通信相关的信息，属于messenger模块，是ceph客户端基于socket封装的网络服务层；
      (2)mon_client实例是专门用来与monitor通信的客户端；
      (3)osd_client实例是专门用来与所有osd通信的客户端。*/
    struct ceph_messenger msgr;   /* messenger instance */
    struct ceph_mon_client monc;
    struct ceph_osd_client osdc;

    ...
};
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/ceph_common.c:

/*ceph_create_client创建ceph_client对象并对其内部的messenger、mon_client、osd_client进行初始化*/
/*
 * create a fresh client instance
 */
struct ceph_client *ceph_create_client(struct ceph_options *opt, void *private,
        unsigned int supported_features, unsigned int required_features)
{
    struct ceph_client *client;
    struct ceph_entity_addr *myaddr = NULL;
    int err = -ENOMEM;

    client = kzalloc(sizeof(*client), GFP_KERNEL);
    if (client == NULL)
        return ERR_PTR(-ENOMEM);

    client-&gt;private = private;
    client-&gt;options = opt;

    ...

    /* msgr */
    if (ceph_test_opt(client, MYIP))
        myaddr = &amp;client-&gt;options-&gt;my_addr;
    ceph_messenger_init(&amp;client-&gt;msgr, myaddr, client-&gt;supported_features,
        client-&gt;required_features, ceph_test_opt(client, NOCRC));

    /* subsystems */
    err = ceph_monc_init(&amp;client-&gt;monc, client);
    if (err &lt; 0)
        goto fail;
    err = ceph_osdc_init(&amp;client-&gt;osdc, client);
    if (err &lt; 0)
        goto fail_monc;

    return client;
    ...
}
</code></pre>
</div>

<p>  接着我们再深入看看messenger、mon_client、osd_client的初始化：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/include/linux/ceph/messenger.h:

struct ceph_messenger {
    struct ceph_entity_inst inst;    /* my name+address */
    struct ceph_entity_addr my_enc_addr;

    atomic_t stopping;
    bool nocrc;

    /*
     * the global_seq counts connections i (attempt to) initiate
     * in order to disambiguate certain connect race conditions.
     */
    u32 global_seq;
    spinlock_t global_seq_lock;

    u32 supported_features;
    u32 required_features;
};
</code></pre>
</div>
<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/messenger.c:

/*
 * initialize a new messenger instance
 */
void ceph_messenger_init(struct ceph_messenger *msgr,
            struct ceph_entity_addr *myaddr,
            u32 supported_features,
            u32 required_features,
            bool nocrc)
{
    msgr-&gt;supported_features = supported_features;
    msgr-&gt;required_features = required_features;

    spin_lock_init(&amp;msgr-&gt;global_seq_lock);

    if (myaddr)
        msgr-&gt;inst.addr = *myaddr;

    /* select a random nonce */
    msgr-&gt;inst.addr.type = 0;
    get_random_bytes(&amp;msgr-&gt;inst.addr.nonce, sizeof(msgr-&gt;inst.addr.nonce));
    encode_my_addr(msgr);
    msgr-&gt;nocrc = nocrc;

    atomic_set(&amp;msgr-&gt;stopping, 0);

    dout("%s %p\n", __func__, msgr);
}
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/include/linux/ceph/mon_client.h:

struct ceph_mon_client {
    struct ceph_client *client;
    struct ceph_monmap *monmap;

    struct mutex mutex;
    struct delayed_work delayed_work;

    struct ceph_auth_client *auth;
    struct ceph_msg *m_auth, *m_auth_reply, *m_subscribe, *m_subscribe_ack;
    int pending_auth;

    bool hunting;
    int cur_mon;                       /* last monitor i contacted */
    unsigned long sub_sent, sub_renew_after;
    struct ceph_connection con;

    /* pending generic requests */
    struct rb_root generic_request_tree;
    int num_generic_requests;
    u64 last_tid;

    /* mds/osd map */
    int want_mdsmap;
    int want_next_osdmap; /* 1 = want, 2 = want+asked */
    u32 have_osdmap, have_mdsmap;
};

</code></pre>
</div>
<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/mon_client.c:

int ceph_monc_init(struct ceph_mon_client *monc, struct ceph_client *cl)
{
    int err = 0;

    dout("init\n");
    memset(monc, 0, sizeof(*monc));
    monc-&gt;client = cl;
    monc-&gt;monmap = NULL;
    mutex_init(&amp;monc-&gt;mutex);

    err = build_initial_monmap(monc);
    if (err)
        goto out;

    /* connection */
    /* authentication */
    monc-&gt;auth = ceph_auth_init(cl-&gt;options-&gt;name, cl-&gt;options-&gt;key);
    ...
    monc-&gt;auth-&gt;want_keys =
        CEPH_ENTITY_TYPE_AUTH | CEPH_ENTITY_TYPE_MON |
        CEPH_ENTITY_TYPE_OSD | CEPH_ENTITY_TYPE_MDS;

    /* msgs */
    err = -ENOMEM;
    monc-&gt;m_subscribe_ack = ceph_msg_new(CEPH_MSG_MON_SUBSCRIBE_ACK,
        sizeof(struct ceph_mon_subscribe_ack), GFP_NOFS, true);
    ...
    monc-&gt;m_subscribe = ceph_msg_new(CEPH_MSG_MON_SUBSCRIBE, 96, GFP_NOFS, true);
    ...
    monc-&gt;m_auth_reply = ceph_msg_new(CEPH_MSG_AUTH_REPLY, 4096, GFP_NOFS, true);
    ...
    monc-&gt;m_auth = ceph_msg_new(CEPH_MSG_AUTH, 4096, GFP_NOFS, true);
    monc-&gt;pending_auth = 0;
    ...

    /*mon_client通过底层messenger模块中的connection对象进行网络通信，这里对mon_client使用的connection进行
      初始化，并指定其网络层回调函数集为mon_con_ops，用来处理消息回调、网络连接故障等*/
    ceph_con_init(&amp;monc-&gt;con, monc, &amp;mon_con_ops, &amp;monc-&gt;client-&gt;msgr);

    monc-&gt;cur_mon = -1;
    monc-&gt;hunting = true;
    monc-&gt;sub_renew_after = jiffies;
    monc-&gt;sub_sent = 0;

    INIT_DELAYED_WORK(&amp;monc-&gt;delayed_work, delayed_work);
    monc-&gt;generic_request_tree = RB_ROOT;
    monc-&gt;num_generic_requests = 0;
    monc-&gt;last_tid = 0;

    monc-&gt;have_mdsmap = 0;
    monc-&gt;have_osdmap = 0;
    monc-&gt;want_next_osdmap = 1;
    return 0;

    ...
}
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/include/linux/ceph/osd_client.h:

struct ceph_osd_client {
    struct ceph_client     *client;

    struct ceph_osdmap     *osdmap;       /* current map */
    struct rw_semaphore    map_sem;
    struct completion      map_waiters;
    u64                    last_requested_map;

    struct mutex           request_mutex;
    struct rb_root         osds;          /* osds */
    struct list_head       osd_lru;       /* idle osds */
    u64                    timeout_tid;   /* tid of timeout triggering rq */
    u64                    last_tid;      /* tid of last request */
    struct rb_root         requests;      /* pending requests */
    struct list_head       req_lru;	      /* in-flight lru */
    struct list_head       req_unsent;    /* unsent/need-resend queue */
    struct list_head       req_notarget;  /* map to no osd */
    struct list_head       req_linger;    /* lingering requests */
    int                    num_requests;
    struct delayed_work    timeout_work;
    struct delayed_work    osds_timeout_work;

    mempool_t              *req_mempool;

    struct ceph_msgpool	msgpool_op;
    struct ceph_msgpool	msgpool_op_reply;

    spinlock_t		event_lock;
    struct rb_root		event_tree;
    u64			event_count;

    struct workqueue_struct	*notify_wq;
};
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/osd_client.c:

int ceph_osdc_init(struct ceph_osd_client *osdc, struct ceph_client *client)
{
    int err;

    dout("init\n");
    osdc-&gt;client = client;
    osdc-&gt;osdmap = NULL;
    init_rwsem(&amp;osdc-&gt;map_sem);
    init_completion(&amp;osdc-&gt;map_waiters);
    osdc-&gt;last_requested_map = 0;
    mutex_init(&amp;osdc-&gt;request_mutex);
    osdc-&gt;last_tid = 0;
    osdc-&gt;osds = RB_ROOT;
    INIT_LIST_HEAD(&amp;osdc-&gt;osd_lru);
    osdc-&gt;requests = RB_ROOT;
    INIT_LIST_HEAD(&amp;osdc-&gt;req_lru);
    INIT_LIST_HEAD(&amp;osdc-&gt;req_unsent);
    INIT_LIST_HEAD(&amp;osdc-&gt;req_notarget);
    INIT_LIST_HEAD(&amp;osdc-&gt;req_linger);
    osdc-&gt;num_requests = 0;
    INIT_DELAYED_WORK(&amp;osdc-&gt;timeout_work, handle_timeout);
    INIT_DELAYED_WORK(&amp;osdc-&gt;osds_timeout_work, handle_osds_timeout);
    spin_lock_init(&amp;osdc-&gt;event_lock);
    osdc-&gt;event_tree = RB_ROOT;
    osdc-&gt;event_count = 0;

    schedule_delayed_work(&amp;osdc-&gt;osds_timeout_work,
        round_jiffies_relative(osdc-&gt;client-&gt;options-&gt;osd_idle_ttl * HZ));

    err = -ENOMEM;
    osdc-&gt;req_mempool = mempool_create_kmalloc_pool(10, sizeof(struct ceph_osd_request));
    if (!osdc-&gt;req_mempool)
        goto out;

    err = ceph_msgpool_init(&amp;osdc-&gt;msgpool_op, CEPH_MSG_OSD_OP,
            OSD_OP_FRONT_LEN, 10, true, "osd_op");
    if (err &lt; 0)
        goto out_mempool;
    err = ceph_msgpool_init(&amp;osdc-&gt;msgpool_op_reply, CEPH_MSG_OSD_OPREPLY,
        OSD_OPREPLY_FRONT_LEN, 10, true, "osd_op_reply");
    if (err &lt; 0)
        goto out_msgpool;

    err = -ENOMEM;
    osdc-&gt;notify_wq = create_singlethread_workqueue("ceph-watch-notify");
    if (!osdc-&gt;notify_wq)
        goto out_msgpool;
    return 0;

out_msgpool:
    ceph_msgpool_destroy(&amp;osdc-&gt;msgpool_op);
out_mempool:
    mempool_destroy(osdc-&gt;req_mempool);
out:
    return err;
}
</code></pre>
</div>

<h5 id="212-ceph_open_session"><strong>2.1.2. ceph_open_session</strong></h5>

<p>  在完成ceph_client的初始化动作后，下一步是打开会话：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/ceph_common.c:

int ceph_open_session(struct ceph_client *client)
{
    ...

    ret = __ceph_open_session(client, started);

    ...
    return ret;
}

/*
 * mount: join the ceph cluster, and open root directory.
 */
int __ceph_open_session(struct ceph_client *client, unsigned long started)
{
    int err;
    /*timeout表示打开会话的超时时间，默认为60秒*/
    unsigned long timeout = client-&gt;options-&gt;mount_timeout * HZ;

    /*ceph_client内部是通过mon_client来打开会话，如果mon_client成功打开会话，
      它会成功获得rados集群的mon_map(描述所有monitors信息)和osd_map(描述所有的osd信息)*/
    /* open session, and wait for mon and osd maps */
    err = ceph_monc_open_session(&amp;client-&gt;monc);
    if (err &lt; 0)
        return err;

    while (!have_mon_and_osd_map(client)) { /*如果没有获得mon_map和osd_map，则等待直到超时*/
        err = -EIO;
        if (timeout &amp;&amp; time_after_eq(jiffies, started + timeout)) /*超时退出*/
            return err;

        /* wait */
        dout("mount waiting for mon_map\n");
        /*下面，当前进程(rbd工具)将进入睡眠状态，直到内核接收到mon_map和osd_map时被重新唤醒，
          或者出现认证错时也将被唤醒*/
        err = wait_event_interruptible_timeout(client-&gt;auth_wq,
            have_mon_and_osd_map(client) || (client-&gt;auth_err &lt; 0),
            timeout);
        if (err == -EINTR || err == -ERESTARTSYS)
            return err;
        if (client-&gt;auth_err &lt; 0)
            return client-&gt;auth_err;
    }

    return 0;
}
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/mon_client.c:

int ceph_monc_open_session(struct ceph_mon_client *monc)
{
    mutex_lock(&amp;monc-&gt;mutex);
    __open_session(monc);
    __schedule_delayed(monc);
    mutex_unlock(&amp;monc-&gt;mutex);
    return 0;
}

/*
 * Open a session with a (new) monitor.
 */
static int __open_session(struct ceph_mon_client *monc)
{
    char r;
    int ret;

    if (monc-&gt;cur_mon &lt; 0) {/*初始时cur_mon为－1，表示没有和任何monitor建立连接*/

        /*通过随机数r，从初始化时生成的mon_map中随机选一个进行会话连接的monitor*/
        get_random_bytes(&amp;r, 1);
        monc-&gt;cur_mon = r % monc-&gt;monmap-&gt;num_mon;
        monc-&gt;sub_sent = 0;
        monc-&gt;sub_renew_after = jiffies;  /* i.e., expired */
        monc-&gt;want_next_osdmap = !!monc-&gt;want_next_osdmap;

        /*与选定的monitor建立网络连接，这里使用的messenger模块提供的接口*/
        ceph_con_open(&amp;monc-&gt;con,
            CEPH_ENTITY_TYPE_MON, monc-&gt;cur_mon,
            &amp;monc-&gt;monmap-&gt;mon_inst[monc-&gt;cur_mon].addr);

        /*初始化发送给monitor的首个hello消息*/
        /* initiatiate authentication handshake */
        ret = ceph_auth_build_hello(monc-&gt;auth,
                monc-&gt;m_auth-&gt;front.iov_base,
                monc-&gt;m_auth-&gt;front_alloc_len);

        /*这里通过messenger模块的ceph_con_send接口将消息发送给monitor*/
        __send_prepared_auth_request(monc, ret);
    } else {
        dout("open_session mon%d already open\n", monc-&gt;cur_mon);
    }
    return 0;
}
</code></pre>
</div>

<h5 id="213-messenger模块使用方法小结"><strong>2.1.3. messenger模块使用方法小结</strong></h5>

<p>  通过前文对mon_client的分析，我们来总结一下mon_client是如何使用底层的messenger模块的：</p>

<blockquote>
  <ul>
    <li>(1)通过ceph_messenger_init在ceph_client中初始化一个messenger对象，定义全局通信信息；</li>
    <li>(2)ceph_con_init(&amp;monc-&gt;con, monc, &amp;mon_con_ops, &amp;monc-&gt;client-&gt;msgr)，网络连接对象初始化，并指明该连接收到消息时的回调处理函数(消息将在内核工作队列上下文被处理)；</li>
    <li>(3)使用ceph_msg_new进行消息的内存分配；</li>
    <li>(4)ceph_con_open打开与选定monitor的网络连接；</li>
    <li>(5)填入消息内容；</li>
    <li>(6)使用ceph_con_send进行消息发送</li>
  </ul>
</blockquote>

<h5 id="214-打开会话时客户端与monitor的交互消息处理回调mon_con_ops分析"><strong>2.1.4. 打开会话时客户端与monitor的交互(消息处理回调mon_con_ops分析)</strong></h5>

<p>  目前monitor对我们来说还是一个黑盒，因此无法分析其内部是如何处理客户端发送的hello消息的。那么我们如何才能获知客户端是如何与monitor进行交互的？这里我们可以打开内核动态日志开关(代码中有很多dout语句)，并通过分析日志来获知整个交互过程。通过打开libceph的messenger模块日志，我们可以得到如下图所示的消息交互过程：</p>

<div align="center">
<img src="/images/posts/i440fx/rbd_5.jpg" height="550" width="400" />  
</div>

<p>  从上图可见，TCP连接建立后的前几次消息交互主要是messenger模块对连接进行初始化，我们在将在深入分析messenger模块时分析。ceph connection初始化完成后，客户端首先便是向monitor发送Auth(hello)认证消息，关于认证的实现细节我们这里暂不深入分析，通过日志可知，整个认证会有三次Auth消息的交互：第一次Auth_reply返回前，monitor会向客户端返回monmap；第三次Auth_reply返回后，表示认证成功。认证成功之后客户端会向monitor发送Subscribe订阅消息，表示关注osdmap的更新。由于是首次认证，monitor会向client返回初始的osdmap，后续只有在osdmap有更新时，monitor才回返回更新的map。</p>

<p>  下面我们打开mon_con_ops看看客户端收到各类消息的处理过程：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>linux/net/ceph/mon_client.c:

static const struct ceph_connection_operations mon_con_ops = {
    ...
    .dispatch = dispatch, /*收到monitor的消息时会调用该函数*/
    ...
};

/*
 * handle incoming message
 */
static void dispatch(struct ceph_connection *con, struct ceph_msg *msg)
{
    struct ceph_mon_client *monc = con-&gt;private;
    int type = le16_to_cpu(msg-&gt;hdr.type);

    if (!monc)
        return;

    /*这里针对不同消息进行不同处理*/
    switch (type) {
    case CEPH_MSG_AUTH_REPLY:
        handle_auth_reply(monc, msg);
        break;

    case CEPH_MSG_MON_SUBSCRIBE_ACK:
        handle_subscribe_ack(monc, msg);
        break;

    case CEPH_MSG_STATFS_REPLY:
        handle_statfs_reply(monc, msg);
        break;

    case CEPH_MSG_POOLOP_REPLY:
        handle_poolop_reply(monc, msg);
        break;

    case CEPH_MSG_MON_MAP:
        ceph_monc_handle_map(monc, msg);
        break;

    case CEPH_MSG_OSD_MAP:
        ceph_osdc_handle_map(&amp;monc-&gt;client-&gt;osdc, msg);
        break;

    default:
        /* can the chained handler handle it? */
        if (monc-&gt;client-&gt;extra_mon_dispatch &amp;&amp;
                monc-&gt;client-&gt;extra_mon_dispatch(monc-&gt;client, msg) == 0)
            break;

        pr_err("received unknown message type %d %s\n", type,
        ceph_msg_type_name(type));
    }
    ceph_msg_put(msg);
}

/*处理monmap消息，接收最新的monitor信息*/
static void ceph_monc_handle_map(struct ceph_mon_client *monc, struct ceph_msg *msg)
{
    struct ceph_client *client = monc-&gt;client;
    struct ceph_monmap *monmap = NULL, *old = monc-&gt;monmap;
    void *p, *end;
    ...

    dout("handle_monmap\n");
    p = msg-&gt;front.iov_base;
    end = p + msg-&gt;front.iov_len;

    /*从收到的消息中解析出monmap内容，它包含所有monitor节点的IP信息*/
    monmap = ceph_monmap_decode(p, end);
    ...

    /*更新monmap*/
    client-&gt;monc.monmap = monmap;
    kfree(old);
    ...

out_unlocked:
    /*唤醒所有在auth_wq中等待的任务*/
    wake_up_all(&amp;client-&gt;auth_wq);
}

/*处理osdmap消息，整体思路和monmap类似；这里分了增量和全量两种模式；
  osdmap中记录了所有osd的IP和状态信息*/
/*
* Process updated osd map.
*
* The message contains any number of incremental and full maps, normally
* indicating some sort of topology change in the cluster.  Kick requests
* off to different OSDs as needed.
*/
void ceph_osdc_handle_map(struct ceph_osd_client *osdc, struct ceph_msg *msg)
{
    ...
}

static void handle_auth_reply(struct ceph_mon_client *monc, struct ceph_msg *msg)
{
    int ret;
    int was_auth = 0;

    ...
    /*处理auth_reply消息*/
    ret = ceph_handle_auth_reply(monc-&gt;auth, msg-&gt;front.iov_base,
                msg-&gt;front.iov_len,
                monc-&gt;m_auth-&gt;front.iov_base,
                monc-&gt;m_auth-&gt;front_alloc_len);
    if (ret &lt; 0) {
        /*如果认证出错，则唤醒等待任务并返回错误信息*/
        monc-&gt;client-&gt;auth_err = ret;
        wake_up_all(&amp;monc-&gt;client-&gt;auth_wq);
    } else if (ret &gt; 0) {
        /*继续发送认证请求，从日志分析共会发送三次*/
        __send_prepared_auth_request(monc, ret);
    } else if (!was_auth &amp;&amp; ceph_auth_is_authenticated(monc-&gt;auth)) {
        /*认证成功，发送针对osdmap的订阅消息*/
        dout("authenticated, starting session\n");

        monc-&gt;client-&gt;msgr.inst.name.type = CEPH_ENTITY_TYPE_CLIENT;
        monc-&gt;client-&gt;msgr.inst.name.num =
        cpu_to_le64(monc-&gt;auth-&gt;global_id);

        __send_subscribe(monc);
        __resend_generic_request(monc);
    }
    ...

}
</code></pre>
</div>

<h4 id="22-rbd_dev_image_probe"><strong>2.2. rbd_dev_image_probe</strong></h4>

<h4 id="23-rbd_dev_device_setup"><strong>2.3. rbd_dev_device_setup</strong></h4>

<h3 id="3-rbd块设备io流程分析">3. RBD块设备IO流程分析</h3>

<p>  我们针对IO流程同样沿着从上至下的顺序进行深入分析，对于和上节重复部分我们将不再展开，重点讨论有差异的部分：</p>

<p>CRUSH</p>

<h3 id="4-libcephko中messenger模块分析">4. libceph.ko中messenger模块分析</h3>

<p><br />
转载请注明：<a href="https://rootw.github.io">吴斌的博客</a> » <a href="https://rootw.github.io/2018/01/RBD-client/">Rados Block Device之二－客户端内核RBD驱动分析</a></p>


  </section>
</article>

<section>
       <ul class="pager">
        
        <li class="previous">
            <a href="/2018/01/RBD-all/" data-toggle="tooltip" data-placement="top" title="Rados Block Device之一－概述">上一篇：  <span>Rados Block Device之一－概述</span>
            </a>
        </li>
        
        
        <li class="next">
            <a href="/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/" data-toggle="tooltip" data-placement="top" title="【计算子系统】进程管理之二：进程替换">下一篇：  <span>【计算子系统】进程管理之二：进程替换</span>
            </a>
        </li>
        
    </ul>
</section>

<section class="post-comments">

<div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
<script>
    var cloudTieConfig = {
        url: document.location.href,
        sourceId: "",
        productKey: "da785706743641a2b774f3a6061e6a66",
        target: "cloud-tie-wrapper"
    };
</script>
<script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script>
  
  
</section>


            <section class="footer">
    <footer>
        <div class = "footer_div">  
        <nav class="cover-navigation navigation--social">
          <ul class="navigation">

          

          
          <!-- Github -->
          <li class="navigation__item_social">
            <a href="https://github.com/rootw" title="@rootw 的 Github" target="_blank">
              <i class='social fa fa-github fa-2x'></i>
              <span class="label">Github</span>
            </a>
          </li>
          
          
          

          

          <!-- RSS -->
          <li class="navigation__item_social">
            <a href="/feed.xml" rel="author" title="RSS" target="_blank">
              <i class='social fa fa-rss fa-2x'></i>
              <span class="label">RSS</span>
            </a>
          </li>

          
          <!-- Email -->
          <li class="navigation__item_social">
            <a href="mailto:wubin_zju@163.com" title="Contact me">
              <i class='social fa fa-envelope fa-2x'></i>
              <span class="label">Email</span>
            </a>
          </li>
          

          </ul>
        </nav>

        </div>

        <div class = "footer_div">  
           <p class="copyright text-muted">
            Copyright &copy; 吴斌 2018 Theme by <a href="http://baixin.io/">leopardpan</a> |
            <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=leopardpan&repo=leopardpan.github.io&type=star&count=true" >
            </iframe>
            </p>
        	<div align="right">
    			<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

          <!-- 访问统计 -->
          <span id="busuanzi_container_site_pv">
            本站总访问量
            <span id="busuanzi_value_site_pv"></span>次
          </span>

        </div>
        <div>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>

<script type="text/javascript" src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




    
  </body>

</html>
