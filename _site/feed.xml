<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>吴斌</title>
    <description>欢迎来到我的技术博客~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 24 Feb 2018 11:38:15 +0800</pubDate>
    <lastBuildDate>Sat, 24 Feb 2018 11:38:15 +0800</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>【时间子系统】四、低精度定时器</title>
        <description>&lt;p&gt;  通过定时器，我们可以控制计算机在将来指定的某个时刻执行特定的动作。传统的定时器，以时钟滴答(jiffy)作为计时单位，因此它的精度较低(例如HZ=1000时，精度为1毫秒)，我们也称之为低精度定时器。&lt;/p&gt;

&lt;h3 id=&quot;1-初始化定时器&quot;&gt;1. 初始化定时器&lt;/h3&gt;

&lt;p&gt;  我们在概述中介绍过，内核中通过init_timer对定时器进行初始化，定时器中最关键的三个信息是：到期时间、到期处理函数、到期处理函数的参数。init_timer宏及定时器结构struct timer_list(取名struct timer可能更合适)的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timer.h:

#define init_timer(timer)                       \
    __init_timer((timer), 0)

#define __init_timer(_timer, _flags)            \
    init_timer_key((_timer), (_flags), NULL, NULL)

struct timer_list {
    /*
     * All fields that change during normal runtime grouped to the
     * same cacheline
     */
    struct list_head entry; /*用于将当前定时器挂到CPU的tvec_base链表中*/
    unsigned long expires; /*定时器到期时间*/
    struct tvec_base *base; /*定时器所属的tvec_base*/

    void (*function)(unsigned long); /*到期处理函数*/
    unsigned long data; /*到期处理函数的参数*/

    int slack; /*允许的偏差值*/

    ...
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  init_timer_key实现时，会将定时器指向执行初始化动作的CPU的tvec_base结构。内核为每个CPU分配一个struct tvec_base对象，用来记录每个CPU上定时器相关的全局信息(我们将在下一节详细说明)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * init_timer_key - initialize a timer
 * @timer: the timer to be initialized
 * @flags: timer flags
 * @name: name of the timer
 * @key: lockdep class key of the fake lock used for tracking timer
 *       sync lock dependencies
 *
 * init_timer_key() must be done to a timer prior calling *any* of the
 * other timer functions.
 */
void init_timer_key(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    debug_init(timer);
    do_init_timer(timer, flags, name, key);
}

static void do_init_timer(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    struct tvec_base *base = __raw_get_cpu_var(tvec_bases);

    timer-&amp;gt;entry.next = NULL;
    timer-&amp;gt;base = (void *)((unsigned long)base | flags);
    timer-&amp;gt;slack = -1;
    ...
}

struct tvec_base {
    spinlock_t lock; /*同步当前tvec_base的链表操作*/
    struct timer_list *running_timer; /*正在运行(到期触发)的定时器*/
    unsigned long timer_jiffies; /*用于判断定时器是否到期的当前时间，通常和系统的jiffies值相等*/
    unsigned long next_timer; /*下一个到期的定时器的到期时间*/
    unsigned long active_timers; /*激活的定时器的个数*/
    struct tvec_root tv1; /*tv1~tv5是用于保存已添加定时器的链表，也称为时间轮*/
    struct tvec tv2;
    struct tvec tv3;
    struct tvec tv4;
    struct tvec tv5;
} ____cacheline_aligned;

/*
 * per-CPU timer vector definitions:
 */
#define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
#define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
#define TVN_SIZE (1 &amp;lt;&amp;lt; TVN_BITS)
#define TVR_SIZE (1 &amp;lt;&amp;lt; TVR_BITS)
#define TVN_MASK (TVN_SIZE - 1)
#define TVR_MASK (TVR_SIZE - 1)
#define MAX_TVAL ((unsigned long)((1ULL &amp;lt;&amp;lt; (TVR_BITS + 4*TVN_BITS)) - 1))

struct tvec {
    struct list_head vec[TVN_SIZE];
};

struct tvec_root {
    struct list_head vec[TVR_SIZE];
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-添加定时器&quot;&gt;2. 添加定时器&lt;/h3&gt;

&lt;p&gt;  add_timer将定时器添加到执行CPU的tvec_base的时间轮链表中。内核根据定时器到期时间与当前时间jiffies的差值(值越小说明到期时间越早)，将定时器分别挂到五个级别的链表数组，级别越低链表到期时间越早，如下表所示：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;链表数组&lt;/th&gt;
      &lt;th&gt;时间差&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;tv1&lt;/td&gt;
      &lt;td&gt;0-255(2^8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv2&lt;/td&gt;
      &lt;td&gt;256–16383(2^14)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv3&lt;/td&gt;
      &lt;td&gt;16384–1048575(2^20)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv4&lt;/td&gt;
      &lt;td&gt;1048576–67108863(2^26)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv5&lt;/td&gt;
      &lt;td&gt;67108864–4294967295(2^32)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  其中tv1的数组大小为TVR_SIZE， tv2 tv3 tv4 tv5的数组大小为TVN_SIZE，根据CONFIG_BASE_SMALL配置项的不同，它们有不同的大小。默认情况下，没有使能CONFIG_BASE_SMALL，TVR_SIZE的大小是256，TVN_SIZE的大小则是64，当需要节省内存空间时，也可以使能CONFIG_BASE_SMALL，这时TVR_SIZE的大小是64，TVN_SIZE的大小则是16，以下的讨论我都是基于没有使能CONFIG_BASE_SMALL的情况。当有一个新的定时器要加入时，系统根据定时器到期的jiffies值和timer_jiffies字段的差值来决定该定时器被放入tv1至tv5中的哪一个数组中，最终，系统中所有的定时器的组织结构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/timer_2.jpg&quot; height=&quot;350&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从add_timer代码实现上看，最终会调用__internal_add_timer并根据时间差将定时器加入到合适的链表中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void
__internal_add_timer(struct tvec_base *base, struct timer_list *timer)
{
    unsigned long expires = timer-&amp;gt;expires;
    unsigned long idx = expires - base-&amp;gt;timer_jiffies; /*idx即为时间差*/
    struct list_head *vec;

    if (idx &amp;lt; TVR_SIZE) {
        int i = expires &amp;amp; TVR_MASK; /*以超时时间(而非时间差idx)作为索引寻找对应的链表，方便后续的超时处理*/
        vec = base-&amp;gt;tv1.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; TVR_BITS) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv2.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 2 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv3.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 3 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + 2 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv4.vec + i;
    } else if ((signed long) idx &amp;lt; 0) {
        /*
         * Can happen if you add a timer with expires == jiffies,
         * or you set a timer to go off in the past
         */
        vec = base-&amp;gt;tv1.vec + (base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK);
    } else {
        int i;
        /* If the timeout is larger than MAX_TVAL (on 64-bit
         * architectures or with CONFIG_BASE_SMALL=1) then we
         * use the maximum timeout.
         */
        if (idx &amp;gt; MAX_TVAL) {
            idx = MAX_TVAL;
            expires = idx + base-&amp;gt;timer_jiffies;
        }
        i = (expires &amp;gt;&amp;gt; (TVR_BITS + 3 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv5.vec + i;
    }
    /*
     * Timers are FIFO:
     */
    list_add_tail(&amp;amp;timer-&amp;gt;entry, vec);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-触发定时器&quot;&gt;3. 触发定时器&lt;/h3&gt;

&lt;p&gt;  在时钟中断部分，我们提到过每次中断处理时都会调用run_local_timers进行本地定时器的处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/*
 * Called by the local, per-CPU timer interrupt on SMP.
 */
void run_local_timers(void)
{
    ...
    raise_softirq(TIMER_SOFTIRQ); /*最终在中断返回时进入软中断处理函数run_timer_softirq*/
}

/*
 * This function runs timers and the timer-tq in bottom half context.
 */
static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    ...

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) /*实际当前时间晚于base中记录的当前时间，说明需要更新base中时间或者有定时器到期*/
        __run_timers(base);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  定时器的到期处理逻辑中，总是先处理tv1中的定时器，如果tv1中所有的链表为空，再从tv2中移动链表并重新添加到tv1中；如果tv1和tv2中为空，再从tv3中移动链表重新添加到tv1和tv2中；依此类推。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * __run_timers - run all expired timers (if any) on this CPU.
 * @base: the timer vector to be processed.
 *
 * This function cascades all vectors and executes all expired timer
 * vectors.
 */
static inline void __run_timers(struct tvec_base *base)
{
    struct timer_list *timer;

    spin_lock_irq(&amp;amp;base-&amp;gt;lock);
    while (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) {
        struct list_head work_list;
        struct list_head *head = &amp;amp;work_list;
        int index = base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK; /*以base中的当前时间为索引取出已到期的定时器*/

        /*
         * Cascade timers:
         */
        /*如果低级链表为空，则从高级别链表中移动添加到低级别中*/
        if (!index &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv2, INDEX(0))) &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv3, INDEX(1))) &amp;amp;&amp;amp;
            !cascade(base, &amp;amp;base-&amp;gt;tv4, INDEX(2)))
                cascade(base, &amp;amp;base-&amp;gt;tv5, INDEX(3));
        ++base-&amp;gt;timer_jiffies; /*累加base中当前时间*/
        list_replace_init(base-&amp;gt;tv1.vec + index, &amp;amp;work_list);
        /*处理已到期的定时期的回调函数*/
        while (!list_empty(head)) {
            void (*fn)(unsigned long);
            unsigned long data;
            bool irqsafe;

            timer = list_first_entry(head, struct timer_list,entry);
            fn = timer-&amp;gt;function;
            data = timer-&amp;gt;data;
            irqsafe = tbase_get_irqsafe(timer-&amp;gt;base);

            timer_stats_account_timer(timer);

            base-&amp;gt;running_timer = timer;
            detach_expired_timer(timer, base);

            if (irqsafe) {
                spin_unlock(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock(&amp;amp;base-&amp;gt;lock);
            } else {
                spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock_irq(&amp;amp;base-&amp;gt;lock);
            }
        }
    }
    base-&amp;gt;running_timer = NULL;
    spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
}

#define INDEX(N) ((base-&amp;gt;timer_jiffies &amp;gt;&amp;gt; (TVR_BITS + (N) * TVN_BITS)) &amp;amp; TVN_MASK)

static int cascade(struct tvec_base *base, struct tvec *tv, int index)
{
    /* cascade all the timers from tv up one level */
    struct timer_list *timer, *tmp;
    struct list_head tv_list;

    list_replace_init(tv-&amp;gt;vec + index, &amp;amp;tv_list);

    /*
     * We are removing _all_ timers from the list, so we
     * don't have to detach them individually.
     */
    list_for_each_entry_safe(timer, tmp, &amp;amp;tv_list, entry) {
        BUG_ON(tbase_get_base(timer-&amp;gt;base) != base);
        /* No accounting, while moving them */
        __internal_add_timer(base, timer);
    }

    return index;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;【时间子系统】四、低精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】三、时钟中断－定时基础</title>
        <description>&lt;p&gt;  时钟中断是各种定时器(timer)能够正常工作的前提，同时它和进程调度(tick事件)也密不可分，因此在分析定时器原理前，我们先来深入了解一下时钟中断的原理。&lt;/p&gt;

&lt;h3 id=&quot;1-中断初始化&quot;&gt;1. 中断初始化&lt;/h3&gt;

&lt;p&gt;  时钟中断涉及时钟事件设备(Clock Event Device)等多个概念，我们先通过分析初始化流程来理解这些概念。&lt;/p&gt;

&lt;h4 id=&quot;11-bsp初始化阶段&quot;&gt;&lt;strong&gt;1.1. BSP初始化阶段&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  时钟中断的初始化发生在启动CPU(BSP)上，由start_kernel函数作为总体入口。在完成IO-APIC中断控制器的相关初始化动作后，由late_time_init作为初始化入口。针对x86架构，该函数的实现体为x86_late_time_init：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/time.c:

void __init time_init(void)
{
    late_time_init = x86_late_time_init;
}

static __init void x86_late_time_init(void)
{
    x86_init.timers.timer_init(); /*指向hpet_time_init*/
    ...
}

void __init hpet_time_init(void)
{
    if (!hpet_enable())
        setup_pit_timer();
    setup_default_timer_irq();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  在我们的示例架构i440fx下，hpet没有使能，因此系统将使用PIT作为启动CPU(BSP)的本地tick设备(tick事件发生源)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/i8253.c:

void __init setup_pit_timer(void)
{
    clockevent_i8253_init(true); /*PIT芯片代号为8253*/
    global_clock_event = &amp;amp;i8253_clockevent;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PIT本质上是一种全局时钟事件设备，也就是说它不和某一个CPU绑定，这和后文将介绍的LAPIC Timer不同。然而在BSP初始化过程中，它将暂时被用作BSP的本地tick设备。后续在完成SMP的初始化后，每个CPU都有各自不同的本地tick设备(即本地LAPIC Timer)。tick设备的作用就是周期性(由内核配置参数HZ控制，例始HZ=1000代表每秒产生1000个tick中断)地产生时钟中断，CPU在处理中断的过程中可以决定是否需要进行进程调度。&lt;/p&gt;

&lt;p&gt;  每个时钟事件设备可以有两种工作模式：单次模式(oneshot)和周期模式(periodic)。工作在单次模式时，每次设置完到期时间后，时钟事件设备只会产生一次中断；而工作在周期模式时，时钟事件设备会以设定频率周期性地产生中断。单次模式相比周期模式具备更强的灵活性，我们可以动态控制时钟中断的间隔，从而实现像动态时钟(nohz)之类的高级特性(我们将在后续博文专题介绍)。从下面的代码中，我们可以看出PIT可以同时支持oneshot和periodic两种模式，并在初始化时指定其亲和CPU为当前执行CPU(即BSP)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/driver/clocksource/i8253.c:

/*
 * On UP the PIT can serve all of the possible timer functions. On SMP systems
 * it can be solely used for the global tick.
 */
struct clock_event_device i8253_clockevent = {
    .name           = &quot;pit&quot;,
    .features       = CLOCK_EVT_FEAT_PERIODIC,
    .set_mode       = init_pit_timer,
    .set_next_event = pit_next_event,
};

void __init clockevent_i8253_init(bool oneshot)
{
    if (oneshot)
        i8253_clockevent.features |= CLOCK_EVT_FEAT_ONESHOT;
    /*
     * Start pit with the boot cpu mask. x86 might make it global
     * when it is used as broadcast device later.
     */
    i8253_clockevent.cpumask = cpumask_of(smp_processor_id());

    clockevents_config_and_register(&amp;amp;i8253_clockevent, PIT_TICK_RATE,
            0xF, 0x7FFF);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PIT是时钟事件设备的一种具体硬件实现，从软件抽象层来说，各种时钟事件设备都会调度内核的clockevents中的注册函数进行注册：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
/**
 * clockevents_config_and_register - Configure and register a clock event device
 * @dev:	device to register
 * @freq:	The clock frequency
 * @min_delta:	The minimum clock ticks to program in oneshot mode
 * @max_delta:	The maximum clock ticks to program in oneshot mode
 *
 * min/max_delta can be 0 for devices which do not support oneshot mode.
 */
void clockevents_config_and_register(struct clock_event_device *dev,
    u32 freq, unsigned long min_delta, unsigned long max_delta)
{
    dev-&amp;gt;min_delta_ticks = min_delta;
    dev-&amp;gt;max_delta_ticks = max_delta;
    clockevents_config(dev, freq); /*根据内部计数器频率计算相关转换参数*/
    clockevents_register_device(dev);
}

void clockevents_register_device(struct clock_event_device *dev)
{
    unsigned long flags;

    ...

    raw_spin_lock_irqsave(&amp;amp;clockevents_lock, flags);

    list_add(&amp;amp;dev-&amp;gt;list, &amp;amp;clockevent_devices); /*将当前设备加入到全局clockevent_devices链表中*/
    tick_check_new_device(dev); /*检测当前设备是否适合作当前执行CPU的本地tick设备或全局broadcast设备*/
    clockevents_notify_released(); /*对于被释放的设备，重新加入全局列表并作tick_check_new_device检测*/

    raw_spin_unlock_irqrestore(&amp;amp;clockevents_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  如上代码所示，新注册一个事钟设备时都会对其进行检测，以判断其是否适合作为本地tick设备(由struct tick_device定义，它是对struct clock_event_device的封装)。如果新的设备适合作本地tick设备，那将替换原有的tcik设备(如果在存的话)。被替换的老设备将有机会重新加入全局clockevent_devices链表并进行检测，此时的检测主要是判定它是否适合作为广播(broadcast)设备。广播设备的作用是为了当某些本地tick设备随CPU进入节电状态而停止工作时，能够再次发生中断以唤醒进入节电状态的CPU继续进行工作。这种情况下本地tick设备是无能为力的，因为它也随CPU进入睡眠状态了。这里我们只需要理解广播设备的作用，不用太深挖其内部实现细节：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

void tick_check_new_device(struct clock_event_device *newdev)
{
    struct clock_event_device *curdev;
    struct tick_device *td;
    int cpu;
    unsigned long flags;

    raw_spin_lock_irqsave(&amp;amp;tick_device_lock, flags);

    cpu = smp_processor_id(); /*取当前执行CPU*/
    if (!cpumask_test_cpu(cpu, newdev-&amp;gt;cpumask)) /*判断当前CPU是否在新设备的CPU掩码位中*/
        goto out_bc; /*不在，则转而判断新设备是否可作为bc(broadcast)设备*/

    td = &amp;amp;per_cpu(tick_cpu_device, cpu); /*取出当前CPU的本地tick设备*/
    curdev = td-&amp;gt;evtdev; /*本地tick设备所封装的当前时钟事件设备，可能为空*/

    /* cpu local device ? */
    if (!tick_check_percpu(curdev, newdev, cpu)) /*判断新设备是否更适合作本地tick设备*/
        goto out_bc; /*如果不合适则进行bc判定*/

    /* Preference decision */
    if (!tick_check_preferred(curdev, newdev)) /*判断新设备是否符合偏好，如oneshot优先等*/
        goto out_bc;

    if (!try_module_get(newdev-&amp;gt;owner))
        return;

    /*如果执行到这里，说明新设备newdev相比老设备curdev更适合作本地tick设备，将进行替换操作*/

    /*
     * Replace the eventually existing device by the new
     * device. If the current device is the broadcast device, do
     * not give it back to the clockevents layer !
     */
    if (tick_is_broadcast_device(curdev)) { /*如果老设备是一个广播设备将对其进行关闭*/
        clockevents_shutdown(curdev);
        curdev = NULL;
    }
    clockevents_exchange_device(curdev, newdev); /*进行交换*/
    tick_setup_device(td, newdev, cpu, cpumask_of(cpu)); /*重新设定新设备为本地tick设备*/
    if (newdev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_ONESHOT)
        tick_oneshot_notify();

    raw_spin_unlock_irqrestore(&amp;amp;tick_device_lock, flags);
    return;

out_bc:
    /*
     * Can the new device be used as a broadcast device ?
     */
    tick_install_broadcast_device(newdev);
    raw_spin_unlock_irqrestore(&amp;amp;tick_device_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于新设定的本地tick设备(没有老设备进行替换时)，初始时内核总是将其设为周期模式(periodic)。随着系统的运行，当外部条件成熟后，在时钟中断的处理过程中会将它的模式切换到单次模式(oneshot)以支持更高级功能。这部分切换我们将在高精度时钟部分进行分析。这里我们看看tick_setup_device的基本动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

static void tick_setup_device(struct tick_device *td,
        struct clock_event_device *newdev, int cpu,
        const struct cpumask *cpumask)
{
    ktime_t next_event;
    void (*handler)(struct clock_event_device *) = NULL;

    /*
     * First device setup ?
     */
    if (!td-&amp;gt;evtdev) {
        /*
         * If no cpu took the do_timer update, assign it to
         * this cpu:
         */
        if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
            if (!tick_nohz_full_cpu(cpu))
                tick_do_timer_cpu = cpu; /*动态时钟未打开情况下，初始化过程中首个注册本地tick设备的CPU将负责在中断处理时完成jiffies更新*/
            else
                tick_do_timer_cpu = TICK_DO_TIMER_NONE;
            tick_next_period = ktime_get(); /*下次tick事件发生时间，这里初始化为当前时间*/
            tick_period = ktime_set(0, NSEC_PER_SEC / HZ); /*tick的时间间隔*/
        }

        /*
         * Startup in periodic mode first.
         */
        td-&amp;gt;mode = TICKDEV_MODE_PERIODIC; /*首次注册tick设备时，将其设为periodic模式*/
    } else {
        handler = td-&amp;gt;evtdev-&amp;gt;event_handler;
        next_event = td-&amp;gt;evtdev-&amp;gt;next_event;
        td-&amp;gt;evtdev-&amp;gt;event_handler = clockevents_handle_noop;
    }

    td-&amp;gt;evtdev = newdev;

    /*
     * When the device is not per cpu, pin the interrupt to the
     * current cpu:
     */
    if (!cpumask_equal(newdev-&amp;gt;cpumask, cpumask))
        irq_set_affinity(newdev-&amp;gt;irq, cpumask);

    /*
     * When global broadcasting is active, check if the current
     * device is registered as a placeholder for broadcast mode.
     * This allows us to handle this x86 misfeature in a generic
     * way. This function also returns !=0 when we keep the
     * current active broadcast state for this CPU.
     */
    if (tick_device_uses_broadcast(newdev, cpu))
    return;

    if (td-&amp;gt;mode == TICKDEV_MODE_PERIODIC)
        tick_setup_periodic(newdev, 0);
    else
        tick_setup_oneshot(newdev, handler, next_event);
}

void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
    tick_set_periodic_handler(dev, broadcast); /*设置dev-&amp;gt;event_handler为tick_handle_periodic*/

    ...

    if ((dev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_PERIODIC) &amp;amp;&amp;amp;
            !tick_broadcast_oneshot_active()) {
        clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC); /*设置时钟事件设备的工作模式为周期模式，内部将调用set_mode函数*/
    } else {
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  回到上层初始化流程，完成将PIT设置为BSP的本地tick设备后，内核在setup_default_timer_irq中完成中断处理函数的设定并使能中断信号。之后BSP在初始化过程中有会周期性地收到PIT产生的0号时钟中断，并进行中断处理。对于PIT时钟中断的处理我们将在下一节展开：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
static struct irqaction irq0  = {
    .handler    = timer_interrupt,
    .flags      = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,
    .name       = &quot;timer&quot;
};

void __init setup_default_timer_irq(void)
{
    setup_irq(0, &amp;amp;irq0); /*设备中断处理对象并使能中断信号，0号中断即时钟中断*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;12-smp初始化阶段&quot;&gt;&lt;strong&gt;1.2. SMP初始化阶段&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在x86 SMP系统中，每个CPU的Local APIC中都有一个高精度的时钟事件设备(LAPIC Timer)，因此在BSP初始化的最后阶段及AP的初始化过程中，都会调用setup_APIC_timer进行LAPIC Timer的初始化：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/apic/apic.c:

static void __cpuinit setup_APIC_timer(void)
{
    struct clock_event_device *levt = &amp;amp;__get_cpu_var(lapic_events); /*每个CPU对应的lapic timer*/

    if (this_cpu_has(X86_FEATURE_ARAT)) { /*ARAT: Always Run Apic Timer，intel实现的特性；timer不随CPU睡眠而停止*/
        lapic_clockevent.features &amp;amp;= ~CLOCK_EVT_FEAT_C3STOP;
        /* Make LAPIC timer preferrable over percpu HPET */
        lapic_clockevent.rating = 150;
    }

    memcpy(levt, &amp;amp;lapic_clockevent, sizeof(*levt));
    levt-&amp;gt;cpumask = cpumask_of(smp_processor_id());

    if (this_cpu_has(X86_FEATURE_TSC_DEADLINE_TIMER)) {
        ...
    } else
        clockevents_register_device(levt);
}

/*
 * The local apic timer can be used for any function which is CPU local.
 */
static struct clock_event_device lapic_clockevent = {
    .name       = &quot;lapic&quot;,
    .features   = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT
                    | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_DUMMY,/*DUMMY标志在BSP初始化时将会被清除*/
    .shift      = 32,
    .set_mode   = lapic_timer_setup,
    .set_next_event	= lapic_next_event,
    .broadcast  = lapic_timer_broadcast,
    .rating     = 100,
    .irq        = -1,
};
static DEFINE_PER_CPU(struct clock_event_device, lapic_events);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  setup_APIC_timer函数内部同样是调用clockevents_register_device进行注册，对于BSP它将使用lapic timer替换PIT作为本地tick设备，而PIT将设为广播设备；对于AP，将直接使用lapic timer作为本地tick设备。注意，对于lapic timer的处理函数入口为smp_apic_timer_interrupt，它是在中断系统初始化过程(start_kernel-&amp;gt;init_IRQ-&amp;gt;apic_intr_init)中设定的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/irqinit.c:

static void __init apic_intr_init(void)
{
    ...
    /*apic_timer_interrupt将跳转到smp_apic_timer_interrupt*/
    alloc_intr_gate(LOCAL_TIMER_VECTOR, apic_timer_interrupt);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-周期性中断处理&quot;&gt;2. 周期性中断处理&lt;/h3&gt;

&lt;h4 id=&quot;21-pit中断处理&quot;&gt;&lt;strong&gt;2.1. PIT中断处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  BSP完成对PIT的初始化并使能中断信号后，BSP便可周期性地接收到来自PIT的中断，它对该中断的处理句柄是：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/time.c:

/*
 * Default timer interrupt handler for PIT/HPET
 */
static irqreturn_t timer_interrupt(int irq, void *dev_id)
{
    global_clock_event-&amp;gt;event_handler(global_clock_event);
    return IRQ_HANDLED;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里的global_clock_event即是i8253_clockevent，它最初工作在周期模式下，相应的处理函数为tick_handle_periodic：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

/*
 * Event handler for periodic ticks
 */
void tick_handle_periodic(struct clock_event_device *dev)
{
    int cpu = smp_processor_id();
    ktime_t next;

    /*实际的周期性处理逻辑*/
    tick_periodic(cpu);

    /*对于周期模式的时钟事件设备直接返回，无须设置下次到期时间*/
    if (dev-&amp;gt;mode != CLOCK_EVT_MODE_ONESHOT)
        return;
    
    /*对于单次模式的设备，如果要实现周期性中断，则在每次中断处理中要设置下次到期时间*/
    /*
     * Setup the next period for devices, which do not have
     * periodic mode:
     */
    next = ktime_add(dev-&amp;gt;next_event, tick_period);
    for (;;) {
        if (!clockevents_program_event(dev, next, false))
            return;
        /*
         * Have to be careful here. If we're in oneshot mode,
         * before we call tick_periodic() in a loop, we need
         * to be sure we're using a real hardware clocksource.
         * Otherwise we could get trapped in an infinite
         * loop, as the tick_periodic() increments jiffies,
         * when then will increment time, posibly causing
         * the loop to trigger again and again.
         */
        if (timekeeping_valid_for_hres())
            tick_periodic(cpu);
        next = ktime_add(next, tick_period);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  tick_periodic负责实际的处理逻辑，它主要完成对jiffies和xtime(墙上时间)的周期性更新，并对进程进行运行计时和调度：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

/*
 * Periodic tick
 */
static void tick_periodic(int cpu)
{
    if (tick_do_timer_cpu == cpu) {
        /*如果当前CPU负责计时更新，则调用do_timer进行更新*/
        write_seqlock(&amp;amp;jiffies_lock);

        /* Keep track of the next tick event */
        tick_next_period = ktime_add(tick_next_period, tick_period);

        do_timer(1);
        write_sequnlock(&amp;amp;jiffies_lock);
    }

    /*更新进程运行时间并做调度判断*/
    update_process_times(user_mode(get_irq_regs()));
    profile_tick(CPU_PROFILING);
}

/*
 * Must hold jiffies_lock
 */
void do_timer(unsigned long ticks)
{
    jiffies_64 += ticks;
    update_wall_time(); /*周期性地更新墙上时间*/
    calc_global_load(ticks);
}

void update_process_times(int user_tick)
{
    struct task_struct *p = current;
    int cpu = smp_processor_id();

    /* Note: this timer irq context must be accounted for as well. */
    account_process_tick(p, user_tick); /*当前进程运行时间统计*/
    run_local_timers(); /*检查本地定时器，我们将在定时器部分分析*/
    ...
    scheduler_tick(); /*调度检测*/
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;22-lapic-timer中断处理&quot;&gt;&lt;strong&gt;2.2. LAPIC Timer中断处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SMP初始化完成后，所有CPU的本地tick设备变更为LAPIC Timer，虽然其工作模式仍然是周期性模式，但中断处理函数入口变更为smp_apic_timer_interrupt：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/apic/apic.c:

void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
{
    struct pt_regs *old_regs = set_irq_regs(regs);

    /*
     * NOTE! We'd better ACK the irq immediately,
     * because timer handling can be slow.
     */
    ack_APIC_irq();
    /*
     * update_process_times() expects us to have done irq_enter().
     * Besides, if we don't timer interrupts ignore the global
     * interrupt lock, which is the WrongThing (tm) to do.
     */
    irq_enter();
    exit_idle();
    local_apic_timer_interrupt();
    irq_exit();

    set_irq_regs(old_regs);
}

/*
 * The guts of the apic timer interrupt
 */
static void local_apic_timer_interrupt(void)
{
    int cpu = smp_processor_id();
    struct clock_event_device *evt = &amp;amp;per_cpu(lapic_events, cpu);

    /*
     * Normally we should not be here till LAPIC has been initialized but
     * in some cases like kdump, its possible that there is a pending LAPIC
     * timer interrupt from previous kernel's context and is delivered in
     * new kernel the moment interrupts are enabled.
     *
     * Interrupts are enabled early and LAPIC is setup much later, hence
     * its possible that when we get here evt-&amp;gt;event_handler is NULL.
     * Check for event_handler being NULL and discard the interrupt as
     * spurious.
     */
    if (!evt-&amp;gt;event_handler) {
        pr_warning(&quot;Spurious LAPIC timer interrupt on cpu %d\n&quot;, cpu);
        /* Switch it off */
        lapic_timer_setup(CLOCK_EVT_MODE_SHUTDOWN, evt);
        return;
    }

    /*
     * the NMI deadlock-detector uses this.
     */
    inc_irq_stat(apic_timer_irqs);

    evt-&amp;gt;event_handler(evt);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  对于周期模式的LAPIC Timer，其event_hander仍然为tick_handle_periodic，因此核心处理逻辑和PIT是一样的。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/时钟中断/&quot;&gt;【时间子系统】三、时钟中断－定时基础&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E6%97%B6%E9%92%9F%E4%B8%AD%E6%96%AD/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E6%97%B6%E9%92%9F%E4%B8%AD%E6%96%AD/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】二、计时原理－timekeeper与clocksource</title>
        <description>&lt;p&gt;  本篇博文我们将深入分析一下内核是如何使用计时硬件对应用提供服务的。&lt;/p&gt;

&lt;h3 id=&quot;1-内核表示时间数据结构&quot;&gt;1. 内核表示时间数据结构&lt;/h3&gt;

&lt;p&gt;  内核中对时间的表示有多种形式，可以使用在不同的应用场景。我们在时间概述中看到的gettimeofday的示例中，采用的数据结构是struct timeval，它的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/time.h:

struct timeval {
    __kernel_time_t         tv_sec;     /* seconds */
    __kernel_suseconds_t    tv_usec;    /* microseconds */
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  从上面的定义中，我们可以看到struct timeval记录了当前时间的秒数和毫秒数，精度就是毫秒。那么这里的秒数和毫秒数是相对哪个时间点(epoch)而言的呢？按照UNIX系统的习惯，记录时间的秒数和毫秒数是相对1970年1月1日00:00:00 +0000(UTC)而言的。另外，记录秒数的__kernel_time_t和记录毫秒的__kernel_suseconds_t在64位系统中都是long型的。&lt;/p&gt;

&lt;p&gt;  除了struct timeval，内核中还定义了精度更高的struct timespec，它的精度是纳秒：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/time.h:

struct timespec {
    __kernel_time_t tv_sec;     /* seconds */
    long            tv_nsec;    /* nanoseconds */
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  此外，为了兼容各种系统架构，内核也定义了ktime_t类型，在64位机器中对应long，时间表示单位是纳秒：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ktime.h:

union ktime {
    s64	tv64;
#if BITS_PER_LONG != 64 &amp;amp;&amp;amp; !defined(CONFIG_KTIME_SCALAR)
    struct {
#ifdef __BIG_ENDIAN
        s32	sec, nsec;
#else
        s32	nsec, sec;
#endif
    } tv;
#endif
};

typedef union ktime ktime_t;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-内核时间类别&quot;&gt;2. 内核时间类别&lt;/h3&gt;

&lt;p&gt;  时间概述中示例程序使用的gettimeofday将返回实时间(real time，或叫墙上时间wall time)，代表现实生活中使用的时间。除了墙上时间，内核也提供了线性时间(monotonic time，它不可调整，随系统运行线性增加，但不包括休眠时间)、启动时间(boot time，它也不可调整，并包括了休眠时间)等多种时间类型，以使应用在不同场景(获取不同类型时间的用户态方法是clock_gettime)，下表汇总了各类时间的要素点：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;时间类别&lt;/th&gt;
      &lt;th&gt;精度&lt;/th&gt;
      &lt;th&gt;可手动调整&lt;/th&gt;
      &lt;th&gt;受NTP调整影响&lt;/th&gt;
      &lt;th&gt;时间起点&lt;/th&gt;
      &lt;th&gt;受闰秒影响&lt;/th&gt;
      &lt;th&gt;系统暂停时是否可工作&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC_RAW&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME_COARSE&lt;/td&gt;
      &lt;td&gt;tick&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC_COARSE&lt;/td&gt;
      &lt;td&gt;tick&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BOOTTIME&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;machine start&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME_ALARM&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BOOTTIME_ALARM&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;machine start&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TAI&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  关于闰秒，我们需要先理解什么是原子秒？原子秒提出的背景是人们对于”秒”的精确定义追求。多长时间可以算作1秒？这是一个很难准确回答的问题。但是后来科学家发现铯133原子在能量跃迁时辐射的电磁波振荡频率非常稳定，因此就被用来定义时间的基本单位：秒，即原子秒。通过原子秒延展出来的时间轴就是TAI(International Atomic Time)。原子时间虽然精准，但是对人类来说不太友好，它和传统的地球自转和公转的周期性自然现象存在时间差。在这样的背景下，UTC(Coordinated Universal Time)被提出。它使用原子秒作为计时单位，但又会适当调整以适应人们的日常生活。这个调整的时间差就是闰秒。TAI和UTC在1972进行了校准，两者相差10秒，从此后到2017年，又调整了27次，因此TAI比UTC快了37秒。&lt;/p&gt;

&lt;h3 id=&quot;3-深入do_gettimeofday&quot;&gt;3. 深入do_gettimeofday&lt;/h3&gt;

&lt;p&gt;  用户态gettimeofday接口在内核中是通过do_gettimeofday实现的，从调用层次上看，它可以分为timekeeper和clocksource两层。&lt;/p&gt;

&lt;h4 id=&quot;31-timekeeper&quot;&gt;&lt;strong&gt;3.1 timekeeper&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  timekeeper是内核中负责计时功能的核心对象，它通过使用当前系统中最优的clocksource来提供时间服务：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/**
 * do_gettimeofday - Returns the time of day in a timeval
 * @tv:		pointer to the timeval to be set
 *
 * NOTE: Users should be converted to using getnstimeofday()
 */
void do_gettimeofday(struct timeval *tv)
{
    struct timespec now;

    getnstimeofday(&amp;amp;now); /*获取纳秒精度的当前时间*/
    tv-&amp;gt;tv_sec = now.tv_sec;
    tv-&amp;gt;tv_usec = now.tv_nsec/1000;
}

/**
 * __getnstimeofday - Returns the time of day in a timespec.
 * @ts:		pointer to the timespec to be set
 *
 * Updates the time of day in the timespec.
 * Returns 0 on success, or -ve when suspended (timespec will be undefined).
 */
int __getnstimeofday(struct timespec *ts)
{
    struct timekeeper *tk = &amp;amp;timekeeper; /*系统全局对象timekeeper*/
    unsigned long seq;
    s64 nsecs = 0;

    do {
        seq = read_seqcount_begin(&amp;amp;timekeeper_seq); /*以顺序锁来同步各个任务对timekeeper的读写操作*/

        ts-&amp;gt;tv_sec = tk-&amp;gt;xtime_sec; /*获取最近更新的墙上时间的秒数(墙上时间会周期性地被更新，将在定时原理部分讨论)*/
        nsecs = timekeeping_get_ns(tk); /*获取当前墙上时间相对(tk-&amp;gt;xtime_sec, 0)的纳秒时间间隔*/

    } while (read_seqcount_retry(&amp;amp;timekeeper_seq, seq));

    ts-&amp;gt;tv_nsec = 0;
    timespec_add_ns(ts, nsecs);/*累加前面获取的纳秒时间间隔以得到正确的当前墙上时间；有可能导致秒数进位*/

    ...
    return 0;
}

static inline s64 timekeeping_get_ns(struct timekeeper *tk)
{
    cycle_t cycle_now, cycle_delta;
    struct clocksource *clock;
    s64 nsec;

    /*通过当前最优clocksource获取当前时间计数cycle；不同的clocksource可以提供不同的read实现*/
    /* read clocksource: */
    clock = tk-&amp;gt;clock;
    cycle_now = clock-&amp;gt;read(clock);
    
    /*通过clocksource中的当前计数值与最近一次更新墙上时间时获取的值的差值来计算时间间隔*/

    /* calculate the delta since the last update_wall_time: */    
    cycle_delta = (cycle_now - clock-&amp;gt;cycle_last) &amp;amp; clock-&amp;gt;mask;

    /*tk-&amp;gt;mult和tk-&amp;gt;shift是用来进行将cycle数值转成纳秒的转换参数，参见clocksource中的说明*/
    nsec = cycle_delta * tk-&amp;gt;mult + tk-&amp;gt;xtime_nsec; /*tk-&amp;gt;xtime_nsec是最近更新的墙上时间的秒纳数左移tk-&amp;gt;shift后的值*/
    nsec &amp;gt;&amp;gt;= tk-&amp;gt;shift;

    /* If arch requires, add in get_arch_timeoffset() */
    return nsec + get_arch_timeoffset();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timekeeper_internal.h:

/* Structure holding internal timekeeping values. */
struct timekeeper {
    /* Current clocksource used for timekeeping. */
    struct clocksource	*clock;
    /* NTP adjusted clock multiplier */
    u32			mult;
    /* The shift value of the current clocksource. */
    u32			shift;
    /* Number of clock cycles in one NTP interval. */
    cycle_t			cycle_interval;
    /* Last cycle value (also stored in clock-&amp;gt;cycle_last) */
    cycle_t			cycle_last;
    /* Number of clock shifted nano seconds in one NTP interval. */
    u64			xtime_interval;
    /* shifted nano seconds left over when rounding cycle_interval */
    s64			xtime_remainder;
    /* Raw nano seconds accumulated per NTP interval. */
    u32			raw_interval;

    /* Current CLOCK_REALTIME time in seconds */
    u64			xtime_sec;
    /* Clock shifted nano seconds */
    u64			xtime_nsec;

    /* Difference between accumulated time and NTP time in ntp
     * shifted nano seconds. */
    s64			ntp_error;
    /* Shift conversion between clock shifted nano seconds and
     * ntp shifted nano seconds. */
    u32			ntp_error_shift;

    /*
     * wall_to_monotonic is what we need to add to xtime (or xtime corrected
     * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
     * at zero at system boot time, so wall_to_monotonic will be negative,
     * however, we will ALWAYS keep the tv_nsec part positive so we can use
     * the usual normalization.
     *
     * wall_to_monotonic is moved after resume from suspend for the
     * monotonic time not to jump. We need to add total_sleep_time to
     * wall_to_monotonic to get the real boot based time offset.
     *
     * - wall_to_monotonic is no longer the boot time, getboottime must be
     * used instead.
     */
    struct timespec		wall_to_monotonic;
    /* Offset clock monotonic -&amp;gt; clock realtime */
    ktime_t			offs_real;
    /* time spent in suspend */
    struct timespec		total_sleep_time;
    /* Offset clock monotonic -&amp;gt; clock boottime */
    ktime_t			offs_boot;
    /* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
    struct timespec		raw_time;
    /* The current UTC to TAI offset in seconds */
    s32			tai_offset;
    /* Offset clock monotonic -&amp;gt; clock tai */
    ktime_t			offs_tai;

};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;32-clocksource&quot;&gt;&lt;strong&gt;3.2 clocksource&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  内核通过clocksource对象来描述物理计时设备，x86架构下最常见的计时设备是tsc，我们来看看tsc对应的clocksource:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/tsc.c:

static struct clocksource clocksource_tsc = {
    .name                   = &quot;tsc&quot;,
    .rating                 = 300,
    .read                   = read_tsc,
    .resume                 = resume_tsc,
    .mask                   = CLOCKSOURCE_MASK(64),
    .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
                              CLOCK_SOURCE_MUST_VERIFY,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/clocksource.h:

/**
 * struct clocksource - hardware abstraction for a free running counter
 *	Provides mostly state-free accessors to the underlying hardware.
 *	This is the structure used for system time.
 *
 * @name:		ptr to clocksource name
 * @list:		list head for registration
 * @rating:		rating value for selection (higher is better)
 *			To avoid rating inflation the following
 *			list should give you a guide as to how
 *			to assign your clocksource a rating
 *			1-99: Unfit for real use
 *				Only available for bootup and testing purposes.
 *			100-199: Base level usability.
 *				Functional for real use, but not desired.
 *			200-299: Good.
 *				A correct and usable clocksource.
 *			300-399: Desired.
 *				A reasonably fast and accurate clocksource.
 *			400-499: Perfect
 *				The ideal clocksource. A must-use where
 *				available.
 * @read:		returns a cycle value, passes clocksource as argument
 * @enable:		optional function to enable the clocksource
 * @disable:		optional function to disable the clocksource
 * @mask:		bitmask for two's complement
 *			subtraction of non 64 bit counters
 * @mult:		cycle to nanosecond multiplier
 * @shift:		cycle to nanosecond divisor (power of two)
 * @max_idle_ns:	max idle time permitted by the clocksource (nsecs)
 * @maxadj:		maximum adjustment value to mult (~11%)
 * @flags:		flags describing special properties
 * @archdata:		arch-specific data
 * @suspend:		suspend function for the clocksource, if necessary
 * @resume:		resume function for the clocksource, if necessary
 * @cycle_last:		most recent cycle counter value seen by ::read()
 */
struct clocksource {
    /*
     * Hotpath data, fits in a single cache line when the
     * clocksource itself is cacheline aligned.
     */
    cycle_t (*read)(struct clocksource *cs);
    cycle_t cycle_last;
    cycle_t mask;
    u32 mult;
    u32 shift;
    u64 max_idle_ns;
    u32 maxadj;
#ifdef CONFIG_ARCH_CLOCKSOURCE_DATA
    struct arch_clocksource_data archdata;
#endif

    const char *name;
    struct list_head list;
    int rating;
    int (*enable)(struct clocksource *cs);
    void (*disable)(struct clocksource *cs);
        unsigned long flags;
    void (*suspend)(struct clocksource *cs);
    void (*resume)(struct clocksource *cs);

    /* private: */
#ifdef CONFIG_CLOCKSOURCE_WATCHDOG
    /* Watchdog related data, used by the framework */
    struct list_head wd_list;
    cycle_t cs_last;
    cycle_t wd_last;
#endif
} ____cacheline_aligned;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  上面代码的注释部分已经清楚地解析了tsc是一种精度良好的clocksource，我们可以使用read_tsc(本质是通过rdtsc指令)来获取当前tsc计数值。cycle_last表示最近一次从clocksource中获取的cycle计数值；mask表示当前clocksource中计数器的有效位数；mult和shift用来计算从cycle到纳秒的转换；max_idle_ns表示当前clocksource允许的最长时间更新间隔，因为如果CPU长期不更新时间，将会导致再次获取到的cycle计数值过大，使得转换成纳秒时发生溢出错误。从理论计算上说，将cycle转换成纳秒的公式是”cycle * 每秒纳秒数 / 频率”，但是由于内核无法进行浮点运算，只能通过一种变通的方法来计算，即”cycle * mult » shift”，这里的mult和shif就是基于频率、计算精度和最大表示范围计算而得的。&lt;/p&gt;

&lt;h3 id=&quot;4-计时初始化&quot;&gt;4. 计时初始化&lt;/h3&gt;

&lt;p&gt;  最后我们再来看看内核计时功能的初始化过程，了解一下计时功能是如何一步步生效的。timekeeper的初始化是在内核启动过程start_kernel中调用timekeeping_init进行：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/*
 * timekeeping_init - Initializes the clocksource and common timekeeping values
 */
void __init timekeeping_init(void)
{
    struct timekeeper *tk = &amp;amp;timekeeper;
    struct clocksource *clock;
    unsigned long flags;
    struct timespec now, boot, tmp;

    /*x86架构下，persistent clock为系统RTC时钟源，我们先从中获取当前时间，精度为秒*/
    read_persistent_clock(&amp;amp;now);

    if (!timespec_valid_strict(&amp;amp;now)) {
        pr_warn(&quot;WARNING: Persistent clock returned invalid value!\n&quot;
                &quot;         Check your CMOS/BIOS settings.\n&quot;);
        now.tv_sec = 0;
        now.tv_nsec = 0;
    } else if (now.tv_sec || now.tv_nsec)
        persistent_clock_exist = true;

    /*x86架构下，没有boot clock，所以boot time为0*/
    read_boot_clock(&amp;amp;boot);
    if (!timespec_valid_strict(&amp;amp;boot)) {
        pr_warn(&quot;WARNING: Boot clock returned invalid value!\n&quot;
                &quot;         Check your CMOS/BIOS settings.\n&quot;);
        boot.tv_sec = 0;
        boot.tv_nsec = 0;
    }

    raw_spin_lock_irqsave(&amp;amp;timekeeper_lock, flags);
    write_seqcount_begin(&amp;amp;timekeeper_seq);
    ntp_init();

    /*获取系统默认时钟源，x86架构中即为jiffies*/
    clock = clocksource_default_clock();
    if (clock-&amp;gt;enable)
    clock-&amp;gt;enable(clock);
    /*将jiffy设备timekeeper中的时钟源并设定内部相关变量*/
    tk_setup_internals(tk, clock);

    /*将当前时间设定为tk的墙上时间，注其中tk-&amp;gt;xtime_sec为当前秒数，tk-&amp;gt;xtime_nsec为纳秒左移shift位后的值*/
    tk_set_xtime(tk, &amp;amp;now);
    /*raw_time设为0*/
    tk-&amp;gt;raw_time.tv_sec = 0;
    tk-&amp;gt;raw_time.tv_nsec = 0;
    /*boot time设为墙上时间*/
    if (boot.tv_sec == 0 &amp;amp;&amp;amp; boot.tv_nsec == 0)
        boot = tk_xtime(tk);

    /*将monotonic time减去wall time的时间偏移记录下来*/
    set_normalized_timespec(&amp;amp;tmp, -boot.tv_sec, -boot.tv_nsec);
    tk_set_wall_to_mono(tk, tmp);

    /*将sleep time初始化为零*/
    tmp.tv_sec = 0;
    tmp.tv_nsec = 0;
    tk_set_sleep_time(tk, tmp);

    /*备份当前timekeeper*/
    memcpy(&amp;amp;shadow_timekeeper, &amp;amp;timekeeper, sizeof(timekeeper));

    write_seqcount_end(&amp;amp;timekeeper_seq);
    raw_spin_unlock_irqrestore(&amp;amp;timekeeper_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  内核jiffies变量代表时间滴答，是对同期性tick事件的记数，因此可以将它视为一个最为简单的时钟源。它和一般时钟源的不同之处在于它没有实际的计时设备与之对应，完全是记录在计算机内存中；另外它的精度和系统tick数相关。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/jiffies.c:

struct clocksource * __init __weak clocksource_default_clock(void)
{
    return &amp;amp;clocksource_jiffies;
}

static struct clocksource clocksource_jiffies = {
    .name		= &quot;jiffies&quot;,
    .rating		= 1, /* lowest valid rating*/
    .read		= jiffies_read,
    .mask		= 0xffffffff, /*32bits*/
    .mult		= NSEC_PER_JIFFY &amp;lt;&amp;lt; JIFFIES_SHIFT, /* details above */
    .shift		= JIFFIES_SHIFT,
};

static cycle_t jiffies_read(struct clocksource *cs)
{
    return (cycle_t) jiffies;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于tsc时钟源，在内核对模块进行初始化时，会注册tsc时钟，并通知timekeeper将时钟源从jiffies切换到tsc:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int __init init_tsc_clocksource(void)
{
    ...
    if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE)) {
        /*注册一个频率为tsc_khz的时钟源，最终调用__clocksource_register_scale实现*/
        clocksource_register_khz(&amp;amp;clocksource_tsc, tsc_khz);
    return 0;
    }
    ...
}
/*
 * We use device_initcall here, to ensure we run after the hpet
 * is fully initialized, which may occur at fs_initcall time.
 */
device_initcall(init_tsc_clocksource);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/clocksource.c:

/**
 * __clocksource_register_scale - Used to install new clocksources
 * @cs:		clocksource to be registered
 * @scale:	Scale factor multiplied against freq to get clocksource hz
 * @freq:	clocksource frequency (cycles per second) divided by scale
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 *
 * This *SHOULD NOT* be called directly! Please use the
 * clocksource_register_hz() or clocksource_register_khz helper functions.
 */
int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
{

    /*首先根据时钟源的频率计算合适的mult和shift，以及最大更新延时max_idle_ns*/
    /* Initialize mult/shift and max_idle_ns */
    __clocksource_updatefreq_scale(cs, scale, freq);

    /* Add clocksource to the clcoksource list */
    mutex_lock(&amp;amp;clocksource_mutex);
    clocksource_enqueue(cs); /*加入到全局clocksource_list*/
    clocksource_enqueue_watchdog(cs); /*加入到时钟源监控中，如果发现当前时钟源精度下降会重新选择更优的时钟源*/
    clocksource_select(); /*选择最优的时钟源，内部会调用timekeeping_notify通知timerkeeper*/
    mutex_unlock(&amp;amp;clocksource_mutex);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/**
* timekeeping_notify - Install a new clock source
* @clock:		pointer to the clock source
*
* This function is called from clocksource.c after a new, better clock
* source has been registered. The caller holds the clocksource_mutex.
*/
void timekeeping_notify(struct clocksource *clock)
{
    struct timekeeper *tk = &amp;amp;timekeeper;

    if (tk-&amp;gt;clock == clock)
        return;
    /*注意，这里会暂停所有CPU的运行，并选定一个默认的CPU(0号核)执行change_clocksource。
      这是因为时钟源是计时的基础，在进行时钟源切换时系统将无法提供正确的时间服务。只有当切换
      完成后系统才可恢复运行。*/
    stop_machine(change_clocksource, clock, NULL);
    tick_clock_notify();
}

/**
 * change_clocksource - Swaps clocksources if a new one is available
 *
 * Accumulates current time interval and initializes new clocksource
 */
static int change_clocksource(void *data)
{
    struct timekeeper *tk = &amp;amp;timekeeper;
    struct clocksource *new, *old;
    unsigned long flags;

    new = (struct clocksource *) data;

    raw_spin_lock_irqsave(&amp;amp;timekeeper_lock, flags);
    write_seqcount_begin(&amp;amp;timekeeper_seq);

    timekeeping_forward_now(tk);
    if (!new-&amp;gt;enable || new-&amp;gt;enable(new) == 0) {
        old = tk-&amp;gt;clock;
        tk_setup_internals(tk, new);
        if (old-&amp;gt;disable)
            old-&amp;gt;disable(old);
    }
    timekeeping_update(tk, true, true);

    write_seqcount_end(&amp;amp;timekeeper_seq);
    raw_spin_unlock_irqrestore(&amp;amp;timekeeper_lock, flags);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/计时/&quot;&gt;【时间子系统】二、计时原理－timekeeper与clocksource&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%AE%A1%E6%97%B6/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%AE%A1%E6%97%B6/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】一、概述</title>
        <description>&lt;p&gt;  除了计算、存储、网络等核心子系统外，计算机内部还包含时间子系统，它对系统的运行起着重要作用。&lt;/p&gt;

&lt;h3 id=&quot;什么是计算机的时间子系统&quot;&gt;什么是计算机的时间子系统？&lt;/h3&gt;

&lt;p&gt;  计算机内的时间子系统包含多种时间设备，我们可以把这些设备分为两大类：&lt;strong&gt;计时&lt;/strong&gt;设备和&lt;strong&gt;定时通知&lt;/strong&gt;设备。&lt;/p&gt;

&lt;p&gt;  我们可以将计时设备理解为生活中所见的”墙上挂钟”或者”手表”，它们为系统提供了时间(时刻)。常见的计时设备有TSC(Time Stamp Counter)、RTC(Real Time Clock)、ACPI_PM：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;TSC是每个CPU内部的一个计数器，它按CPU主频以固定频率递增，例如一个2400Mhz的CPU，计数器每秒逐一增加2400M个计数值。计数器在CPU启动时初始化为零，假设我们已知CPU启动时刻，那么只要把当前计算器的值除以频率再加上启动时刻，就可以得知当前时间了。&lt;/li&gt;
    &lt;li&gt;RTC是位于CMOS电路中的一个计时设备，它与TSC相比的优点是有独立的电池供电，即使计算机下电，RTC计时器仍可以继续工作；缺点是计数频率较低，因此时间精度较差。通常我们初始时将RTC计数器的值设置为当前时刻相对于1970年1月1日的时间差，因此通过RTC提供的寄存器接口，我们可以直接获取到当前时间。&lt;/li&gt;
    &lt;li&gt;ACPI_PM通常是南桥中的APCI电源管理模块提供的计时设备，其精度较低，通常不推荐使用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  我们可以将定时通知设备理解为生活中所见的”闹钟”，它们周期性地或者在一定时间间隔后向系统通知到期事件。常见的通知设备有Local APIC Timer、PIT(Programmable Interval Timer)、HPET(High Precision Event Timer)。初始时我们向时间通知设备的计数器中写入一个到期计数值，然后时间通知设备按固定频率递减计数器中的值，当计数器值为零时便通过中断向CPU通知事件发生。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Local APIC Timer是每个CPU的本地中断控制器(APIC)内部的定时设备，精度较高，是系统正常运行时采用的通知设备。&lt;/li&gt;
    &lt;li&gt;PIT是CPU之外的独立定时通知设备，属全局设备，精度较低，通常不使用。&lt;/li&gt;
    &lt;li&gt;HPET也是全局定时通知设备，精度较高，需要系统中含专属硬件。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  计算机用户是无法直接使用这些时间设备的，必须通过运行在CPU上的内核程序，应用程序或者用户才能最终获取计时和时间通知服务。因此我们可以将计算机时间子系统分为硬件和软件两部分：各种时间设备属于硬件部分，内核使能这些硬件的模块(也称内核时间子系统)属于软件部分。内核中将计时设备称为&lt;strong&gt;时钟源(Clock Source)&lt;/strong&gt;，将定时通知设备称为&lt;strong&gt;时钟事件设备(Clock Event Device)&lt;/strong&gt;。时间子系统整体结构如下图所示(感谢droidphone的分享，&lt;a href=&quot;http://blog.csdn.net/droidphone/article/details/8017604&quot;&gt;原文链接&lt;/a&gt;)，后续我们将深入内核分析其实现原理。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/time_1.jpg&quot; height=&quot;280&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;h3 id=&quot;为什么需要时间子系统&quot;&gt;为什么需要时间子系统？&lt;/h3&gt;

&lt;p&gt;  基于各种时间设备和内核时间子系统模块，应用程序可以实现计时和到期通知(&lt;strong&gt;定时器&lt;/strong&gt;)功能：例如我们桌面应用中的日历程序就使用了计时功能来提供实时时间，又例如邮件客户端的定时接收功能就使用了定时器。此外，内核自己的进程调度功能也依赖于内核的定时器，从而进行周期性的调度决策。&lt;/p&gt;

&lt;p&gt;  下面我们给出了两段示例程序来展示时间子系统的基本使用方法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//用户态计时功能示例程序

#include&amp;lt;stdio.h&amp;gt;
#include&amp;lt;sys/time.h&amp;gt;
#include&amp;lt;unistd.h&amp;gt;

int main()
{
    struct  timeval    tv;
    struct  timezone   tz;

    gettimeofday(&amp;amp;tv,&amp;amp;tz); /*获取当前时间*/

    printf(“tv_sec:%d\n”,tv.tv_sec);
    printf(“tv_usec:%d\n”,tv.tv_usec);

    printf(“tz_minuteswest:%d\n”,tz.tz_minuteswest);
    printf(“tz_dsttime:%d\n”,tz.tz_dsttime);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//用户态定时器参考setitime，这里以内核使用定时器示例

/*1. 初始化定时器结构*/
init_timer(&amp;amp;wb_timer);

/*2. 定时器超时函数*/
wb_timer.function = wb_timer_function; 

/*3.或者初始化定时器和超时函数作为一步(data作为fn的参数)*/
setup_timer(timer, fn, data)    

/*4. 添加定时器*/
add_timer(&amp;amp;buttons_timer); 

/*5. 设置定时器超时时间 1\100 s（修改一次超时时间只会触发一次定时器*/
mod_timer(&amp;amp;buttons_timer, jiffies+HZ/100); 

/*6.删除定时器*/
del_timer(&amp;amp;timer);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;内核如何实现时间子系统&quot;&gt;内核如何实现时间子系统？&lt;/h3&gt;

&lt;p&gt;  后续我们将以一系统博文深入分析内核时间子系统的实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/01/计时/&quot;&gt;计时原理－timekeeper与clocksource&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/时钟中断/&quot;&gt;时钟中断－定时基础&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;低精度定时器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;高精度定时器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;动态时钟－nohz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/时间子系统概述/&quot;&gt;【时间子系统】一、概述&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 19 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E6%97%B6%E9%97%B4%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E6%97%B6%E9%97%B4%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【计算子系统】进程管理之二：进程替换</title>
        <description>&lt;p&gt;  本篇讨论进程替换(exec)，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是进程替换为什么需要它&quot;&gt;什么是进程替换？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  进程替换是用新的代码和数据替换当前进程已有代码和数据，从而开始执行新的业务逻辑。&lt;/p&gt;

&lt;p&gt;  通过fork创建出来的进程是继承父进程的代码和数据，如果想要进程执行一些新的任务，那就得从磁盘程序中加载新的代码和数据并替换当前进程已有的代码和数据。&lt;/p&gt;

&lt;h3 id=&quot;如何实现进程替换&quot;&gt;如何实现进程替换？&lt;/h3&gt;

&lt;h4 id=&quot;1-exec用户态示例代码&quot;&gt;&lt;strong&gt;1. exec用户态示例代码&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来看看在用户态程序中是如何实现替换的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exec_test.c:

#include &amp;lt;stdio.h&amp;gt;  
#include &amp;lt;unistd.h&amp;gt;  

/*示例函数fork一个新进程并在其中执行ps命令*/
int main()  
{
    /*构造命令行参数ps_argv和环境变量ps_envp*/
    char *const ps_argv[] ={&quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL};  
    char *const ps_envp[] ={&quot;PATH=/bin:/usr/bin&quot;, &quot;TERM=console&quot;, NULL};

    if(fork()==0){ 
        /*执行execve系统调用，第一个参数表示可执行文件的位置，第二个表示命令行参数，第三个表示环境变量。
          这里注意exec是一个函数簇，其有6种类似的系统调用：
          (1)6种调用的前4个字符相同，均是exec;
          (2)第5位有v和l两种，v表示向量表示法，l表示逐个列举;
          (3)第6位有e和p两种，e表示带环境变量，p表示可执行程序以文件名方式查找，而不是路径查找;*/
        if(execve(&quot;/bin/ps&quot;, ps_argv, ps_envp) &amp;lt; 0)  {  
            perror(&quot;execve error!&quot;);  
            return -1 ;  
        }  
    }  
    return 0 ;  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-elf可执行文件格式解析&quot;&gt;&lt;strong&gt;2. ELF可执行文件格式解析&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  通过上面的例子，我们看到应用程序通过调用execve系统调用实现执行程序的替换。Linux平台上主流的可执行文件格式是ELF(Executable and Linkable Format，类似Windows平台上的exe文件格式)，如果想深入分析execve调用功能，那我们就得了解ELF文件格式，详细规范说明&lt;a href=&quot;http://www.skyfree.org/linux/references/ELF_Format.pdf&quot;&gt;点此&lt;/a&gt;进入。&lt;/p&gt;

&lt;h5 id=&quot;21-示例程序&quot;&gt;&lt;strong&gt;2.1. 示例程序&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  真正理解ELF格式后，我们能够将C语言源文件与ELF二进制文件进行对应，这样可以提升对底层系统问题的定位能力。因此为方便大家入门，这里以一个简单的C语言程序为例，来逐一对应ELF中的各部分：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/tmp_analyze_elf/main.c:

#include &amp;lt;stdio.h&amp;gt;

void say_hello(char *who)
{
    printf(&quot;hello, %s!\n&quot;, who);
}

char *my_name = &quot;wb&quot;;

int main()
{
    say_hello(my_name);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们将其编译生成名为app的可执行程序并运行：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;linux-XqAfQZ:~/tmp_analyze_elf # &lt;strong&gt;gcc -o&lt;/strong&gt; app main.c&lt;br /&gt;
linux-XqAfQZ:~/tmp_analyze_elf # &lt;strong&gt;./app&lt;/strong&gt;&lt;br /&gt;
hello, wb!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;22-elf整体布局&quot;&gt;&lt;strong&gt;2.2. ELF整体布局&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  ELF格式可以表达三种类型的二进制对象文件(object files)：可重定位文件(relocatable file，就是大家平常见到的.o文件)、可执行文件(executable file, 例上述示例代码生成的app文件)、共享库文件(shared object files，就是.so文件，用来做动态链接)。可重定位文件用在编译和链接阶段；可执行文件用在程序运行阶段；共享库则同时用在编译链接和运行阶段。在不同阶段，我们可以用不同视角来理解ELF文件，整体布局如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_1.jpg&quot; height=&quot;300&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从上图可见，ELF格式文件整体可分为四大部分：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;ELF Header, ELF头部，定义全局性信息；&lt;/li&gt;
    &lt;li&gt;Program Header Table， 描述段(Segment)信息的数组，每个元素对应一个段；通常包含在可执行文件中，可重定文件中可选(通常不包含)；&lt;/li&gt;
    &lt;li&gt;Segment and Section，段(Segment)由若干区(Section)组成；段在运行时被加载到进程地址空间中，包含在可执行文件中；区是段的组成单元，包含在可执行文件和可重定位文件中；&lt;/li&gt;
    &lt;li&gt;Section Header Table，描述区(Section)信息的数组，每个元素对应一个区；通常包含在可重定位文件中，可执行文件中为可选(通常包含)；&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;23-elf-header实例解析&quot;&gt;&lt;strong&gt;2.3. ELF Header实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  ELF规范中对ELF Header中各字段的定义如下所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_2.jpg&quot; height=&quot;300&quot; width=&quot;350&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  接下来我们通过readelf -h命令来看看示例程序app中的ELF Header内容，显示结果如下图：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-header.jpg&quot; height=&quot;380&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;e_ident&lt;/strong&gt;含前16个字节，又可细分成class、data、version等字段，具体含义不用太关心，只需知道前4个字节点包含”ELF”关键字，这样可以判断当前文件是否是ELF格式；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_type&lt;/strong&gt;表示具体ELF类型，可重定位文件/可执行文件/共享库文件，显然这里是一个可执行文件；&lt;strong&gt;e_machine&lt;/strong&gt;表示执行的机器平台，这里是x86_64；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_version&lt;/strong&gt;表示文件版本号，这里的1表示初始版本号；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_entry&lt;/strong&gt;对应”Entry point address”，程序入口函数地址，通过进程虚拟地址空间地址(0x400440)表达；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phoff&lt;/strong&gt;对应“Start of program headers”，表示program header table在文件内的偏移位置，这里是从第64号字节(假设初始为0号字节)开始；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shoff&lt;/strong&gt;对应”Start of section headers”，表示section header table在文件内的偏移位置，这里是从第4472号字节开始，靠近文件尾部；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_flags&lt;/strong&gt;表示与CPU处理器架构相关的信息，这里为零；&lt;strong&gt;e_ehsize&lt;/strong&gt;对应”Size of this header”，表示本ELF header自身的长度，这里为64个字节，回看前面的e_phoff为64，说明ELF header后紧跟着program header table；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phentsize&lt;/strong&gt;对应“Size of program headers”，表示program header table中每个元素的大小，这里为56个字节；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phnum&lt;/strong&gt;对应”Number of program headers”，表示program header table中元素个数，这里为9个；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shentsize&lt;/strong&gt;对应”Size of section headers”，表示section header table中每个元素的大小，这里为64个字节；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shnum&lt;/strong&gt;对应”Number of section headers”，表示section header table中元素的个数，这里为30个；&lt;/li&gt;
    &lt;li&gt;最后， &lt;strong&gt;e_shstrndx&lt;/strong&gt;对应”Section header string table index”，表示描述各section字符名称的string table在section header table中的下标，详见后文对string table的介绍。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;24-program-header-table实例解析&quot;&gt;&lt;strong&gt;2.4. Program Header Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  Program Header Table是一个数组，每个元素叫Program Header，规范对其结构定义如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_3.jpg&quot; height=&quot;200&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  同样我们用readelf -l命令查看示例程序的program header table：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-program-header1.jpg&quot; height=&quot;500&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  上图截取了readelf命令返回的上半部，我们重点看下前面几项：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;PHDR&lt;/strong&gt;，此类型header元素描述了program header table自身的信息。从这里的内容看出，示例程序的program header table在文件中的偏移(Offset)为0x40，即64号字节处；该段映射到进程空间的虚拟地址(VirtAddr)为0x400040；PhysAddr暂时不用，其保持和VirtAddr一致；该段占用的文件大小FileSiz为00x1f8；运行时占用进程空间内存大小MemSiz也为0x1f8；Flags标记表示该段的读写权限，这里”R E”表示可读可执行，说明本段属于代码段；Align对齐为8，表明本段按8字节对齐。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;INTERP&lt;/strong&gt;，此类型header元素描述了一个特殊内存段，该段内存记录了动态加载解析器的访问路径字符串。示例程序中，该段内存位于文件偏移0x238处，即紧跟program header table；映射的进程虚拟地址空间地址为0x400238；文件长度和内存映射长度均为0x1c，即28个字符，具体内容为”/lib64/ld-linux-x86-64.so.2”；段属性为只读，并按字节对齐；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;LOAD&lt;/strong&gt;，此类型header元素描述了可加载到进程空间的代码段或数据段：第三项为代码段，文件内偏移为0，映射到进程地址0x400000处，代码段长度为0x764个字节，属性为只读可执行，段地址按2M边界对齐；第四段为数据段，文件内偏移为0xe10，映射到进程地址为0x600e10处(按2M对齐移动)，文件大小为0x230，内存大小为0x238(因为其内部包含了8字节的bss段，即未初始化数据段，该段内容不占文件空间，但在运行时需要为其分配空间并清零)，属性为读写，段地址也按2M边界对齐。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;DYNAMIC&lt;/strong&gt;，此类型header元素描述了动态加载段，其内部通常包含了一个名为”.dynamic”的动态加载区；这也是一个数组，每个元素描述了与动态加载相关的各方面信息，我们将在动态加载中介绍。该段是从文件偏移0xe28处开始，长度为0x1d0，并映射到进程的0x600e28；可见该段和上一个数据段是有重叠的。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  readelf命令返回内容的下半部分给出了各段(segment)和各区(section)之间的包含关系，如下图所示。INTERP段只包含了”.interp”区；代码段包含”.interp”、”.plt”、”.text”等区；数据段包含”.dynamic”、”.data”、”.bss”等区；DYNAMIC段包含”.dynamic”区。从这里可以看出，有些区被包含在多个段中。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-program-header2.jpg&quot; height=&quot;100&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;25-section-header-table实例解析&quot;&gt;&lt;strong&gt;2.5. Section Header Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  针对各区的描述信息由Section Header Table提供，该数组中每个元素的定义如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_4.jpg&quot; height=&quot;200&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  下面我们再通过readelf -S命令看看示例程序中section header table的内容，如下图所示。示例程序共生成30个区，Name表示每个区的名字，Type表示每个区的功能，Address表示每个区的进程映射地址，Offset表示文件内偏移，Size表示区的大小，EntSize表示区中每个元素的大小(如果该区为一个数组的话，否则该值为0)，Flags表示每个区的属性(参见图中最后的说明)，Link和Info记录不同类型区的相关信息(不同类型含义不同，具体参见规范)，Align表示区的对齐单位。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-section-header1.jpg&quot; height=&quot;600&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-section-header2.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;26-string-table实例解析&quot;&gt;&lt;strong&gt;2.6. String Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  从上述Section Header Table示例中，我们看到有一种类型为STRTAB的区(在Section Header Table中的下标为6,27,29)。此类区叫做String Table，其作用是集中记录字符串信息，其它区在需要使用字符串的时候，只需要记录字符串起始地址在该String Table表中的偏移即可，而无需包含整个字符串内容。&lt;/p&gt;

&lt;p&gt;  我们使用readelf -x读出下标27区的详细内容观察：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-strtable1.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  红框内为该区实际内容，左侧为区内偏移地址，后侧为对应内容的字符表示。我们可以发现，这里其实是一堆字符串，这些字符串对应的就是各个区的名字。因此section header table中每个元素的Name字段其实是这个string table的索引。再回头看看ELF header中的e_shstrndx，它的值正好就是27，指向了当前的string table。&lt;/p&gt;

&lt;p&gt;  同理再来看下29区的内容，如下图所示。这里我们看到了”main”、”say_hello”字符串，这些是我们在示例中源码中定义的符号，由此可以29区是应用自身的String Table，记录了应用使用的字符串。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-strtable2.jpg&quot; height=&quot;700&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;27-symbol-table实例解析&quot;&gt;&lt;strong&gt;2.7. Symbol Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  Section Header Table中，还有一类SYMTAB(DYNSYM)区，该区叫符号表。符号表中的每个元素对应一个符号，记录了每个符号对应的实际数值信息，通常用在重定位过程中或问题定位过程中，进程执行阶段并不加载符号表。符号表中每个元素定义如下：name表示符号对应的源码字符串，为对应String Table中的索引；value表示符号对应的数值；size表示符号对应数值的空间占用大小；info表示符号的相关信息，如符号类型(变量符号、函数符号)；shndx表示与该符号相关的区的索引，例如函数符号与对应的代码区相关。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_5.jpg&quot; height=&quot;160&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们用readelf -s读出示例程序中的符号表，如下图所示。如红框中内容所示，我们示例程序定义的main函数符号对应的数值为0x400554，其类型为FUNC，大小为26字节，对应的代码区在Section Header Table中的索引为13；say_hello函数符号对应数值为0x400530，其类型为FUNC，大小为36字节，对应的代码区也为13。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-symtable1.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-symtable2.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;28-代码段实例解析&quot;&gt;&lt;strong&gt;2.8. 代码段实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  在理解了String Table和Symbol Table的作用后，我们通过objdump反汇编来理解一下.text代码段：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-main-code.jpg&quot; height=&quot;300&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  这里截取了与示例程序相关部分，我们看到0x400530和0x400554两处各定义一个函数，其符号分别为say_hello和main，这部分信息实际是通过符号表解析而来的；在涉及到内存地址的指令中，除了对数据段地址的引用是通过绝对地址进行的之外，对于代码段地址的引用都是以相对地址的方式进行的，这样做的好处是在二进制文件的重定位过程中，我们不用修改指令的访问地址(因为相对地址保持不变)；最后，我们看到对于库函数printf的访问指向了代码段地址0x400410，那么这个地址处放的是printf函数么？要回答这个问题就涉及动态链接，我们将在下文专题分析。&lt;/p&gt;

&lt;h5 id=&quot;29-动态链接dynamic-linking&quot;&gt;&lt;strong&gt;2.9. 动态链接(Dynamic Linking)&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  基于模块化设计思路，我们在应用开发时会将基础的、共用的功能抽取出来，设计成可共享的库。应用程序在编译时只是建立了与共享库的联系，并不将其包含在内；运行时，由系统负责加载所需的共享库。这就是动态链接，如此一来，既可以节省磁盘和内存的空间占用(相同功能在磁盘和内存中均只存在一份)，也可以方便基础模块自身的优化改进。&lt;/p&gt;

&lt;p&gt;  要实现动态链接，需要解决两个大的问题：(1)共享库内部的函数的地址访问需要与加载地址无关，因为不同的程序可能将库加载到不同的地址处；回顾2.8中的代码分析，我们看到这个可以通过相对寻址的方式解决。(2)调用共享库的应用程序如何能够获知共享库的加载地址并准确对其进行调用？&lt;/p&gt;

&lt;p&gt;  ELF规范对问题2的解决方法给出明确思路：系统中需要有一个Program Interpreter配合内核完成进程执行上下文的准备。Program Interpreter可以是一个可执行程序，也可以是一个共享库；在Linux x86_64平台下，这个解析器就是/lib64/ld-ld-linux-x86-64.so.2，就是由INTERP段指明的。解析器和内核需要配合完成以下动作：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;内核加载可执行程序和解析器到进程空间，之后将控制权交给解析器；&lt;/li&gt;
    &lt;li&gt;解析器加载共享库到进程空间；&lt;/li&gt;
    &lt;li&gt;解析器进行重定位；&lt;/li&gt;
    &lt;li&gt;解析器将控制权交给进程正常执行。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  解析器工作时需要从DYNAMIC段中获取信息，并通过GOT(Global Offset Table)记录解析后的库函数地址；应用程序通过PLT(Procedure Linkage Table)中的代码间接访问GOT，并最终完成向目标库函数的跳转。&lt;/p&gt;

&lt;p&gt;  结合我们的示例程序，我们通过readelf -d获取DYNAMIC段中的信息，如下图所示。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  重点看一下红框中标出的两行，NEEDED表示当前可执行程序依赖的共享库，这里只有libc.so.6一项；PLTGOT表示当前程序调用共享库时使用的GOT表的地址为0x601000，回看前文的Section Header Table，可知对应区的索引为23。我们接着可以看看该区的内容：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic-got.jpg&quot; height=&quot;100&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从23区的section header中可知该区为一个数组，每个元素大小为8字节。结合规范中的说明可知，GOT中的前三个元素有着特殊作用：第一个元素转换成地址为0x600e28，即为DYNAMIC段的映射地址；第二元素和第三个元素会在解析器获得控制权后被放置动态解析函数的参数和入口地址，其作用将在后续说明PLT功能是说明。从第四个元素起，每个元素代表一个调用地址，依次为0x400416、0x400426、0x400436，这些地址对应什么内容呢？&lt;/p&gt;

&lt;p&gt;  我们通过objdump来看看.plt段的内容：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic-plt.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  plt包含几段相似的汇编指令，回顾前文，.text代码段的say_hello在调用printf时访问的函数地址即为0x40010，对应plt中第二段汇编的第一条指令。这是一条jump指令，跳转到0x601018即GOT表的第4个元素。前文分析时指出，GOT中第4个元素为0x400416，正好对应jump指令的下一条指令。感觉上转了一圈却只是跳到了下一条指令处，有点多余，那么我们接着往下分析。后续的push指令把0压入了栈中(代表每个调用函数的索引，这里printf索引值为0),然后跳转到plt表中的第一段汇编指令处。这里把GOT表的第二个元素压力栈中，然后跳转到GOT表中第三个元素指定的函数。这个函数是解析器的动态解析函数，它的作用是针对栈中的调用函数索引，找到调用函数的实际加载地址，并将该地址填入GOT表中对应的元素位置(这里就是将printf的实际加载地址填入0x601018处(即GOT表中的第4个元素))，然后跳转到printf处执行。当应用程序再次调用printf时，跳转到plt中对应的函数后，那里的jump指令将根据GOT表中被解析器更新后的地址直接跳转到printf处开始执行，这样就不用解析器的干预了，从而达到了动态跳转的目的。&lt;/p&gt;

&lt;p&gt;  我们对动态调用做个总结：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;需要进行动态调用的可执行程序在编译时会自动生成DYNAMIC段、GOT表和PLT表；&lt;/li&gt;
    &lt;li&gt;对动态库的每个函数调用都会在GOT(从第4个元素开始)和PLT(从第二段汇编指令开始)中生成一项；&lt;/li&gt;
    &lt;li&gt;解析器在获得控制权后会在GOT第2个元素和第3个元素放置解析函数的参数和入口地址；PLT的第一段汇编指令会将GOT第2个元素压栈并跳转到第3个元素指定的函数位置；&lt;/li&gt;
    &lt;li&gt;进行过一次动态调用后，GOT中对应的元素中就记录了库函数的实际加载地址，后续的调用就可以进行直接跳转。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;3-深入内核sys_execve&quot;&gt;&lt;strong&gt;3. 深入内核sys_execve&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们在2.9节中介绍过，内核和解析器相互配合完了进程执行空间的替换，下面我们深入内核代码来看看sys_execve的实现原理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/fs/exec.c:

SYSCALL_DEFINE3(execve,
        const char __user *, filename,
        const char __user *const __user *, argv,
        const char __user *const __user *, envp)
{
    struct filename *path = getname(filename);
    int error = PTR_ERR(path);
    if (!IS_ERR(path)) {
        error = do_execve(path-&amp;gt;name, argv, envp);
        putname(path);
    }
    return error;
}

int do_execve(const char *filename,
            const char __user *const __user *__argv,
    const char __user *const __user *__envp)
{
    struct user_arg_ptr argv = { .ptr.native = __argv };
    struct user_arg_ptr envp = { .ptr.native = __envp };
    return do_execve_common(filename, argv, envp);
}

/*
 * sys_execve() executes a new program.
 */
static int do_execve_common(const char *filename,
                    struct user_arg_ptr argv,
                    struct user_arg_ptr envp)
{
    struct linux_binprm *bprm;
    struct file *file;
    struct files_struct *displaced;
    bool clear_in_exec;
    int retval;
    const struct cred *cred = current_cred();

    ...

    retval = -ENOMEM;
    bprm = kzalloc(sizeof(*bprm), GFP_KERNEL);
    if (!bprm)
        goto out_files;

    ...

    /*打开可执行文件*/
    file = open_exec(filename);
    ...

    bprm-&amp;gt;file = file;
    bprm-&amp;gt;filename = filename;
    bprm-&amp;gt;interp = filename;

    /*初始化将替换当前进程的mm_struct*/
    retval = bprm_mm_init(bprm);
    ...

    /*计算命令行参数和环境变量个数*/
    bprm-&amp;gt;argc = count(argv, MAX_ARG_STRINGS);
    ...
    bprm-&amp;gt;envc = count(envp, MAX_ARG_STRINGS);
    ...

    /*准备bprm中相关信息并读取可执行文件头部*/
    retval = prepare_binprm(bprm);
    ...

    /*复制信息到用户栈空间*/
    retval = copy_strings_kernel(1, &amp;amp;bprm-&amp;gt;filename, bprm);
    ...
    bprm-&amp;gt;exec = bprm-&amp;gt;p;
    retval = copy_strings(bprm-&amp;gt;envc, envp, bprm);
    ...
    retval = copy_strings(bprm-&amp;gt;argc, argv, bprm);
    ...

    /*在内核中搜索能够加载当前可执行文件的二进制解析驱动，这里是elf驱动，
      最终会调用elf_load_binary*/
    retval = search_binary_handler(bprm);
    ...

    return retval;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/fs/binfmt_elf.c:

static int load_elf_binary(struct linux_binprm *bprm)
{
    struct file *interpreter = NULL; /* to shut gcc up */
    unsigned long load_addr = 0, load_bias = 0;
    int load_addr_set = 0;
    char * elf_interpreter = NULL;
    unsigned long error;
    struct elf_phdr *elf_ppnt, *elf_phdata;
    unsigned long elf_bss, elf_brk;
    int retval, i;
    unsigned int size;
    unsigned long elf_entry;
    unsigned long interp_load_addr = 0;
    unsigned long start_code, end_code, start_data, end_data;
    unsigned long reloc_func_desc __maybe_unused = 0;
    int executable_stack = EXSTACK_DEFAULT;
    unsigned long def_flags = 0;
    struct pt_regs *regs = current_pt_regs();
    struct {
        struct elfhdr elf_ex; /*记录当前可执行程序的elf header*/
        struct elfhdr interp_elf_ex; /*记录程序解析器的elf header*/
    } *loc;

    loc = kmalloc(sizeof(*loc), GFP_KERNEL);
    if (!loc) {
        retval = -ENOMEM;
        goto out_ret;
    }

    /* Get the exec-header */
    loc-&amp;gt;elf_ex = *((struct elfhdr *)bprm-&amp;gt;buf); /*当前可执行程序头部128字节已经读到buff中*/

    retval = -ENOEXEC;
    /* First of all, some simple consistency checks */
    if (memcmp(loc-&amp;gt;elf_ex.e_ident, ELFMAG, SELFMAG) != 0) /*较验头部魔术字*/
        goto out;

    if (loc-&amp;gt;elf_ex.e_type != ET_EXEC &amp;amp;&amp;amp; loc-&amp;gt;elf_ex.e_type != ET_DYN) /*校验可执行文件类型*/
        goto out;
    ...
    if (!bprm-&amp;gt;file-&amp;gt;f_op || !bprm-&amp;gt;file-&amp;gt;f_op-&amp;gt;mmap)
        goto out;

    /* Now read in all of the header information */
    if (loc-&amp;gt;elf_ex.e_phentsize != sizeof(struct elf_phdr)) /*校验program header大小*/
        goto out;
    if (loc-&amp;gt;elf_ex.e_phnum &amp;lt; 1 ||
            loc-&amp;gt;elf_ex.e_phnum &amp;gt; 65536U / sizeof(struct elf_phdr)) /*校验program header 个数*/
        goto out;
    size = loc-&amp;gt;elf_ex.e_phnum * sizeof(struct elf_phdr);
    retval = -ENOMEM;
    elf_phdata = kmalloc(size, GFP_KERNEL);
    if (!elf_phdata)
        goto out;

    /*根据e_phoff记录的文件偏移读取program header table*/
    retval = kernel_read(bprm-&amp;gt;file, loc-&amp;gt;elf_ex.e_phoff,
            (char *)elf_phdata, size);
    if (retval != size) {
        if (retval &amp;gt;= 0)
            retval = -EIO;
        goto out_free_ph;
    }

    elf_ppnt = elf_phdata;
    elf_bss = 0;
    elf_brk = 0;

    start_code = ~0UL;
    end_code = 0;
    start_data = 0;
    end_data = 0;

    /*从program header table中找出INTERP段获取程序解析器的访问路径并加载其头部*/
    for (i = 0; i &amp;lt; loc-&amp;gt;elf_ex.e_phnum; i++) {
        if (elf_ppnt-&amp;gt;p_type == PT_INTERP) {
            ...
            elf_interpreter = kmalloc(elf_ppnt-&amp;gt;p_filesz, GFP_KERNEL);
            ...
            retval = kernel_read(bprm-&amp;gt;file, elf_ppnt-&amp;gt;p_offset, elf_interpreter,
                elf_ppnt-&amp;gt;p_filesz);
            ...
            interpreter = open_exec(elf_interpreter);
            ...
            retval = kernel_read(interpreter, 0, bprm-&amp;gt;buf, BINPRM_BUF_SIZE);

            loc-&amp;gt;interp_elf_ex = *((struct elfhdr *)bprm-&amp;gt;buf);
            break;
        }
        elf_ppnt++;
    }

    ...
    /* Some simple consistency checks for the interpreter */
    if (elf_interpreter) {
        ...
    }

    /* Flush all traces of the currently running executable */
    retval = flush_old_exec(bprm); /*替换当前进程的mm_struct*/
    if (retval)
        goto out_free_dentry;

    ...

    setup_new_exec(bprm);

    ...
    retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack);
    if (retval &amp;lt; 0) {
        send_sig(SIGKILL, current, 0);
        goto out_free_dentry;
    }

    current-&amp;gt;mm-&amp;gt;start_stack = bprm-&amp;gt;p;

    /* Now we do a little grungy work by mmapping the ELF image into
       the correct location in memory. */
    for(i = 0, elf_ppnt = elf_phdata; i &amp;lt; loc-&amp;gt;elf_ex.e_phnum; i++, elf_ppnt++) {
        int elf_prot = 0, elf_flags;
        unsigned long k, vaddr;
        unsigned long total_size = 0;

        if (elf_ppnt-&amp;gt;p_type != PT_LOAD) /*只处理LOAD段*/
            continue;

        ...
        /*根据段的读写属性设置elf_prot*/
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_R)
            elf_prot |= PROT_READ;
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_W)
            elf_prot |= PROT_WRITE;
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_X)
            elf_prot |= PROT_EXEC;

        elf_flags = MAP_PRIVATE | MAP_DENYWRITE | MAP_EXECUTABLE;

        vaddr = elf_ppnt-&amp;gt;p_vaddr; /*段映射的进程虚拟地址，例如示例程的代码段映射到0x400000*/
        if (loc-&amp;gt;elf_ex.e_type == ET_EXEC || load_addr_set) {
            elf_flags |= MAP_FIXED;
        } else if (loc-&amp;gt;elf_ex.e_type == ET_DYN) {
            ...
        }

        /*将LOAD段映射到进程的vaddr处*/
        error = elf_map(bprm-&amp;gt;file, load_bias + vaddr, elf_ppnt,
                elf_prot, elf_flags, total_size);
        ...

        if (!load_addr_set) {
            load_addr_set = 1;
            load_addr = (elf_ppnt-&amp;gt;p_vaddr - elf_ppnt-&amp;gt;p_offset); /*段起始映射位置*/
            if (loc-&amp;gt;elf_ex.e_type == ET_DYN) {
                ...
            }
        }
        /*下面这段代码用来计算各段的加载位置，针对示例程序：
           start_code = 0x400000;
           end_code = 0x40764;
           start_data = 0x600e10;
           end_data = 0x601040;
           elf_bss = 0x601040;
           elf_brk = 0x601048; */
        k = elf_ppnt-&amp;gt;p_vaddr;
        if (k &amp;lt; start_code)
            start_code = k;
        if (start_data &amp;lt; k)
            start_data = k;

        ...
        k = elf_ppnt-&amp;gt;p_vaddr + elf_ppnt-&amp;gt;p_filesz;

        if (k &amp;gt; elf_bss)
            elf_bss = k;
        if ((elf_ppnt-&amp;gt;p_flags &amp;amp; PF_X) &amp;amp;&amp;amp; end_code &amp;lt; k)
            end_code = k;
        if (end_data &amp;lt; k)
            end_data = k;
        k = elf_ppnt-&amp;gt;p_vaddr + elf_ppnt-&amp;gt;p_memsz;
        if (k &amp;gt; elf_brk)
            elf_brk = k;
    }

    loc-&amp;gt;elf_ex.e_entry += load_bias;
    elf_bss += load_bias;
    elf_brk += load_bias;
    start_code += load_bias;
    end_code += load_bias;
    start_data += load_bias;
    end_data += load_bias;

    /* Calling set_brk effectively mmaps the pages that we need
     * for the bss and break sections.  We must do this before
     * mapping in the interpreter, to make sure it doesn't wind
     * up getting placed where the bss needs to go.
     */
    retval = set_brk(elf_bss, elf_brk); /*为bss映射匿名页并清零*/
    ...

    if (elf_interpreter) {
        unsigned long interp_map_addr = 0;
        
        /*elf_entry记录了execve调用返回后将执行的函数入口，这里指向程序解析器linux-ld*/
        elf_entry = load_elf_interp(&amp;amp;loc-&amp;gt;interp_elf_ex, interpreter, 
                &amp;amp;interp_map_addr, load_bias);
        if (!IS_ERR((void *)elf_entry)) {
            interp_load_addr = elf_entry;
            elf_entry += loc-&amp;gt;interp_elf_ex.e_entry;
        }
        ...
    }

    kfree(elf_phdata);

    set_binfmt(&amp;amp;elf_format);

    /*在用户态栈中放入elf解析获得的相关信息，供用户态程序使用*/
    retval = create_elf_tables(bprm, &amp;amp;loc-&amp;gt;elf_ex, load_addr, interp_load_addr);
    ...
    /* N.B. passed_fileno might not be initialized? */
    current-&amp;gt;mm-&amp;gt;end_code = end_code;
    current-&amp;gt;mm-&amp;gt;start_code = start_code;
    current-&amp;gt;mm-&amp;gt;start_data = start_data;
    current-&amp;gt;mm-&amp;gt;end_data = end_data;
    current-&amp;gt;mm-&amp;gt;start_stack = bprm-&amp;gt;p;
    ...

    /*修改regs指向的栈寄存器，使得execve调用返回到用户空间时，入口函数为elf_entry，栈指向bprm-&amp;gt;p*/
    start_thread(regs, elf_entry, bprm-&amp;gt;p);
    retval = 0;
out:
    kfree(loc);
out_ret:
    return retval;

/* error cleanup */
out_free_dentry:
    allow_write_access(interpreter);
    if (interpreter)
        fput(interpreter);
out_free_interp:
    kfree(elf_interpreter);
out_free_ph:
    kfree(elf_phdata);
goto out;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，我们已经将进程生命周期中最基本的fork和exec分析完成，内容较多，建议大家多思考多理解，打好基础。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/进程替换/&quot;&gt;【计算子系统】进程管理之二：进程替换&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>Rados Block Device之二－客户端内核RBD驱动分析</title>
        <description>&lt;p&gt;  下面我们通过分析客户端内核驱动，而将服务端(Monitor，OSD)当作黑盒来理解整个ceph系统原理。如果按软件栈分层，linux内核中的RBD驱动涉及上中下两层：上层是在内核块层中的RBD驱动(rbd.ko)；中层是在内核网络层中的libceph驱动(libceph.ko)；下层内核网络socket通信。rbd.ko为客户端提供可访问的块设备，libceph.ko为rbd.ko提供对象存储接口，socket则提供最底层的网络通信，软件栈及其核心对象概览如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/rbd_4.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h3 id=&quot;1-rbd块设备创建&quot;&gt;1. RBD块设备创建&lt;/h3&gt;

&lt;p&gt;  回顾开篇介绍RBD块设备的使用方法，分为创建、映射和使用三步。创建过程主要是用户态rbd工具与rados集群通信完成的，不涉及内核RBD驱动；但问题是rados集群中的OSD提供的是对象服务(我们可以把对象简单理解成key/value对)，如何来表达一个RBD块设备？&lt;/p&gt;

&lt;p&gt;  我们通过rados ls命令来查看一下wbpool中创建了一个名为wb的RBD设备后都生成了哪些对象：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados ls --pool&lt;/strong&gt; wbpool&lt;br /&gt;
rbd_header.371e643c9869&lt;br /&gt;
rbd_directory  &lt;br /&gt;
rbd_object_map.371e643c9869&lt;br /&gt;
rbd_id.wb&lt;br /&gt;
rbd_info&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  这些对象里到底包含了哪些信息呢？我们可以用rados get命令获取对象内容到文件中并打开文件进行查看。先看看rbd_id.wb对象内容：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados get&lt;/strong&gt; rbd_id.wb ./result.txt &lt;strong&gt;--pool&lt;/strong&gt; wbpool; &lt;strong&gt;hexdump -Cv&lt;/strong&gt; ./result.txt&lt;br /&gt;
00000000  0c 00 00 00 33 37 31 65  36 34 33 63 39 38 36 39 |….371e643c9869|&lt;br /&gt;
00000010&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  从文件内容看，rbd_id.wb中保存的主要是个id信息，为”371e643c9869”(前四个字节代表什么意思？通过小端法表示对象实际内容的字节长度，例如这里是0x0000000C，即12个字节)。知道了wb块设备的id，我们再看来看看rbd_head.371e643c9869对象中的内容：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# rados get rbd_header.371e643c9869 ./result.txt –pool wbpool; hexdump -Cv ./result.txt&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;  rbd_header.371e643c9869对象内容为空，怎么回事呢？我们再来看看该对象相关的key/value omap值(可以用来描述对象的元数据信息)：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados listomapvals&lt;/strong&gt; rbd_header.371e643c9869 &lt;strong&gt;--pool&lt;/strong&gt; wbpool&lt;br /&gt;
create_timestamp&lt;br /&gt;
value (8 bytes) :&lt;br /&gt;
00000000  63 38 26 5a 84 c1 71 08 |c8&amp;amp;Z..q.|&lt;br /&gt;
00000008&lt;/p&gt;

  &lt;p&gt;features&lt;br /&gt;
value (8 bytes) :&lt;br /&gt;
00000000  01 00 00 00 00 00 00 00 |……..|&lt;br /&gt;
00000008&lt;/p&gt;

  &lt;p&gt;flags&lt;br /&gt;
value (8 bytes) :&lt;br /&gt;
00000000  00 00 00 00 00 00 00 00 |……..|&lt;br /&gt;
00000008&lt;/p&gt;

  &lt;p&gt;object_prefix&lt;br /&gt;
value (25 bytes) :&lt;br /&gt;
00000000  15 00 00 00 72 62 64 5f  64 61 74 61 2e 33 37 31 |….rbd_data.371|&lt;br /&gt;
00000010  65 36 34 33 63 39 38 36  39 |e643c9869|&lt;br /&gt;
00000019&lt;/p&gt;

  &lt;p&gt;order 
value (1 bytes) :&lt;br /&gt;
00000000  16                                                |.|&lt;br /&gt;
00000001&lt;/p&gt;

  &lt;p&gt;size&lt;br /&gt;
value (8 bytes) :&lt;br /&gt;
00000000  00 00 00 40 00 00 00 00 |…@….|&lt;br /&gt;
00000008&lt;/p&gt;

  &lt;p&gt;snap_seq&lt;br /&gt;
value (8 bytes) :&lt;br /&gt;
00000000  00 00 00 00 00 00 00 00 |……..|&lt;br /&gt;
00000008&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  这样我们看到rbd_header.371e643c9869对象的omap中保存与RBD相关的元数据信息，如RBD数据存放对象前缀为”rbd_data.371e643c9869”；order为0x16，表示以4M单位来划分RBD块设备，每4M对应一个对象，对象名为”rbd_data.371e643c9869.偏移”；size为0x40000000，即1G。&lt;/p&gt;

&lt;p&gt;  最后我们来看看另外两个与具体rbd设备无关的对象rbd_info和rbd_directory：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados get&lt;/strong&gt; rbd_info ./result.txt &lt;strong&gt;--pool&lt;/strong&gt; wbpool; &lt;strong&gt;hexdump -Cv&lt;/strong&gt; ./result.txt&lt;br /&gt;
00000000  6f 76 65 72 77 72 69 74  65 20 76 61 6c 69 64 61 |overwrite valida|&lt;br /&gt;
00000010  74 65 64                                          |ted|&lt;br /&gt;
00000013&lt;/p&gt;

  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados get&lt;/strong&gt; rbd_directory ./result.txt &lt;strong&gt;--pool&lt;/strong&gt; wbpool; &lt;strong&gt;hexdump -Cv&lt;/strong&gt; ./result.txt&lt;br /&gt;
[root@ceph-client]# &lt;strong&gt;rados listomapvals&lt;/strong&gt; rbd_directory &lt;strong&gt;--pool&lt;/strong&gt; wbpool&lt;br /&gt;
id_371e643c9869&lt;br /&gt;
value (6 bytes) :&lt;br /&gt;
00000000  02 00 00 00 77 62                                 |….wb|&lt;br /&gt;
00000006&lt;/p&gt;

  &lt;p&gt;name_wb&lt;br /&gt;
value (16 bytes) :&lt;br /&gt;
00000000  0c 00 00 00 33 37 31 65  36 34 33 63 39 38 36 39 |….371e643c9869|&lt;br /&gt;
00000010&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  从结果我们看出rbd_info中只是保存了一个提示字符串”overwirte validated”。rbd_directory对象的map中保存了所有RBD块设备的名字和id的对应关系，可实现双向查找。&lt;/p&gt;

&lt;p&gt;  综上所述，ceph中也是利用了对象存储的方式来保存RBD信息：”rbd_id.块设备名”对象中保存对应块设备的id；”rbd_header.块设备id”的omap中保存了块设备的各种元数据信息(这里没有直接保存的对象内容中，通过omap更易解析)；”rbd_data.块设备id.偏移”对象用来保存块设备对应偏移处的数据；rbd_directory用来保存pool中所有的RBD设备的名称和id。&lt;/p&gt;

&lt;h3 id=&quot;2-rbd块设备映射流程分析&quot;&gt;2. RBD块设备映射流程分析&lt;/h3&gt;

&lt;p&gt;  rbd工具通过map子命令映射产生一个可使用的内核块设备，其本质原理是由rbd工具往sysfs(“/sys/bus/rbd/add”)内核文件接口中写入待创建块设备的信息(例如前文实例写入的内容为”9.22.115.154:6789 name=amdin,key=client, wbpool wb -“，其中9.22.115.154为主monitor的IP)，内核在处理这个写入请求时会调用由RBD驱动事先注册好的处理函数来生成一个内核块设备对象。&lt;/p&gt;

&lt;p&gt;  我们先来看看rbd模块加载初始化时完成了哪些初始化动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int __init rbd_init(void)
{
    int rc;

    /*首先检查rbd驱动依赖的底层libceph驱动的兼容性*/
    if (!libceph_compatible(NULL)) {
        rbd_warn(NULL, &quot;libceph incompatibility (quitting)&quot;);

        return -EINVAL;
    }

    /*初始化rbd驱动中使用的内存分配器*/
    rc = rbd_slab_init();
    if (rc)
        return rc;

    /*sysfs文件接口初始化，为用户态rbd工具暴露访问接口*/
    rc = rbd_sysfs_init();
    if (rc)
        rbd_slab_exit();
    else
    pr_info(&quot;loaded &quot; RBD_DRV_NAME_LONG &quot;\n&quot;);

    return rc;
}

module_init(rbd_init);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*
 * create control files in sysfs
 * /sys/bus/rbd/...
 */
static int rbd_sysfs_init(void)
{
    int ret;

    ret = device_register(&amp;amp;rbd_root_dev);
    if (ret &amp;lt; 0)
        return ret;

    /*根据rbd_bus_type定义的信息在/sys/bus/下生成子目录*/
    ret = bus_register(&amp;amp;rbd_bus_type);
    if (ret &amp;lt; 0)
        device_unregister(&amp;amp;rbd_root_dev);

    return ret;
}

static struct bus_attribute rbd_bus_attrs[] = {
    __ATTR(add, S_IWUSR, NULL, rbd_add), /*对rbd目录下的add文件进行写操作时将调用rbd_add*/
    __ATTR(remove, S_IWUSR, NULL, rbd_remove), /*对rbd目录下的remove文件进行写操作时将调用rbd_remove*/
    __ATTR_NULL
};

static struct bus_type rbd_bus_type = {
    .name		= &quot;rbd&quot;, /* /sys/bus/成生的子目录名 */
    .bus_attrs	= rbd_bus_attrs,
};

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  下面我们顺着本文开头给出的软件栈从上至下(rbd-&amp;gt;libceph)深入分析rbd_add函数，看看内核RBD驱动是如何进行块设备的生成的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*用户态工具往/sys/bus/rbd/add中写入RBD设备信息，内核在处理该写入请求时就会
  调用这里的rbd_add，其中bus就是/sys/bus/rbd对象，buf中存放用户态工具写入的
  字符串，如&quot;9.22.115.154:6789 name=amdin,key=client, wbpool wb -&quot;，
  count为写入的字符串长度*/

static ssize_t rbd_add(struct bus_type *bus,
                const char *buf, size_t count)
{
    struct rbd_device *rbd_dev = NULL;
    struct ceph_options *ceph_opts = NULL;
    struct rbd_options *rbd_opts = NULL;
    struct rbd_spec *spec = NULL;
    struct rbd_client *rbdc;
    struct ceph_osd_client *osdc;
    bool read_only;
    int rc = -ENOMEM;

    ...

    /*首先对用户态写入的字符串进行解析，结果返回到ceph_opts，rbd_opt和spec中：
      (1)ceph_opts中保存ceph集群相关信息，针对前文示例，ceph_opts.mon_addr中将保存
         &quot;9.22.115.154:6789&quot;；ceph_opts.name为&quot;admin&quot;，代表访问ceph集群的角色名称；
         ceph_opts.key对应系统配置中的&quot;ceph.client.admin.keyring&quot;，为登陆密钥。
      (2)rbd_opt中包含当前rbd设备是否只读，针对前文示例，rbd_opt.read_only为false。
      (3)spec中包含当前rbd设备的详细描述信息，针对示例，spec.pool_name为&quot;wb_pool&quot;，
         spec.image_name为&quot;wb&quot;，spec.snap_name为&quot;-&quot;，表示无快照*/

    /* parse add command */
    rc = rbd_add_parse_args(buf, &amp;amp;ceph_opts, &amp;amp;rbd_opts, &amp;amp;spec);
    if (rc &amp;lt; 0)
        goto err_out_module;
    read_only = rbd_opts-&amp;gt;read_only;
    kfree(rbd_opts);
    rbd_opts = NULL;	/* done with this */

    /*接着根据ceph集群配置ceph_opts在当前客户端查找是否已生成rbd_client(对应一个ceph_client，
      内部包含mon_client和osd_client)；如果未找到则生成一个新的rbd_client，并建立会话(完成认证、获
      取monmap和osdmap等一系统初始化动作)*/
    rbdc = rbd_get_client(ceph_opts);
    if (IS_ERR(rbdc)) {
        rc = PTR_ERR(rbdc);
        goto err_out_args;
    }

    /*从ceph集群返回的osdmap中获取当前pool对应的id*/
    /* pick the pool */
    osdc = &amp;amp;rbdc-&amp;gt;client-&amp;gt;osdc;
    rc = ceph_pg_poolid_by_name(osdc-&amp;gt;osdmap, spec-&amp;gt;pool_name);
    if (rc &amp;lt; 0)
        goto err_out_client;
    spec-&amp;gt;pool_id = (u64)rc;

    ...

    /*创建内核中的rbd_dev对象*/
    rbd_dev = rbd_dev_create(rbdc, spec);
    if (!rbd_dev)
        goto err_out_client;
    rbdc = NULL;		/* rbd_dev now owns this */
    spec = NULL;		/* rbd_dev now owns this */

    /*从ceph集群中查询当前rbd设备(image)的元数据信息，即从rbd_id.[rbd设备名]中获知image id，并从
      rbd_header.[image id]的omap中获得设备大小，分片大小等信息*/
    rc = rbd_dev_image_probe(rbd_dev, true);
    if (rc &amp;lt; 0)
        goto err_out_rbd_dev;

    ...

    /*根据ceph集群返回的rbd信息，完成对rbd_dev的设置，并通过内核块层接口生成新的块设备对象*/
    rc = rbd_dev_device_setup(rbd_dev);
    if (rc) {
        rbd_dev_image_release(rbd_dev);
        goto err_out_module;
    }

    return count;

    ...
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;21-rbd_get_client&quot;&gt;&lt;strong&gt;2.1. rbd_get_client&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  针对同一个rados集群，每个客户端节点都会在内核中生成一个rbd_client对象，用来和该rados集群进行会话；在一个客户端节点上，如果针对同一个rados集群创建了多个RBD设备，那么这些RBD设备会共用同一个rbd_client。下面我们看看rbd_get_client的内部实现：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*
 * Get a ceph client with specific addr and configuration, if one does
 * not exist create it.  Either way, ceph_opts is consumed by this
 * function.
 */
static struct rbd_client *rbd_get_client(struct ceph_options *ceph_opts)
{
    struct rbd_client *rbdc;

    rbdc = rbd_client_find(ceph_opts); /*在全局链表rbd_client_list中根据ceph_opts的集群信息查找rbd_client*/
    if (rbdc)	/* using an existing client */
        ceph_destroy_options(ceph_opts); /*如果找到相同的rbd_client(对应同一个rados集群)，则复用该对象，同时销毁ceph_opts*/
    else
        rbdc = rbd_client_create(ceph_opts); /*如果没有找到相同的rbd_client，则新建一个并添加到rbd_client_list中*/

    return rbdc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接着看rbd_client_create：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*rbd_client代表rbd客户端实例，多个rbd设备可共用一个客户端实例；其内部主要包含一个ceph_client对象*/
/*
 * an instance of the client.  multiple devices may share an rbd client.
 */
struct rbd_client {
    struct ceph_client	*client;
    struct kref		kref;
    struct list_head	node;
};

/*
 * Initialize an rbd client instance.  Success or not, this function
 * consumes ceph_opts.
 */
static struct rbd_client *rbd_client_create(struct ceph_options *ceph_opts)
{
    struct rbd_client *rbdc;
    int ret = -ENOMEM;

    rbdc = kmalloc(sizeof(struct rbd_client), GFP_KERNEL);
    ...

    /*调用libceph.ko中的函数创建ceph_client对象*/
    rbdc-&amp;gt;client = ceph_create_client(ceph_opts, rbdc, 0, 0);
    ...

    /*调用libceph.ko中的函数打开ceph_client对象的会话，基于该会话可以和rados集群(monitor or OSD)进行通信*/
    ret = ceph_open_session(rbdc-&amp;gt;client);
    ...

    spin_lock(&amp;amp;rbd_client_list_lock);
    list_add_tail(&amp;amp;rbdc-&amp;gt;node, &amp;amp;rbd_client_list);
    spin_unlock(&amp;amp;rbd_client_list_lock);

    ...

    return rbdc;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  下面我们再深入一层，看看libceph中ceph_client的相关实现，但不会深入到messenger模块中(复杂性较高)，我们在2.1.3节会总结一下对messenger模块的使用方法(API)。对于messenger的分析我们将放到本篇博文最后一节。&lt;/p&gt;

&lt;h5 id=&quot;211-rbd_get_client---ceph_create_client&quot;&gt;&lt;strong&gt;2.1.1. rbd_get_client -&amp;gt; ceph_create_client&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/libceph.h:

/*
 * per client state
 *
 * possibly shared by multiple mount points, if they are
 * mounting the same ceph filesystem/cluster.
 */
struct ceph_client {
    ...

    /*每个ceph_client包含一个messenger实例、一个mon_client实例和一个osd_client实例：
      (1)messenger实例描述了与网络通信相关的信息，属于messenger模块，是ceph客户端基于socket封装的网络服务层；
      (2)mon_client实例是专门用来与monitor通信的客户端；
      (3)osd_client实例是专门用来与所有osd通信的客户端。*/
    struct ceph_messenger msgr;   /* messenger instance */
    struct ceph_mon_client monc;
    struct ceph_osd_client osdc;

    ...
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/ceph_common.c:

/*ceph_create_client创建ceph_client对象并对其内部的messenger、mon_client、osd_client进行初始化*/
/*
 * create a fresh client instance
 */
struct ceph_client *ceph_create_client(struct ceph_options *opt, void *private,
        unsigned int supported_features, unsigned int required_features)
{
    struct ceph_client *client;
    struct ceph_entity_addr *myaddr = NULL;
    int err = -ENOMEM;

    client = kzalloc(sizeof(*client), GFP_KERNEL);
    if (client == NULL)
        return ERR_PTR(-ENOMEM);

    client-&amp;gt;private = private;
    client-&amp;gt;options = opt;

    ...

    /* msgr */
    if (ceph_test_opt(client, MYIP))
        myaddr = &amp;amp;client-&amp;gt;options-&amp;gt;my_addr;
    ceph_messenger_init(&amp;amp;client-&amp;gt;msgr, myaddr, client-&amp;gt;supported_features,
        client-&amp;gt;required_features, ceph_test_opt(client, NOCRC));

    /* subsystems */
    err = ceph_monc_init(&amp;amp;client-&amp;gt;monc, client);
    if (err &amp;lt; 0)
        goto fail;
    err = ceph_osdc_init(&amp;amp;client-&amp;gt;osdc, client);
    if (err &amp;lt; 0)
        goto fail_monc;

    return client;
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接着我们再深入看看messenger、mon_client、osd_client的初始化：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/messenger.h:

struct ceph_messenger {
    struct ceph_entity_inst inst;    /* my name+address */
    struct ceph_entity_addr my_enc_addr;

    atomic_t stopping;
    bool nocrc;

    /*
     * the global_seq counts connections i (attempt to) initiate
     * in order to disambiguate certain connect race conditions.
     */
    u32 global_seq;
    spinlock_t global_seq_lock;

    u32 supported_features;
    u32 required_features;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

/*
 * initialize a new messenger instance
 */
void ceph_messenger_init(struct ceph_messenger *msgr,
            struct ceph_entity_addr *myaddr,
            u32 supported_features,
            u32 required_features,
            bool nocrc)
{
    msgr-&amp;gt;supported_features = supported_features;
    msgr-&amp;gt;required_features = required_features;

    spin_lock_init(&amp;amp;msgr-&amp;gt;global_seq_lock);

    if (myaddr)
        msgr-&amp;gt;inst.addr = *myaddr;

    /* select a random nonce */
    msgr-&amp;gt;inst.addr.type = 0;
    get_random_bytes(&amp;amp;msgr-&amp;gt;inst.addr.nonce, sizeof(msgr-&amp;gt;inst.addr.nonce));
    encode_my_addr(msgr);
    msgr-&amp;gt;nocrc = nocrc;

    atomic_set(&amp;amp;msgr-&amp;gt;stopping, 0);

    dout(&quot;%s %p\n&quot;, __func__, msgr);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/mon_client.h:

struct ceph_mon_client {
    struct ceph_client *client;
    struct ceph_monmap *monmap;

    struct mutex mutex;
    struct delayed_work delayed_work;

    struct ceph_auth_client *auth;
    struct ceph_msg *m_auth, *m_auth_reply, *m_subscribe, *m_subscribe_ack;
    int pending_auth;

    bool hunting;
    int cur_mon;                       /* last monitor i contacted */
    unsigned long sub_sent, sub_renew_after;
    struct ceph_connection con;

    /* pending generic requests */
    struct rb_root generic_request_tree;
    int num_generic_requests;
    u64 last_tid;

    /* mds/osd map */
    int want_mdsmap;
    int want_next_osdmap; /* 1 = want, 2 = want+asked */
    u32 have_osdmap, have_mdsmap;
};

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/mon_client.c:

int ceph_monc_init(struct ceph_mon_client *monc, struct ceph_client *cl)
{
    int err = 0;

    dout(&quot;init\n&quot;);
    memset(monc, 0, sizeof(*monc));
    monc-&amp;gt;client = cl;
    monc-&amp;gt;monmap = NULL;
    mutex_init(&amp;amp;monc-&amp;gt;mutex);

    err = build_initial_monmap(monc);
    if (err)
        goto out;

    /* connection */
    /* authentication */
    monc-&amp;gt;auth = ceph_auth_init(cl-&amp;gt;options-&amp;gt;name, cl-&amp;gt;options-&amp;gt;key);
    ...
    monc-&amp;gt;auth-&amp;gt;want_keys =
        CEPH_ENTITY_TYPE_AUTH | CEPH_ENTITY_TYPE_MON |
        CEPH_ENTITY_TYPE_OSD | CEPH_ENTITY_TYPE_MDS;

    /* msgs */
    err = -ENOMEM;
    monc-&amp;gt;m_subscribe_ack = ceph_msg_new(CEPH_MSG_MON_SUBSCRIBE_ACK,
        sizeof(struct ceph_mon_subscribe_ack), GFP_NOFS, true);
    ...
    monc-&amp;gt;m_subscribe = ceph_msg_new(CEPH_MSG_MON_SUBSCRIBE, 96, GFP_NOFS, true);
    ...
    monc-&amp;gt;m_auth_reply = ceph_msg_new(CEPH_MSG_AUTH_REPLY, 4096, GFP_NOFS, true);
    ...
    monc-&amp;gt;m_auth = ceph_msg_new(CEPH_MSG_AUTH, 4096, GFP_NOFS, true);
    monc-&amp;gt;pending_auth = 0;
    ...

    /*mon_client通过底层messenger模块中的connection对象进行网络通信，这里对mon_client使用的connection进行
      初始化，并指定其网络层回调函数集为mon_con_ops，用来处理消息回调、网络连接故障等*/
    ceph_con_init(&amp;amp;monc-&amp;gt;con, monc, &amp;amp;mon_con_ops, &amp;amp;monc-&amp;gt;client-&amp;gt;msgr);

    monc-&amp;gt;cur_mon = -1;
    monc-&amp;gt;hunting = true;
    monc-&amp;gt;sub_renew_after = jiffies;
    monc-&amp;gt;sub_sent = 0;

    INIT_DELAYED_WORK(&amp;amp;monc-&amp;gt;delayed_work, delayed_work);
    monc-&amp;gt;generic_request_tree = RB_ROOT;
    monc-&amp;gt;num_generic_requests = 0;
    monc-&amp;gt;last_tid = 0;

    monc-&amp;gt;have_mdsmap = 0;
    monc-&amp;gt;have_osdmap = 0;
    monc-&amp;gt;want_next_osdmap = 1;
    return 0;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/osd_client.h:

struct ceph_osd_client {
    struct ceph_client     *client;

    struct ceph_osdmap     *osdmap;       /* current map */
    struct rw_semaphore    map_sem;
    struct completion      map_waiters;
    u64                    last_requested_map;

    struct mutex           request_mutex;
    struct rb_root         osds;          /* osds */
    struct list_head       osd_lru;       /* idle osds */
    u64                    timeout_tid;   /* tid of timeout triggering rq */
    u64                    last_tid;      /* tid of last request */
    struct rb_root         requests;      /* pending requests */
    struct list_head       req_lru;	      /* in-flight lru */
    struct list_head       req_unsent;    /* unsent/need-resend queue */
    struct list_head       req_notarget;  /* map to no osd */
    struct list_head       req_linger;    /* lingering requests */
    int                    num_requests;
    struct delayed_work    timeout_work;
    struct delayed_work    osds_timeout_work;

    mempool_t              *req_mempool;

    struct ceph_msgpool	msgpool_op;
    struct ceph_msgpool	msgpool_op_reply;

    spinlock_t		event_lock;
    struct rb_root		event_tree;
    u64			event_count;

    struct workqueue_struct	*notify_wq;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/osd_client.c:

int ceph_osdc_init(struct ceph_osd_client *osdc, struct ceph_client *client)
{
    int err;

    dout(&quot;init\n&quot;);
    osdc-&amp;gt;client = client;
    osdc-&amp;gt;osdmap = NULL;
    init_rwsem(&amp;amp;osdc-&amp;gt;map_sem);
    init_completion(&amp;amp;osdc-&amp;gt;map_waiters);
    osdc-&amp;gt;last_requested_map = 0;
    mutex_init(&amp;amp;osdc-&amp;gt;request_mutex);
    osdc-&amp;gt;last_tid = 0;
    osdc-&amp;gt;osds = RB_ROOT;
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;osd_lru);
    osdc-&amp;gt;requests = RB_ROOT;
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_lru);
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_unsent);
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_notarget);
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_linger);
    osdc-&amp;gt;num_requests = 0;
    INIT_DELAYED_WORK(&amp;amp;osdc-&amp;gt;timeout_work, handle_timeout);
    INIT_DELAYED_WORK(&amp;amp;osdc-&amp;gt;osds_timeout_work, handle_osds_timeout);
    spin_lock_init(&amp;amp;osdc-&amp;gt;event_lock);
    osdc-&amp;gt;event_tree = RB_ROOT;
    osdc-&amp;gt;event_count = 0;

    schedule_delayed_work(&amp;amp;osdc-&amp;gt;osds_timeout_work,
        round_jiffies_relative(osdc-&amp;gt;client-&amp;gt;options-&amp;gt;osd_idle_ttl * HZ));

    err = -ENOMEM;
    osdc-&amp;gt;req_mempool = mempool_create_kmalloc_pool(10, sizeof(struct ceph_osd_request));
    if (!osdc-&amp;gt;req_mempool)
        goto out;

    err = ceph_msgpool_init(&amp;amp;osdc-&amp;gt;msgpool_op, CEPH_MSG_OSD_OP,
            OSD_OP_FRONT_LEN, 10, true, &quot;osd_op&quot;);
    if (err &amp;lt; 0)
        goto out_mempool;
    err = ceph_msgpool_init(&amp;amp;osdc-&amp;gt;msgpool_op_reply, CEPH_MSG_OSD_OPREPLY,
        OSD_OPREPLY_FRONT_LEN, 10, true, &quot;osd_op_reply&quot;);
    if (err &amp;lt; 0)
        goto out_msgpool;

    err = -ENOMEM;
    osdc-&amp;gt;notify_wq = create_singlethread_workqueue(&quot;ceph-watch-notify&quot;);
    if (!osdc-&amp;gt;notify_wq)
        goto out_msgpool;
    return 0;

out_msgpool:
    ceph_msgpool_destroy(&amp;amp;osdc-&amp;gt;msgpool_op);
out_mempool:
    mempool_destroy(osdc-&amp;gt;req_mempool);
out:
    return err;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;212-rbd_get_client---ceph_open_session&quot;&gt;&lt;strong&gt;2.1.2. rbd_get_client -&amp;gt; ceph_open_session&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  在完成ceph_client的初始化动作后，下一步是打开会话：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/ceph_common.c:

int ceph_open_session(struct ceph_client *client)
{
    ...

    ret = __ceph_open_session(client, started);

    ...
    return ret;
}

/*
 * mount: join the ceph cluster, and open root directory.
 */
int __ceph_open_session(struct ceph_client *client, unsigned long started)
{
    int err;
    /*timeout表示打开会话的超时时间，默认为60秒*/
    unsigned long timeout = client-&amp;gt;options-&amp;gt;mount_timeout * HZ;

    /*ceph_client内部是通过mon_client来打开会话，如果mon_client成功打开会话，
      它会成功获得rados集群的mon_map(描述所有monitors信息)和osd_map(描述所有的osd信息)*/
    /* open session, and wait for mon and osd maps */
    err = ceph_monc_open_session(&amp;amp;client-&amp;gt;monc);
    if (err &amp;lt; 0)
        return err;

    while (!have_mon_and_osd_map(client)) { /*如果没有获得mon_map和osd_map，则等待直到超时*/
        err = -EIO;
        if (timeout &amp;amp;&amp;amp; time_after_eq(jiffies, started + timeout)) /*超时退出*/
            return err;

        /* wait */
        dout(&quot;mount waiting for mon_map\n&quot;);
        /*下面，当前进程(rbd工具)将进入睡眠状态，直到内核接收到mon_map和osd_map时被重新唤醒，
          或者出现认证错时也将被唤醒*/
        err = wait_event_interruptible_timeout(client-&amp;gt;auth_wq,
            have_mon_and_osd_map(client) || (client-&amp;gt;auth_err &amp;lt; 0),
            timeout);
        if (err == -EINTR || err == -ERESTARTSYS)
            return err;
        if (client-&amp;gt;auth_err &amp;lt; 0)
            return client-&amp;gt;auth_err;
    }

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/mon_client.c:

int ceph_monc_open_session(struct ceph_mon_client *monc)
{
    mutex_lock(&amp;amp;monc-&amp;gt;mutex);
    __open_session(monc);
    __schedule_delayed(monc);
    mutex_unlock(&amp;amp;monc-&amp;gt;mutex);
    return 0;
}

/*
 * Open a session with a (new) monitor.
 */
static int __open_session(struct ceph_mon_client *monc)
{
    char r;
    int ret;

    if (monc-&amp;gt;cur_mon &amp;lt; 0) {/*初始时cur_mon为－1，表示没有和任何monitor建立连接*/

        /*通过随机数r，从初始化时生成的mon_map中随机选一个进行会话连接的monitor*/
        get_random_bytes(&amp;amp;r, 1);
        monc-&amp;gt;cur_mon = r % monc-&amp;gt;monmap-&amp;gt;num_mon;
        monc-&amp;gt;sub_sent = 0;
        monc-&amp;gt;sub_renew_after = jiffies;  /* i.e., expired */
        monc-&amp;gt;want_next_osdmap = !!monc-&amp;gt;want_next_osdmap;

        /*与选定的monitor建立网络连接，这里使用的messenger模块提供的接口*/
        ceph_con_open(&amp;amp;monc-&amp;gt;con,
            CEPH_ENTITY_TYPE_MON, monc-&amp;gt;cur_mon,
            &amp;amp;monc-&amp;gt;monmap-&amp;gt;mon_inst[monc-&amp;gt;cur_mon].addr);

        /*初始化发送给monitor的首个hello消息*/
        /* initiatiate authentication handshake */
        ret = ceph_auth_build_hello(monc-&amp;gt;auth,
                monc-&amp;gt;m_auth-&amp;gt;front.iov_base,
                monc-&amp;gt;m_auth-&amp;gt;front_alloc_len);

        /*这里通过messenger模块的ceph_con_send接口将消息发送给monitor*/
        __send_prepared_auth_request(monc, ret);
    } else {
        dout(&quot;open_session mon%d already open\n&quot;, monc-&amp;gt;cur_mon);
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;213-messenger模块使用方法小结&quot;&gt;&lt;strong&gt;2.1.3. messenger模块使用方法小结&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  通过前文对mon_client的分析，我们来总结一下mon_client是如何使用底层的messenger模块的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;(1)通过ceph_messenger_init在ceph_client中初始化一个messenger对象，定义全局通信信息；&lt;/li&gt;
    &lt;li&gt;(2)ceph_con_init(&amp;amp;monc-&amp;gt;con, monc, &amp;amp;mon_con_ops, &amp;amp;monc-&amp;gt;client-&amp;gt;msgr)，网络连接对象初始化，并指明该连接收到消息时的回调处理函数(消息将在内核工作队列上下文被处理)；&lt;/li&gt;
    &lt;li&gt;(3)使用ceph_msg_new进行消息的内存分配；&lt;/li&gt;
    &lt;li&gt;(4)ceph_con_open打开与选定monitor的网络连接；&lt;/li&gt;
    &lt;li&gt;(5)填入消息内容；&lt;/li&gt;
    &lt;li&gt;(6)使用ceph_con_send进行消息发送；&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;214-打开会话时客户端与monitor的交互消息处理回调mon_con_ops分析&quot;&gt;&lt;strong&gt;2.1.4. 打开会话时客户端与monitor的交互(消息处理回调mon_con_ops分析)&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  目前monitor对我们来说还是一个黑盒，因此无法分析其内部是如何处理客户端发送的hello消息的。那么我们如何才能获知客户端是如何与monitor进行交互的？这里我们可以打开内核动态日志开关(代码中有很多dout语句)，并通过分析日志来获知整个交互过程。通过打开libceph的messenger模块日志，我们可以得到如下图所示的消息交互过程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/rbd_5.jpg&quot; height=&quot;550&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从上图可见，TCP连接建立后的前几次消息交互主要是messenger模块对连接进行初始化，我们在将在深入分析messenger模块时分析。ceph connection初始化完成后，客户端首先便是向monitor发送Auth(hello)认证消息，关于认证的实现细节我们这里暂不深入分析，通过日志可知，整个认证会有三次Auth消息的交互：第一次Auth_reply返回前，monitor会向客户端返回monmap；第三次Auth_reply返回后，表示认证成功。认证成功之后客户端会向monitor发送Subscribe订阅消息，表示关注osdmap的更新。由于是首次认证，monitor会向client返回初始的osdmap，后续只有在osdmap有更新时，monitor才回返回更新的map。&lt;/p&gt;

&lt;p&gt;  下面我们打开mon_con_ops看看客户端收到各类消息的处理过程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/mon_client.c:

static const struct ceph_connection_operations mon_con_ops = {
    ...
    .dispatch = dispatch, /*收到monitor的消息时会调用该函数*/
    ...
};

/*
 * handle incoming message
 */
static void dispatch(struct ceph_connection *con, struct ceph_msg *msg)
{
    struct ceph_mon_client *monc = con-&amp;gt;private;
    int type = le16_to_cpu(msg-&amp;gt;hdr.type);

    if (!monc)
        return;

    /*这里针对不同消息进行不同处理*/
    switch (type) {
    case CEPH_MSG_AUTH_REPLY:
        handle_auth_reply(monc, msg);
        break;

    case CEPH_MSG_MON_SUBSCRIBE_ACK:
        handle_subscribe_ack(monc, msg);
        break;

    case CEPH_MSG_STATFS_REPLY:
        handle_statfs_reply(monc, msg);
        break;

    case CEPH_MSG_POOLOP_REPLY:
        handle_poolop_reply(monc, msg);
        break;

    case CEPH_MSG_MON_MAP:
        ceph_monc_handle_map(monc, msg);
        break;

    case CEPH_MSG_OSD_MAP:
        ceph_osdc_handle_map(&amp;amp;monc-&amp;gt;client-&amp;gt;osdc, msg);
        break;

    default:
        /* can the chained handler handle it? */
        if (monc-&amp;gt;client-&amp;gt;extra_mon_dispatch &amp;amp;&amp;amp;
                monc-&amp;gt;client-&amp;gt;extra_mon_dispatch(monc-&amp;gt;client, msg) == 0)
            break;

        pr_err(&quot;received unknown message type %d %s\n&quot;, type,
        ceph_msg_type_name(type));
    }
    ceph_msg_put(msg);
}

/*处理monmap消息，接收最新的monitor信息*/
static void ceph_monc_handle_map(struct ceph_mon_client *monc, struct ceph_msg *msg)
{
    struct ceph_client *client = monc-&amp;gt;client;
    struct ceph_monmap *monmap = NULL, *old = monc-&amp;gt;monmap;
    void *p, *end;
    ...

    dout(&quot;handle_monmap\n&quot;);
    p = msg-&amp;gt;front.iov_base;
    end = p + msg-&amp;gt;front.iov_len;

    /*从收到的消息中解析出monmap内容，它包含所有monitor节点的IP信息*/
    monmap = ceph_monmap_decode(p, end);
    ...

    /*更新monmap*/
    client-&amp;gt;monc.monmap = monmap;
    kfree(old);
    ...

out_unlocked:
    /*唤醒所有在auth_wq中等待的任务*/
    wake_up_all(&amp;amp;client-&amp;gt;auth_wq);
}

/*处理osdmap消息，整体思路和monmap类似；这里分了增量和全量两种模式；
  osdmap中记录了所有osd的IP和状态信息*/
/*
* Process updated osd map.
*
* The message contains any number of incremental and full maps, normally
* indicating some sort of topology change in the cluster.  Kick requests
* off to different OSDs as needed.
*/
void ceph_osdc_handle_map(struct ceph_osd_client *osdc, struct ceph_msg *msg)
{
    ...
}

static void handle_auth_reply(struct ceph_mon_client *monc, struct ceph_msg *msg)
{
    int ret;
    int was_auth = 0;

    ...
    /*处理auth_reply消息*/
    ret = ceph_handle_auth_reply(monc-&amp;gt;auth, msg-&amp;gt;front.iov_base,
                msg-&amp;gt;front.iov_len,
                monc-&amp;gt;m_auth-&amp;gt;front.iov_base,
                monc-&amp;gt;m_auth-&amp;gt;front_alloc_len);
    if (ret &amp;lt; 0) {
        /*如果认证出错，则唤醒等待任务并返回错误信息*/
        monc-&amp;gt;client-&amp;gt;auth_err = ret;
        wake_up_all(&amp;amp;monc-&amp;gt;client-&amp;gt;auth_wq);
    } else if (ret &amp;gt; 0) {
        /*继续发送认证请求，从日志分析共会发送三次*/
        __send_prepared_auth_request(monc, ret);
    } else if (!was_auth &amp;amp;&amp;amp; ceph_auth_is_authenticated(monc-&amp;gt;auth)) {
        /*认证成功，发送针对osdmap的订阅消息*/
        dout(&quot;authenticated, starting session\n&quot;);

        monc-&amp;gt;client-&amp;gt;msgr.inst.name.type = CEPH_ENTITY_TYPE_CLIENT;
        monc-&amp;gt;client-&amp;gt;msgr.inst.name.num =
        cpu_to_le64(monc-&amp;gt;auth-&amp;gt;global_id);

        __send_subscribe(monc);
        __resend_generic_request(monc);
    }
    ...

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;22-rbd_dev_image_probe&quot;&gt;&lt;strong&gt;2.2. rbd_dev_image_probe&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  通过前文的分析我们可知，当monitor返回monmap和osdmap后，rbd进程将继续往下执行到rbd_dev_image_probe，这里将从存放对象的OSD中获取与当前RBD设备相关的信息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*rbd_dev已分配内存空间，mapping为true*/
static int rbd_dev_image_probe(struct rbd_device *rbd_dev, bool mapping)
{
    int ret;
    int tmp;

    /*从rados OSD池中获取当前RBD设备的image id，即对象&quot;rbd_id.[RBD设备名称]&quot;的内容*/
    ret = rbd_dev_image_id(rbd_dev);
    ...

    /*构造当前RBD设备元数据对象的名称，即&quot;rbd_header.[image id]&quot;*/
    ret = rbd_dev_header_name(rbd_dev);
    ...

    if (mapping) {
        ret = rbd_dev_header_watch_sync(rbd_dev, true);
        ...
    }

    if (rbd_dev-&amp;gt;image_format == 1)
        ret = rbd_dev_v1_header_info(rbd_dev);
    else
        /*从rados OSD池中获取肖前RBD设备元数据对象的omap信息*/
        ret = rbd_dev_v2_header_info(rbd_dev);
    ...

    return 0;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;221-rbd_dev_image_probe---rbd_dev_image_id&quot;&gt;&lt;strong&gt;2.2.1. rbd_dev_image_probe -&amp;gt; rbd_dev_image_id&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_dev_image_id(struct rbd_device *rbd_dev)
{
    int ret;
    size_t size;
    char *object_name;
    void *response;
    char *image_id;

    ...

    /*对象名称为RBD_ID_PREFIX(&quot;rbd_id.&quot;)+image_name*/
    size = sizeof (RBD_ID_PREFIX) + strlen(rbd_dev-&amp;gt;spec-&amp;gt;image_name);
    object_name = kmalloc(size, GFP_NOIO);
    ...

    /*返回的对象内容是一个经过编码的字符串，前四个字节代表后续内容的字节长度*/
    size = sizeof (__le32) + RBD_IMAGE_ID_LEN_MAX;
    response = kzalloc(size, GFP_NOIO);
    ...

    /*调用rbd同步对象访问接口，这里本质就是获取&quot;rbd_id.RBD名称&quot;对象的内容：
      (1)rbd_device为rbd_dev，即当前RBD设备；
      (2)接口访问对象名称为object_name，即&quot;rbd_id.RBD名称&quot;；
      (3)接口访问类(class)名称为&quot;rbd&quot;；
      (4)接口访问方法(method)名称为&quot;get_id&quot;；
      (5)接口访问输入参数为空；
      (6)接口访问输入参数长度为0；
      (7)接口访问输出结果保存内存位置为response；
      (8)接口访问输出结果保存内存最大长度为64。*/
    ret = rbd_obj_method_sync(rbd_dev, object_name,
                &quot;rbd&quot;, &quot;get_id&quot;, NULL, 0,
                response, RBD_IMAGE_ID_LEN_MAX);

    if (ret == -ENOENT) {
        ...
    } else if (ret &amp;gt; sizeof (__le32)) {
        void *p = response;

        /*从返回结果中解析出对象内容，即image_id*/
        image_id = ceph_extract_encoded_string(&amp;amp;p, p + ret, NULL, GFP_NOIO);
        ret = IS_ERR(image_id) ? PTR_ERR(image_id) : 0;
        if (!ret)
            rbd_dev-&amp;gt;image_format = 2;
    } else {
        ret = -EINVAL;
    }

    if (!ret) {
        rbd_dev-&amp;gt;spec-&amp;gt;image_id = image_id;
        dout(&quot;image_id is %s\n&quot;, image_id);
    }
    ...
    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;222-rbd_dev_image_probe---rbd_dev_header_name&quot;&gt;&lt;strong&gt;2.2.2. rbd_dev_image_probe -&amp;gt; rbd_dev_header_name&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_dev_header_name(struct rbd_device *rbd_dev)
{
    struct rbd_spec *spec = rbd_dev-&amp;gt;spec;
    size_t size;

    ...
    /*header_name表示RBD设备对应头部对象(其omap保存了RBD设备元数据信息)名称
      即&quot;rbd_header.&quot;+image_id*/
    size = sizeof (RBD_HEADER_PREFIX) + strlen(spec-&amp;gt;image_id);

    rbd_dev-&amp;gt;header_name = kmalloc(size, GFP_KERNEL);
    ...
    sprintf(rbd_dev-&amp;gt;header_name, &quot;%s%s&quot;, RBD_HEADER_PREFIX, spec-&amp;gt;image_id);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;223-rbd_dev_image_probe---rbd_dev_v2_header_info&quot;&gt;&lt;strong&gt;2.2.3. rbd_dev_image_probe -&amp;gt; rbd_dev_v2_header_info&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*这里我们忽略与快照特性(snapshot)相关代码*/
static int rbd_dev_v2_header_info(struct rbd_device *rbd_dev)
{
    bool first_time = rbd_dev-&amp;gt;header.object_prefix == NULL;
    int ret;

    down_write(&amp;amp;rbd_dev-&amp;gt;header_rwsem);

    ret = rbd_dev_v2_image_size(rbd_dev);
    ...

    if (first_time) {
        ret = rbd_dev_v2_header_onetime(rbd_dev);
        ...
    }
    
    ...
    up_write(&amp;amp;rbd_dev-&amp;gt;header_rwsem);

    return ret;
}

static int rbd_dev_v2_image_size(struct rbd_device *rbd_dev)
{
    return _rbd_dev_v2_snap_size(rbd_dev, CEPH_NOSNAP,
            &amp;amp;rbd_dev-&amp;gt;header.obj_order,
            &amp;amp;rbd_dev-&amp;gt;header.image_size);
}

static int _rbd_dev_v2_snap_size(struct rbd_device *rbd_dev, u64 snap_id,
        u8 *order, u64 *snap_size)
{
    __le64 snapid = cpu_to_le64(snap_id);
    int ret;
    struct {
        u8 order;
        __le64 size;
    } __attribute__ ((packed)) size_buf = { 0 };

    /*调用rbd同步对象访问接口，这里本质就是获取“rbd_header.image_id&quot;对象中omap对应key的内容：
      (1)rbd_device为rbd_dev，即当前RBD设备；
      (2)接口访问对象名称为“rbd_header.image_id&quot;；
      (3)接口访问类(class)名称为&quot;rbd&quot;；
      (4)接口访问方法(method)名称为&quot;get_size&quot;，对应omap中的key为&quot;size&quot;；
      (5)接口访问输入参数为snapid；
      (6)接口访问输入参数长度为snapid的字节长度；
      (7)接口访问输出结果保存内存位置为size_buf；
      (8)接口访问输出结果保存内存最大长度为size_buf空间大小。*/
    ret = rbd_obj_method_sync(rbd_dev, rbd_dev-&amp;gt;header_name,
            &quot;rbd&quot;, &quot;get_size&quot;,
            &amp;amp;snapid, sizeof (snapid),
            &amp;amp;size_buf, sizeof (size_buf));
    dout(&quot;%s: rbd_obj_method_sync returned %d\n&quot;, __func__, ret);
    ...

    if (order) {
        *order = size_buf.order;
        dout(&quot;  order %u&quot;, (unsigned int)*order);
    }
    *snap_size = le64_to_cpu(size_buf.size);

    ...

    return 0;
}

static int rbd_dev_v2_header_onetime(struct rbd_device *rbd_dev)
{
    int ret;

    /*通过rbd_obj_method_sync获取头部对象omap中的&quot;object_prefix&quot;key的内容*/
    ret = rbd_dev_v2_object_prefix(rbd_dev);
    if (ret)
        goto out_err;

    /*通过rbd_obj_method_sync获取头部对象omap中的&quot;features&quot;key的内容*/
    ret = rbd_dev_v2_features(rbd_dev);
    if (ret)
        goto out_err;

    ...

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;224-rbd_obj_method_sync&quot;&gt;&lt;strong&gt;2.2.4. rbd_obj_method_sync&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  通过前文分析，我们看到多个子函数中均使用了rbd_obj_method_sync函数来访问rados对象内容。该函数是对远程rados对象的同步访问接口，是对底层libceph接口(osd_client)的封装：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*
 * Synchronous osd object method call.  Returns the number of bytes
 * returned in the outbound buffer, or a negative error code.
 */
/*osd对象的同步方法调用，返回结果在inbound中。注：上文的注释有问题*/
static int rbd_obj_method_sync(struct rbd_device *rbd_dev,  /*当前RBD设备*/
                        const char *object_name,            /*访问对象名称*/
                        const char *class_name,             /*访问类名称*/
                        const char *method_name,            /*访问方法名称*/
                        const void *outbound,               /*输入参数内存位置*/
                        size_t outbound_size,               /*输入参数大小*/
                        void *inbound,                      /*返回结果内存位置*/
                        size_t inbound_size)                /*返回结果大小*/
{
    struct ceph_osd_client *osdc = &amp;amp;rbd_dev-&amp;gt;rbd_client-&amp;gt;client-&amp;gt;osdc; /*osd_client是底层libceph接口对象*/
    struct rbd_obj_request *obj_request;
    struct page **pages;
    u32 page_count;
    int ret;

    /*
     * Method calls are ultimately read operations.  The result
     * should placed into the inbound buffer provided.  They
     * also supply outbound data--parameters for the object
     * method.  Currently if this is present it will be a
     * snapshot id.
     */
    /*上面这段注释的意思是说针对对象的method call最终其实是对象的读操作。
      outbound表示method输入参数，当前只支持snapid；
      inbound表示返回结果。*/

    /*为返回结果inbound分配新的内存页， 为何要重新分配内存？*/
    page_count = (u32)calc_pages_for(0, inbound_size);
    pages = ceph_alloc_page_vector(page_count, GFP_KERNEL);
    if (IS_ERR(pages))
        return PTR_ERR(pages);

    ret = -ENOMEM;
    /*创建新的rbd_obj_request对象并初始化*/
    obj_request = rbd_obj_request_create(object_name, 0, inbound_size, OBJ_REQUEST_PAGES);
    if (!obj_request)
        goto out;

    obj_request-&amp;gt;pages = pages;
    obj_request-&amp;gt;page_count = page_count;

    /*创建rbd_obj_request在libceph对应的osd_request对象*/
    obj_request-&amp;gt;osd_req = rbd_osd_req_create(rbd_dev, false, obj_request);
    if (!obj_request-&amp;gt;osd_req)
        goto out;
    
    /*初始化osd_request中的op对象，它的类型为CEPH_OSD_OP_CALL，表示调用远端OSD中对象的方法*/
    osd_req_op_cls_init(obj_request-&amp;gt;osd_req, 0, CEPH_OSD_OP_CALL, class_name, method_name);
    if (outbound_size) {
        /*如果存在输入参数outbound，则为其分配新的内存页并复制其内容*/

        struct ceph_pagelist *pagelist;

        pagelist = kmalloc(sizeof (*pagelist), GFP_NOFS);
        if (!pagelist)
            goto out;

        ceph_pagelist_init(pagelist);
        ceph_pagelist_append(pagelist, outbound, outbound_size); /*将outbound中内容复制到pagelist*/
        osd_req_op_cls_request_data_pagelist(obj_request-&amp;gt;osd_req, 0,pagelist); /*将pagelist作为op的输入参数(request_data)*/
    }
    osd_req_op_cls_response_data_pages(obj_request-&amp;gt;osd_req, 0,
            obj_request-&amp;gt;pages, inbound_size, 0, false, false);/*将obj_request-&amp;gt;pages作为op的输出结果(response_data)*/
    rbd_osd_req_format_read(obj_request);

    /*将rbd_object_request对象提交给底层libceph发送给远端osd节点*/
    ret = rbd_obj_request_submit(osdc, obj_request);
    if (ret)
        goto out;

    /*等待远端osd节点返回响应*/
    ret = rbd_obj_request_wait(obj_request);
    if (ret)
        goto out;

    /*远端osd节点返回响应后，result中保存返回结果*/
    ret = obj_request-&amp;gt;result;
    if (ret &amp;lt; 0)
        goto out;

    rbd_assert(obj_request-&amp;gt;xferred &amp;lt; (u64)INT_MAX);
    ret = (int)obj_request-&amp;gt;xferred;
    /*根据远端osd返回的数据量xferred，将数据拷贝到inbound中*/
    ceph_copy_from_page_vector(pages, inbound, 0, obj_request-&amp;gt;xferred);
out:
    if (obj_request)
        rbd_obj_request_put(obj_request);
    else
        ceph_release_page_vector(pages, page_count);

return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  rbd_obj_request_submit中涉及ceph中比较核心的CRUSH算法，我们将在介绍IO流程中对此展开分析。&lt;/p&gt;

&lt;h4 id=&quot;23-rbd_dev_device_setup&quot;&gt;&lt;strong&gt;2.3. rbd_dev_device_setup&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  基于之前获取的信息，我们可以通过内核块层提供的相关接口创建一个新的RBD块设备对象供应用使用：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_dev_device_setup(struct rbd_device *rbd_dev)
{
    int ret;

    /*首先根据全局变量rbd_dev_id_max生成一个新的最大id号*/
    /* generate unique id: find highest unique id, add one */
    rbd_dev_id_get(rbd_dev);

    /*根据id号生成设备名称*/
    /* Fill in the device name, now that we have its id. */
    BUILD_BUG_ON(DEV_NAME_LEN &amp;lt; sizeof (RBD_DRV_NAME) + MAX_INT_FORMAT_WIDTH);
    sprintf(rbd_dev-&amp;gt;name, &quot;%s%d&quot;, RBD_DRV_NAME, rbd_dev-&amp;gt;dev_id);

    /* Get our block major device number. */

    /*通过块层接口register_blkdev注册一个新的块设备*/
    ret = register_blkdev(0, rbd_dev-&amp;gt;name);
    if (ret &amp;lt; 0)
        goto err_out_id;
    rbd_dev-&amp;gt;major = ret;

    /* Set up the blkdev mapping. */

    /*调用块层接口初始化gendisk对象，绑定其IO处理函数为rbd_request_fn(我们将以此为入口来分析IO流程)*/
    ret = rbd_init_disk(rbd_dev);
    if (ret)
        goto err_out_blkdev;

    /*快照相关，暂不关心*/
    ret = rbd_dev_mapping_set(rbd_dev);
    if (ret)
        goto err_out_disk;
    /*设备容量*/
    set_capacity(rbd_dev-&amp;gt;disk, rbd_dev-&amp;gt;mapping.size / SECTOR_SIZE);

    /*在sys目录下添加设备*/
    ret = rbd_bus_add_dev(rbd_dev);
    if (ret)
        goto err_out_mapping;

    /* Everything's ready.  Announce the disk to the world. */

    /*通过add_disk接口在系统中呈现一个新的块设备*/
    set_bit(RBD_DEV_FLAG_EXISTS, &amp;amp;rbd_dev-&amp;gt;flags);
    add_disk(rbd_dev-&amp;gt;disk);

    pr_info(&quot;%s: added with size 0x%llx\n&quot;, rbd_dev-&amp;gt;disk-&amp;gt;disk_name,
        (unsigned long long) rbd_dev-&amp;gt;mapping.size);

    return ret;

err_out_mapping:
    rbd_dev_mapping_clear(rbd_dev);
err_out_disk:
    rbd_free_disk(rbd_dev);
err_out_blkdev:
    unregister_blkdev(rbd_dev-&amp;gt;major, rbd_dev-&amp;gt;name);
err_out_id:
    rbd_dev_id_put(rbd_dev);
    rbd_dev_mapping_clear(rbd_dev);

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-rbd块设备io流程分析&quot;&gt;3. RBD块设备IO流程分析&lt;/h3&gt;

&lt;p&gt;  我们针对IO流程同样沿着从上至下的顺序进行深入分析，对于和上节重复部分我们将不再展开，重点讨论有差异的部分。 IO流程可分为请求下发和响应返回两个阶段。&lt;/p&gt;

&lt;h4 id=&quot;31-io下发rbd_request_fn&quot;&gt;&lt;strong&gt;3.1. IO下发：rbd_request_fn&lt;/strong&gt;&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*当应用向RBD块设备下发读写请求时，最终会调用该函数；
  q代表请求所在的请求队列*/
static void rbd_request_fn(struct request_queue *q)
__releases(q-&amp;gt;queue_lock) __acquires(q-&amp;gt;queue_lock)
{
    struct rbd_device *rbd_dev = q-&amp;gt;queuedata; /*队列私有数据，初始化时指定，代表rbd_dev对象*/
    struct request *rq;
    int result;

    /*通过一个大循环，不断调用blk_fetch_request从请求队列中取出待处理的请求rq*/
    while ((rq = blk_fetch_request(q))) {
        bool write_request = rq_data_dir(rq) == WRITE; /*判断请求读写类型*/
        struct rbd_img_request *img_request;
        u64 offset;
        u64 length;

        ...
        offset = (u64) blk_rq_pos(rq) &amp;lt;&amp;lt; SECTOR_SHIFT; /*计算请求起始位置，从扇区转换成字节*/
        length = (u64) blk_rq_bytes(rq); /*获取请求大小*/
        ...

        result = -ENOMEM;
        /*针对每个rq，这里会生成一个img_request与之对应；其内部会记录请求起始位置、长度和读写类型等信息*/
        img_request = rbd_img_request_create(rbd_dev, offset, length, write_request);
        if (!img_request)
            goto end_request;
        
        /*绑定img_request和rq之间的关系*/
        img_request-&amp;gt;rq = rq;

        /*向img_request中填入bio信息，下面将展开分析*/
        result = rbd_img_request_fill(img_request, OBJ_REQUEST_BIO, rq-&amp;gt;bio);
        /*将img_request递交给底层libceph过行网络发送，下面将展开分析*/
        if (!result)
            result = rbd_img_request_submit(img_request);
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;311-rbd_img_request_fill&quot;&gt;&lt;strong&gt;3.1.1. rbd_img_request_fill&lt;/strong&gt;&lt;/h5&gt;

&lt;h5 id=&quot;312-rbd_img_request_submit&quot;&gt;&lt;strong&gt;3.1.2. rbd_img_request_submit&lt;/strong&gt;&lt;/h5&gt;

&lt;h4 id=&quot;32-io返回&quot;&gt;&lt;strong&gt;3.2. IO返回&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;CRUSH&lt;/p&gt;

&lt;h3 id=&quot;4-libcephko中messenger模块分析&quot;&gt;4. libceph.ko中messenger模块分析&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/RBD-client/&quot;&gt;Rados Block Device之二－客户端内核RBD驱动分析&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/RBD-client/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/RBD-client/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>Rados Block Device之一－概述</title>
        <description>&lt;p&gt;  ceph作为流行的SDS(Software Define Storage)开源实现备受业界关注，从本篇博文开始我们将从它提供的块服务(Rados Block Device)视角对ceph进行一步步深入分析。&lt;/p&gt;

&lt;h3 id=&quot;什么是rados-block-device&quot;&gt;什么是Rados Block Device?&lt;/h3&gt;

&lt;h4 id=&quot;什么是ceph&quot;&gt;&lt;strong&gt;什么是ceph?&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  看一下官方的解释：”Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.” 这里说的是，ceph是一个&lt;strong&gt;统一&lt;/strong&gt;分布式存储系统(功能)；另外，它也具有极佳的性能、可靠性和扩展性。功能和DFx兼具，很完美。&lt;/p&gt;

&lt;p&gt;  之所以说ceph是一个统一存储，是因为用户从功能上看，会认为ceph同时支持对象、块和文件三种形态(如下图所示)。其中块设备形态，就是Rados Block Device，简称RBD。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/rbd_1.jpg&quot; height=&quot;200&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;如何使用rados-block-device&quot;&gt;&lt;strong&gt;如何使用Rados Block Device?&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  为了使大家对ceph(RBD)有一个更直观的理解，下面我们来看看ceph(RBD)的实际使用方法：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.创建ceph集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  ceph是一个分布式系统，内部包含许多计算、存储和网络节点，这里我们暂且把它当做一个黑盒，仅需了解使用ceph之前需要搭建这样一个集群。具体搭建方法我们会在深入分析时介绍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.创建pool&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  pool是ceph中比较重要的一个概念，一个个对象(块、文件最终也转换成对象存储)均放在pool中，管理员针对不同的pool可以采用不同的配置策略。默认情况下，一个ceph集群搭建完成后，就会有一个名为”rbd”的pool，其中专门用来存放RDB对象。我们可以用ceph集群管理命令rados来查询当前已有的pools：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados lspools&lt;/strong&gt;&lt;br /&gt;
rbd&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  这里我们手动建一个新的pool，然后再查询结果：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rados mkpool&lt;/strong&gt; wbpool&lt;br /&gt;
successfully created pool wbpool&lt;br /&gt;
[root@ceph-client]# &lt;strong&gt;rados lspools&lt;/strong&gt;&lt;br /&gt;
rbd&lt;br /&gt;
wbpool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;3.创建块设备&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  接下来我们使用rbd命令在wbpool池中新建一个1G大小的块设备，取名wb：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rbd create&lt;/strong&gt; wb &lt;strong&gt;--pool&lt;/strong&gt; wbpool &lt;strong&gt;--size&lt;/strong&gt; 1G&lt;br /&gt;
[root@ceph-client]# &lt;strong&gt;rbd ls --pool&lt;/strong&gt; wbpool&lt;br /&gt;
wb&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;4.映射块设备&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  创建完块设备之后，我们需要在使用RBD的主机上将它映射成一个可使用的设备：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;rbd map&lt;/strong&gt; wb &lt;strong&gt;--pool&lt;/strong&gt; wbpool&lt;br /&gt;
/dev/rbd0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;5.使用块设备&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  通过映射，我们获知RBD设备的本地访问设备为/dev/rbd0，那么我们就可以像使用本地块设备一样使用RBD块设备，例如将其格式化成文件系统并挂载使用：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;[root@ceph-client]# &lt;strong&gt;mkfs.ext4&lt;/strong&gt; /dev/rbd0&lt;br /&gt;
[root@ceph-client]# &lt;strong&gt;mount&lt;/strong&gt; /dev/rbd0 /mnt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;h3 id=&quot;为什么需要rados-block-device&quot;&gt;为什么需要Rados Block Device?&lt;/h3&gt;

&lt;p&gt;  ceph兼具对象、块、文件三种存储形态，支持PB级超大存储容量，同时具备较好的性能、可靠性和扩展性，因此非常适合企业级应用存储需求，公有云、私有云都有ceph成功部署的案例。Redhat将其收购后，更是进一步加速了它的应用和推广。&lt;/p&gt;

&lt;p&gt;  三种存储形态中，对象和块相对更稳定一些；块设备方式可以完全兼容已有应用，因此使用范围更为广泛。&lt;/p&gt;

&lt;h3 id=&quot;如何实现rados-block-device&quot;&gt;如何实现Rados Block Device?&lt;/h3&gt;

&lt;h4 id=&quot;1-ceph集群结构&quot;&gt;&lt;strong&gt;1. ceph集群结构&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们已经知道ceph是一个集群系统，那么从物理视角深入看，集群内部有哪些部件？他们又是如何相互协作对外提供服务的？下图是ceph官方给出的系统部署图：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/rbd_2.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从图中我们可以看到这些组件(简单地说，可见将组件视为部署了不同ceph程序的服务器)：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Ceph Mon(Monitor)&lt;/strong&gt;，集群的管理者，负责维护集群状态图(如monitor map, OSD map, CRUSH map)，并对客户端提供管理服务(客户端通过Monitor获取全局存储信息并建立访问连接)；集群中通常有多个Monitor，以提升系统可靠性&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Ceph OSD(Object Storage Daemon)&lt;/strong&gt;，对象存储服务提供者，每个OSD提供数据存储空间、处理数据复制、恢复、均衡并通过彼此间的心跳机制为Monitor提供监控信息；一台配置了单独数据硬盘的服务器即可视为一个OSD节点；系统中通常也有多个OSD节点&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Ceph MDS(MetaData Server)&lt;/strong&gt;，为文件服务提供元数据管理功能；可选组件，在RBD中不涉及&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Ceph Client(客户端)&lt;/strong&gt;，数据访问端点，其上部署了各种软件驱动(或库)，最终为应用提供对象、块、文件服务&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Public and Cluster Network&lt;/strong&gt;，集群内部网络平面，Public平面提供客户端到集群的数据访问通信，Cluster平面为OSD之间的心跳和数据复制提供通信，采用两个独立的网络平面可以避免相互影响，提升系统整体服务性能&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  通过组件的协作，正常的数据流大体如下：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;客户端通过驱动首先与Monitor建立网络连接，并完成认证，最后Monitor返回系统中所有的OSD状态图&lt;/li&gt;
    &lt;li&gt;客户端基于OSD状态图、访问对象和CRUSH算法计算出存放对象的主备OSD&lt;/li&gt;
    &lt;li&gt;客户端与主OSD建立连接并发起数据访问请求&lt;/li&gt;
    &lt;li&gt;主OSD执行客户端访问请求，对于写请求会将请求复制到其它备OSD，待其它OSD返回结果后再对客户端返回执行结果&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;2-ceph功能结构&quot;&gt;&lt;strong&gt;2. ceph功能结构&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  切换到功能视角，我们也可以看到ceph整体的软件栈架构：底部是由Monitor、MDS、OSD实现的Rados集群功能；客户端通过librados库为应用提供多语言的对象访问支持，或者通过RADOSGW实现兼客S3和Swift的对象服务，或者通过librbd为应用(如虚拟化程序)提供块服务，或者通过文件驱动为应用提供兼容posix标准的文件服务。注，RADOSGW和librbd基于librados构建，但内核块服务和文件服务直接在内核中实现，其中涵盖了librados的功能。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/rbd_3.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;3-客户端内核rbd驱动分析&quot;&gt;&lt;strong&gt;3. 客户端内核RBD驱动分析&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  参见&lt;a href=&quot;https://rootw.github.io/2018/01/RBD-client/&quot;&gt;Rados Block Device之二－客户端内核RBD驱动分析&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;4-ceph-monitor原理分析&quot;&gt;&lt;strong&gt;4. ceph Monitor原理分析&lt;/strong&gt;&lt;/h4&gt;

&lt;h4 id=&quot;5-ceph-osd原理分析&quot;&gt;&lt;strong&gt;5. ceph OSD原理分析&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/RBD-all/&quot;&gt; Rados Block Device之一－概述&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/RBD-all/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/RBD-all/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【计算子系统】进程管理之一：进程创建</title>
        <description>&lt;p&gt;  进程管理也是计算子系统(CPU&amp;amp;Memory)的核心功能，从本篇博文起，我们开始讨论进程管理。计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是进程什么是进程管理为什么需要进程管理&quot;&gt;什么是进程？什么是进程管理？为什么需要进程管理？&lt;/h3&gt;

&lt;p&gt;  从物理视角说上，进程是CPU上的一段逻辑过程，它的控制(代码段)和数据(数据段)存放于内存。回顾一下计算子系统开篇中描绘的系统结构图(如下图)，进程的执行要素包括CPU中的寄存器和内存段两个部分(虚拟内存段最终会映射到物理内存段)：寄存器代表进程的瞬时运行状态；代码段存储指令，控制进程执行逻辑；数据段存储进程的全局数据；堆栈段存储局部数据和动态数据。从功能视角说，进程是各种“功能”的实现实体，计算机为人们提供的诸如聊天、上网、看视频等各种功能都是通过进程实现的，因此进程有时也被叫作“任务“。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/cpu_low_level.jpg&quot; height=&quot;550&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  进程管理是指与进程相关的一系列动作，如创建、替换、终止、调度、通信等等。进程管理使得一个CPU可以执行若干进程，各进程分时复用CPU的物理资源；内存管理使得多个进程可以共享物理内存；基于上述两个核心功能，计算机系统可以实现多任务并行，大大提升系统运行效率，方使客户使用(想象一下，如果你的计算机一个时刻只能运行一个任务，那将是一种多么糟糕体验)。&lt;/p&gt;

&lt;h3 id=&quot;如何创建进程&quot;&gt;如何创建进程？&lt;/h3&gt;

&lt;p&gt;  进程创建就是新建一个进程，这是进程管理最基本的功能，也是进程生命周期的起点。下面我们就来看看进程创建在Linux内核中是如何实现的。&lt;/p&gt;

&lt;h4 id=&quot;forkvfork和clone&quot;&gt;&lt;strong&gt;fork、vfork和clone&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  从应用程序开发的层次上，我们应该知道创建进程(或线程，即轻量级进程)有fork、vfork和clone三种&lt;a href=&quot;https://rootw.github.io/2017/02/系统调用/&quot;&gt;系统调用&lt;/a&gt;：fork是创建进程标准做法，父子进程共享代码段，但拥有独立数据、堆栈段；vfork是轻量级进程创建方法，父子进程共享代码、数据和堆栈段，子进程运行期间父进程是睡眠的，当子进程结束后父进程才继续运行；clone则提供了更灵活的进程创建方式，可以通过clone_flags来控制创建过程，libpthread库提供的相关API即是通过clone系统调用实现的。大家可以在网上找一些这三种方式的示例代码，动手实验一下以加深理解。到了内核态，这三个系统调用最终都通过do_fork函数来实现其核心功能：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/fork.c:

SYSCALL_DEFINE0(fork)
{
    return do_fork(SIGCHLD, 0, 0, NULL, NULL);
}

SYSCALL_DEFINE0(vfork)
{
    return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, 
        0, NULL, NULL);
}

SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
                int __user *, parent_tidptr,
                int __user *, child_tidptr,
                int, tls_val)
{
    return do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;进程控制块struct-task_struct&quot;&gt;&lt;strong&gt;进程控制块：struct task_struct&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在深入分析do_fork之前，我们首先要明白内核对进程需要有一个抽象的数据表达，基于这种数据表达才能实现各种管理功能。我们将内核中表达进程的数据结构叫做进程控制块，在linux中则是struct task_struct。这里我不打算对task_struct中的各个字段进行逐一描述，因为难以表述清楚，大家可以结合后续的代码流程来深入理解各字段的含义，下面是一幅整体结构图，供参考：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process1_1.jpg&quot; height=&quot;500&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  另外，在早期的linux版本中，进程控制块是包含进程的内核态栈的(通常是8KB大小)。什么是内核态栈？每个进程都有用户态空间和内核态空间两个执行空间，出于安全隔离的考虑，两个空间使用独立的栈，因此内核栈就被安排在了进程控制块中，栈底在高地址端，从高地址往低地址扩展，而进程控制块其它数据则被放置在8K的低地址起始位置处。随着内核的发展，各种功能不断被加入，进程控制块的数据结构也在不断变大，因此就存在挤占内核栈的风险。所以高版本内核将进程控制块和内核栈进行了分离：内核栈的低地址端只保留基本的进程信息，并通过指针对向真正的进程控制块结构，如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process1_2.jpg&quot; height=&quot;300&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;深入do_fork&quot;&gt;&lt;strong&gt;深入do_fork&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  do_fork在传入参数clone_flags的控制下，基于当前进程复制了一个新进程，其大体流程是：先复制当前进程产生新的进程控制块，然后再调度新进程进入运行态。代码框架如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/fork.c:

/*
 *  Ok, this is the main fork-routine.
 *
 * It copies the process, and if successful kick-starts
 * it and waits for it to finish using the VM if required.
 */
long do_fork(unsigned long clone_flags,
        unsigned long stack_start,
        unsigned long stack_size,
        int __user *parent_tidptr,
    int __user *child_tidptr)
{
    struct task_struct *p;
    long nr;

    ...

    /*基于当前进程的task_struct和clone_flags复制新进程*/
    p = copy_process(clone_flags, stack_start, stack_size,
                                child_tidptr, NULL, trace);
    /*
     * Do this prior waking up the new thread - the thread pointer
     * might get invalid after that point, if the thread exits quickly.
     */
    if (!IS_ERR(p)) {
        struct completion vfork;
        struct pid *pid;

        pid = get_task_pid(p, PIDTYPE_PID);
        nr = pid_vnr(pid);

        if (clone_flags &amp;amp; CLONE_PARENT_SETTID)
            put_user(nr, parent_tidptr);

        /*如果clone_flags中置了CLONE_VFORK标置，则需要初始化等待结构体*/
        if (clone_flags &amp;amp; CLONE_VFORK) {
            p-&amp;gt;vfork_done = &amp;amp;vfork;
            init_completion(&amp;amp;vfork);
            get_task_struct(p);
        }

        /*将新创建的进程加入调度队列*/
        wake_up_new_task(p);

        ...

        /*对于VFORK，当前进程(即父进程)需要等待子进程完成后才能继续运行*/
        if (clone_flags &amp;amp; CLONE_VFORK) {
            if (!wait_for_vfork_done(p, &amp;amp;vfork))
                ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
            }

        put_pid(pid);
    } else {
        nr = PTR_ERR(p);
    }
    return nr;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  补充一下关于clone_flags标记的注释说明，建议大家在使用到的代码位置处仔细阅读，以加深理解：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/sched.h:

/*
 * cloning flags:
 */
#define CSIGNAL		0x000000ff	/* signal mask to be sent at exit */
#define CLONE_VM	0x00000100	/* set if VM shared between processes */
#define CLONE_FS	0x00000200	/* set if fs info shared between processes */
#define CLONE_FILES	0x00000400	/* set if open files shared between processes */
#define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
#define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
#define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
#define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
#define CLONE_THREAD	0x00010000	/* Same thread group? */
#define CLONE_NEWNS	0x00020000	/* New namespace group? */
#define CLONE_SYSVSEM	0x00040000	/* share system V SEM_UNDO semantics */
#define CLONE_SETTLS	0x00080000	/* create a new TLS for the child */
#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */
#define CLONE_CHILD_CLEARTID	0x00200000	/* clear the TID in the child */
#define CLONE_DETACHED		0x00400000	/* Unused, ignored */
#define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
#define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
/* 0x02000000 was previously the unused CLONE_STOPPED (Start in stopped state)
and is now available for re-use. */
#define CLONE_NEWUTS		0x04000000	/* New utsname group? */
#define CLONE_NEWIPC		0x08000000	/* New ipcs */
#define CLONE_NEWUSER		0x10000000	/* New user namespace */
#define CLONE_NEWPID		0x20000000	/* New pid namespace */
#define CLONE_NEWNET		0x40000000	/* New network namespace */
#define CLONE_IO		0x80000000	/* Clone io context */

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接下来深入看一下核心函数copy_process，它主要完成了页表和寄存器值的复制，这里我们略去cgroup和一些非重点代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/fork.c:

/*
 * This creates a new process as a copy of the old one,
 * but does not actually start it yet.
 *
 * It copies the registers, and all the appropriate
 * parts of the process environment (as per the clone
 * flags). The actual kick-off is left to the caller.
 */
static struct task_struct *copy_process(unsigned long clone_flags,
            unsigned long stack_start,
            unsigned long stack_size,
            int __user *child_tidptr,
            struct pid *pid,
            int trace)
{
    int retval;
    struct task_struct *p;
    
    ...
    /*分配task_struct结构内存和thread_info页，并将当前进程相关信息复制到对应内存字段*/
    retval = -ENOMEM;
    p = dup_task_struct(current);
    if (!p)
        goto fork_out;

    ...
    /* Perform scheduler related setup. Assign this task to a CPU. */
    sched_fork(p);

    ...
    /*新建并复制task_struct中files字段，它表示已打开文件；
      如果CLONE_FILES置位，则共享当前进程的files*/
    retval = copy_files(clone_flags, p);

    ...
    /*新建并复制task_struct中的fs字段，它表示当前目录；
      如果CLONE_FS置位，则共享当前进程的fs*/
    retval = copy_fs(clone_flags, p);

    ...
    /*复制信号及信号处理函数*/
    retval = copy_sighand(clone_flags, p);
    ...
    retval = copy_signal(clone_flags, p);

    ...
    /*新建并复制mm_struct，并完成页表复制；
      如果CLONE_VM置位，则共享当前进程的mm_struct*/
    retval = copy_mm(clone_flags, p);

    ...
    /*复制寄存器值*/
    retval = copy_thread(clone_flags, stack_start, stack_size, p);

    ...
    return p;
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
{
    struct mm_struct *mm, *oldmm;
    int retval;

    ...
    tsk-&amp;gt;mm = NULL;
    tsk-&amp;gt;active_mm = NULL;

    /*
     * Are we cloning a kernel thread?
     *
     * We need to steal a active VM for that..
     */
    oldmm = current-&amp;gt;mm;
    if (!oldmm)
        return 0;

    /*如果CLONE_VM置位，则共享当前进程mm_struct*/
    if (clone_flags &amp;amp; CLONE_VM) {
        atomic_inc(&amp;amp;oldmm-&amp;gt;mm_users);
        mm = oldmm;
        goto good_mm;
    }

    retval = -ENOMEM;
    mm = dup_mm(tsk);
    if (!mm)
        goto fail_nomem;

good_mm:
    tsk-&amp;gt;mm = mm;
    tsk-&amp;gt;active_mm = mm;
    return 0;

fail_nomem:
    return retval;
}

/*
 * Allocate a new mm structure and copy contents from the
 * mm structure of the passed in task structure.
 */
struct mm_struct *dup_mm(struct task_struct *tsk)
{
    struct mm_struct *mm, *oldmm = current-&amp;gt;mm;
    int err;

    if (!oldmm)
        return NULL;

    /*分配mm_struct内存*/
    mm = allocate_mm();
    if (!mm)
        goto fail_nomem;
    
    /*复制mm_struct内容，这里没有加锁保护，我理解是因为其中关键字段
      会在后续流程中重新赋值*/
    memcpy(mm, oldmm, sizeof(*mm));
    mm_init_cpumask(mm);

    ...
    /*重新初始化mm_struct中相关字段，这里会重新分配pgd表并将内核空间
      地址映射复制到其中*/
    if (!mm_init(mm, tsk))
        goto fail_nomem;

    if (init_new_context(tsk, mm))
        goto fail_nocontext;

    dup_mm_exe_file(oldmm, mm);

    /*复制用户态vma段和页表*/
    err = dup_mmap(mm, oldmm);
    if (err)
        goto free_pt;

    ...

    return mm;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int copy_thread(unsigned long clone_flags, unsigned long sp,
        unsigned long arg, struct task_struct *p)
{
    int err;
    struct pt_regs *childregs;
    struct task_struct *me = current;

    /*task_stack_page(p)，即p-&amp;gt;stack，为thread_info起始地址；加上THREAD_SIZE
      后为内核栈起始地址*/
    p-&amp;gt;thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;

    /*childregs指向内核栈中保留所有寄存器后的偏移位置*/
    childregs = task_pt_regs(p);
    p-&amp;gt;thread.sp = (unsigned long) childregs;

    /*复制当前进程的用户态栈指针*/
    p-&amp;gt;thread.usersp = me-&amp;gt;thread.usersp;

    /*设置TIF_FORK标记，fork系统调用返回时用来判断是否
      为新生成的进程*/
    set_tsk_thread_flag(p, TIF_FORK);
    ...

    /*对于内核进程，sp中保存的是入口函数指针*/
    if (unlikely(p-&amp;gt;flags &amp;amp; PF_KTHREAD)) {
        /* kernel thread */
        memset(childregs, 0, sizeof(struct pt_regs));
        childregs-&amp;gt;sp = (unsigned long)childregs;
        childregs-&amp;gt;ss = __KERNEL_DS;
        childregs-&amp;gt;bx = sp; /* function */
        childregs-&amp;gt;bp = arg;
        childregs-&amp;gt;orig_ax = -1;
        childregs-&amp;gt;cs = __KERNEL_CS | get_kernel_rpl();
        childregs-&amp;gt;flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;
        return 0;
    }
    
    /*复制当前进程在执行fork系统调用时保存的寄存器状态*/
    *childregs = *current_pt_regs();

    /*子进程的ax寄存器赋为零，该值即fork系统调用的返回值*/
    childregs-&amp;gt;ax = 0;

    /*如果传入sp指针，则更新fork返回后栈指针值*/
    if (sp)
        childregs-&amp;gt;sp = sp;

    ...
    return err;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;零号进程与一号进程&quot;&gt;&lt;strong&gt;零号进程与一号进程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  系统中有两个比较特殊的进程，即零号和一号进程。零号进程是内核初始化过程中最早产生的进程，最终成为bsp(SMP系统中的启动CPU)上的idle进程(swapper)。零号进程会创建一号进程，由一号进程完成部分初始化动作并拉起shell进程。最终一号进程成为所有孤儿进程的回收进程而长期存在于系统之中。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/进程创建/&quot;&gt;【计算子系统】进程管理之一：进程创建&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【计算子系统】内存管理之六：初始化</title>
        <description>&lt;p&gt;  初始化过程往往是比较冗长且乏味的，如果一接触就开始学习这块内容会让人烦闷。在了解内存管理几个核心功能模块后，我们再回头看看内存管理的初始化过程，相信大家会有新的收获。计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;低级阶段-汇编实现&quot;&gt;低级阶段-汇编实现&lt;/h3&gt;

&lt;p&gt;  我们可以把整个内存初始化过程大体分成低级阶段和高级阶段两个过程。低级阶段主要是用低级汇编语言实现的，又可细分成三步：&lt;/p&gt;

&lt;p&gt;  首先是BIOS，它在计算机启动的最初阶段检测了物理内存的分布情况，并在x86实模式的高端地址处(1M内存以下)记录了这些内存分布信息(称为e820表)。e820表是一个数组，每一项记录了一段连续内存信息，包含起始地址、结束地址和内存类型。内存类型分为普通内存(RAM)、保留内存(RESERVED，如BIOS内存)、ACPI表空间等。此外，对于NUMA结构系统，BIOS还将产生ACPI的SRAT表，用来记录每个numa节点的内存分布。&lt;/p&gt;

&lt;p&gt;  接着BIOS通过引导GRUB，再由GRUB将内核实模式部分和保护模式部分加载进内存。之后GRUB跳转到内核实模式部分执行，此时内核通过int指令获取e820表信息，由此得知物理内存分布。&lt;/p&gt;

&lt;p&gt;  在低级阶段的最后，内核进入保护模式，设置了高端虚拟地址到物理地址的线性映射，并将栈空间设定在了0号进程(BSP启动核对应的IDLE进程)的栈空间，随后就进入了高级阶段。&lt;/p&gt;

&lt;h3 id=&quot;高级阶段-c语言实现&quot;&gt;高级阶段-C语言实现&lt;/h3&gt;

&lt;p&gt;  高级阶段的入口函数是start_kernel，与内存管理相关的部分主要setup_arch中。&lt;/p&gt;

&lt;p&gt;  首先，我们会看到一些以memblock_打头的函数，这是干嘛的？我们应该知道，完整的内存初始化过程完成之前，正常的内存申请和释放功能是没法使用的。但是内核初始化过程中也需要动态分配内存，这就产生了矛盾。内核的做法是在初始化阶段使用一个简便的内存管理方法，这就是memblock。它从e820表中获知内存的分布情况，并以简单的连续分配方法来管理内存。所以在内核初始化阶段，它使用memblock来进行内存的申请和释放。&lt;/p&gt;

&lt;p&gt;  接着在init_mem_mapping中，内核将direct mapping区线性映射到整个物理空间。如此一来，内核便可访问所有物理内存了。大家可以回顾下Linux 3.10/Documentation/x86/x86_64/mm.txt。&lt;/p&gt;

&lt;p&gt;  再接着在initmem_init中，通过读取ACPI的SRAT表获知NUMA信息，并将这些信息更新到memblock中，此时内核就得知了完整的内存分布信息：有多少段内存，每段内存分别属于哪个NUMA节点。这里内核会为每个节点创建struct pglist_data结构，用来记录内存分布信息。&lt;/p&gt;

&lt;p&gt;  随后进入了和分页相关的初始化过程paging_init。这里又涉及内核的sparse_memory特性，这又是什么鬼？内核在管理内存时，是需要分配独立的内存页来记录内存信息的，比如struct page数组。早期的内核是按最大物理内存量固定分配，对于小内存场景，这种方法问题不大。然而当前系统内存越来越大(x86_64最大支持2^46)，而且内存可能动态增加(热插内存条)时，固定分配的方法就不适用了。sparse memory则以更灵活的方式来分配管理内存，如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory6_1.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  内核将所有内存(最大2^46)划分成区(128M)，并通过一个数组mem_section来记录每个区的信息。对于数组mem_section，是按4K页为粒度来分配空间，本质可以视为一个两维数组。内核通过memblock记录的信息为可用内存动态分配管理空间，不可用的区间将置为空，或者将section_mem_map低位清零(代表对应的区不存在)。在此过程中，内核也会对struct page数组分配空间，并将地址记录到section_mem_map中。&lt;/p&gt;

&lt;p&gt;  完成sparse memory的初始化后，内核通过free_area_init_nodes来初始化内核NUMA节点的空闲内存信息。此时页管理系统没有任何空闲内存。那么空闲内存是怎么来的？这就回到start_kernel中，它先通过build_all_zonelists建立分配zone序列，再通过mem_init调用free_all_bootmem，这里会把memblock中的空闲内存释放到页管理系统中。此后，内核就可以使用正常的alloc_pages函数来分配页了。&lt;/p&gt;

&lt;p&gt;  在2017年的最后几天里，终于把内核中有关内存管理方面的基础内容分析完了，但整个内存管理涉及的知识面非常广，还包括：反向映射、大页内存管理、KSM、cgroup_memory等多个方面。要想真正精通内存管理，需要坚持长期学习，并不断总结与实践。2018，继续努力！&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/12/内存初始化/&quot;&gt;【计算子系统】内存管理之六：内存初始化&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%88%9D%E5%A7%8B%E5%8C%96/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%88%9D%E5%A7%8B%E5%8C%96/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【计算子系统】内存管理之五：内存压缩</title>
        <description>&lt;p&gt;  本节将讨论内存压缩，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是内存压缩为什么需要它&quot;&gt;什么是内存压缩？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  内存压缩(memory compaction)是指把应用程序正在使用的物理页内容迁移(migrate)到其它物理页中，以释放原有物理页，从而方便空闲页的合并以形成大段连续空闲内存，有效避免内存碎片的产生。&lt;/p&gt;

&lt;p&gt;  前文介绍优化型伙伴算法时，我们已经看到空闲内存被分成不可移动、可回收、可移动几个类别，应用程序从可移动类别中获取内存页。连续的内存分配和释放如果产生碎片无法满足当前内存分配请求时，内核进入慢速分配流程触发内存压缩，进行碎片整理，从而最大限度满足分配需求。&lt;/p&gt;

&lt;h3 id=&quot;如何实现&quot;&gt;如何实现？&lt;/h3&gt;

&lt;p&gt;  从整体流程上看，内存分配函数alloc_pages在快速流程get_page_from_freelist无法满足时，会进入慢速流程__alloc_pages_slowpath。慢速流程中进行一系列尝试后如果仍然无法满足分配需求，则进入compact_zone内存压缩流程。&lt;/p&gt;

&lt;p&gt;  内存压缩是通过迁移实现，迁移就涉及源端和目的端，那么内存压缩过程所要考虑的第一个问题就是源端页从哪来？目的端页又从哪来？Linux内核采用的策略是把低地址的页移动到高地址，从而在低地址端形成大段连续内存。因此内核从低往高扫描内存区(zone)，收集需要迁移的源端内存页；另一方面从高往低扫描，获取迁移目的端的空闲页。迁移源端页的收集在isolate_migratepage函数中实现，目的端页的收集在isolate_freepages函数中实现，有兴趣的读者可以深入进一步分析。&lt;/p&gt;

&lt;p&gt;  迁移源端和目的端确认后，下面要考虑的就是迁移动作该如何完成。粗略来说，需要将源端页内容拷贝到目的页、将文件映射关系中的源端页替换成目的端页、解除源端页在页表中的映射并重新映射目的页。&lt;/p&gt;

&lt;p&gt;  下图展示了使用中的物理页的映射关系：进程的虚拟地址段(vma)通过mmap被映射到文件段；文件内容读取到物理内存后以缓存页方式被组织成文件映射树；页表将虚拟地址映射到文件缓存页，供MMU使用实现地址转换。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory5_1.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  基于上图，我们看看迁移过程：总体分两步，首先解映射源端页；其次是将映射树的源端页替换成目的页，包括内存页内容的拷贝。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory5_2.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory5_3.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  有的读者可能会问，怎么最后没有重新将虚拟地址映射到目的页呢？内核采用的是lazy的方式，当进程访问缺页时会完成重新映射。
详细代码在migrate_pages中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/migrate.c:

/*
 * migrate_pages - migrate the pages specified in a list, to the free pages
 *		   supplied as the target for the page migration
 *
 * @from:		The list of pages to be migrated.
 * @get_new_page:	The function used to allocate free pages to be used
 *			as the target of the page migration.
 * @private:		Private data to be passed on to get_new_page()
 * @mode:		The migration mode that specifies the constraints for
 *			page migration, if any.
 * @reason:		The reason for page migration.
 *
 * The function returns after 10 attempts or if no pages are movable any more
 * because the list has become empty or no retryable pages exist any more.
 * The caller should call putback_lru_pages() to return pages to the LRU
 * or free list only if ret != 0.
 *
 * Returns the number of pages that were not migrated, or an error code.
 */
int migrate_pages(struct list_head *from, new_page_t get_new_page,
                unsigned long private, enum migrate_mode mode, int reason)
{
    int retry = 1;
    int nr_failed = 0;
    int nr_succeeded = 0;
    int pass = 0;
    struct page *page;
    struct page *page2;
    int swapwrite = current-&amp;gt;flags &amp;amp; PF_SWAPWRITE;
    int rc;

    if (!swapwrite)
        current-&amp;gt;flags |= PF_SWAPWRITE;

    for(pass = 0; pass &amp;lt; 10 &amp;amp;&amp;amp; retry; pass++) {
        retry = 0;

        list_for_each_entry_safe(page, page2, from, lru) {
            cond_resched();

            /*解映射页表并移动内存页，内部将调用__unmap_and_move*/
            rc = unmap_and_move(get_new_page, private, page, pass &amp;gt; 2, mode);

            switch(rc) {
            case -ENOMEM:
                goto out;
            case -EAGAIN:
                retry++;
                break;
            case MIGRATEPAGE_SUCCESS:
                nr_succeeded++;
                break;
            default:
                /* Permanent failure */
                nr_failed++;
                break;
            }
        }
    }
    rc = nr_failed + retry;
out:
    if (nr_succeeded)
        count_vm_events(PGMIGRATE_SUCCESS, nr_succeeded);
    if (nr_failed)
        count_vm_events(PGMIGRATE_FAIL, nr_failed);
    trace_mm_migrate_pages(nr_succeeded, nr_failed, mode, reason);

    if (!swapwrite)
        current-&amp;gt;flags &amp;amp;= ~PF_SWAPWRITE;

    return rc;
}

static int __unmap_and_move(struct page *page, struct page *newpage,
                                int force, enum migrate_mode mode)
{
    int rc = -EAGAIN;
    int remap_swapcache = 1;
    struct mem_cgroup *mem;
    struct anon_vma *anon_vma = NULL;

    /*锁定迁移源端页*/
    if (!trylock_page(page)) {
        ...
    }

    if (PageWriteback(page)) {
        /*跳过或等待writeback页回刷完成*/
        ...
    }
    ...

    /* Establish migration ptes or remove ptes */
    try_to_unmap(page, TTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);

skip_unmap:
    if (!page_mapped(page))
        rc = move_to_new_page(newpage, page, remap_swapcache, mode);

    ...
out:
    return rc;
}

static int move_to_new_page(struct page *newpage, struct page *page,
                    int remap_swapcache, enum migrate_mode mode)
{
    struct address_space *mapping;
    int rc;

    /*
     * Block others from accessing the page when we get around to
     * establishing additional references. We are the only one
     * holding a reference to the new page at this point.
     */
    if (!trylock_page(newpage))
        BUG();

    /* Prepare mapping for the new page.*/
    newpage-&amp;gt;index = page-&amp;gt;index;
    newpage-&amp;gt;mapping = page-&amp;gt;mapping;
    if (PageSwapBacked(page))
        SetPageSwapBacked(newpage);
    
    mapping = page_mapping(page);
    /*调用底层文件系统接口实现页迁移*/
    if (!mapping)
        rc = migrate_page(mapping, newpage, page, mode);
    else if (mapping-&amp;gt;a_ops-&amp;gt;migratepage)
    /*
     * Most pages have a mapping and most filesystems provide a
     * migratepage callback. Anonymous pages are part of swap
     * space which also has its own migratepage callback. This
     * is the most common path for page migration.
     */
        rc = mapping-&amp;gt;a_ops-&amp;gt;migratepage(mapping, newpage, page, mode);
    else
        rc = fallback_migrate_page(mapping, newpage, page, mode);
    if (rc != MIGRATEPAGE_SUCCESS) {
        newpage-&amp;gt;mapping = NULL;
    } else {
        if (remap_swapcache)
            remove_migration_ptes(page, newpage);
        page-&amp;gt;mapping = NULL;
    }

    unlock_page(newpage);

    return rc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，慢速分配中的核心的内存压缩流程已分析完成，更底层的细节涉及具体文件系统实现，大家可以继续深入分析。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/12/内存压缩/&quot;&gt;【计算子系统】内存管理之五：内存压缩&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%8E%8B%E7%BC%A9/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%8E%8B%E7%BC%A9/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
  </channel>
</rss>
