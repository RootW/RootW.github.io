<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>吴斌</title>
    <description>欢迎来到我的技术博客~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 13 Apr 2018 15:10:58 +0800</pubDate>
    <lastBuildDate>Fri, 13 Apr 2018 15:10:58 +0800</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>【Rados Block Device】八、OSD原理分析－FileStore模块</title>
        <description>&lt;p&gt;  从前面的博文分析中，我们知道OSD模块在请求处理的最后阶段会向ObjectStore发起操作请求，ObjectStore负责对象的实际存储功能，有filestore、bluestore、memstore、kstore等多种存储方式，这里我们以基本的filestore为例展开分析。&lt;/p&gt;

&lt;h3 id=&quot;filestore模块中对象流程概览&quot;&gt;FileStore模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  我们照例先整体来看一下FileStore模块的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_fs.jpg&quot; height=&quot;500&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  OSD模块在tp_osd_tp线程上下文的最后阶段，通过queue_transactions调用FileStore模块功能将操作请求以日志的方式提交到日志队列中，至此tp_osd_tp线程中的工作就完成了。后续由一个独立的日志写入线程journal_write从日志队列中取出操作日志并调用文件系统写入接口将日志操作写入实际的日志文件中(&lt;strong&gt;注，这里我们以journal ahead模式为例进行说明&lt;/strong&gt;)；日志写入完成后，通过queue_completions_thru接口将日志完成回调任务放入fn_jrn_objstorep线程的完成队列中。fn_jrn_objstore线程从完成队列中取出回调任务并立即调用回调，回调任务中会执行两个并发的子任务：一方面是通过op_queue将操作递交给OpWQ进行实际的数据落盘动作；另一方面是把OSD模块传入的回调任务放入fn_odsk_fstore线程池中进行处理。OpWQ队列对应tp_fstore_op线程，它会从队列中取出操作请求，并执行实际的数据落盘动作，落盘完成后再把落盘回调任务放入到fn_appl_fstore队列中进行回调处理。fn_odsk_fstore在处理OSD模块的回调任务时，会把OSD复本操作减一(因为写入日志后就认为本地写入操作完成了)，如果远端OSD复本也完成了，那就会对客户端返回操作结果。此外，还有一个fileStore_sync线程负责日志空间的回收，便于重复使用日志文件。&lt;/p&gt;

&lt;h3 id=&quot;filestore模块中类的概览&quot;&gt;FileStore模块中类的概览&lt;/h3&gt;

&lt;p&gt;  下面我们来看看FileStore模块中涉及的主要类及其关系：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_fs_class.jpg&quot; height=&quot;550&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;JournalingObjectStore是FileStore的父类，继承自抽象父类ObjectStore。JournalingObjectStore包含一个Journal对象和一个Finisher对象，分别代表日志操作对象和日志操作完成后的回调对象。&lt;/li&gt;
    &lt;li&gt;FileJournal继承了Journal类，以文件的方式实现了日志的主要操作功能。内部有一个专门的journal_write线程负责日志的落盘操作。&lt;/li&gt;
    &lt;li&gt;FileStore是核心类，继承自JournalingObjectStore。OpWQ队列用来存放数据落盘请求，op_tp线程池会从该队列中取出请求执行落盘动作。sync_thread线程负责数据同步与日志空间回收。ondisk_finishers用来处理日志落盘后的回调；apply_finishers用来处理数据落盘后的回调。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  filestore的初始化由FileStore::mount完成，其调用栈如下所示，大家可以自行展开阅读：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    \-OSD::init()
        \-FileStore::mount()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-tp_osd_tp线程上下文的日志提交&quot;&gt;&lt;strong&gt;2. tp_osd_tp线程上下文的日志提交&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来分析一下tp_osd_tp线程中的日志提交过程，其栈心处理函数为FileStore::queue_transactions：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FileStore::queue_transactions()
    |-ObjectStore::Transaction::collect_context()
    |-FileStore::build_op()
    |-JournalFile::prepare_entry()
    \-JournalingObjectStore::_op_journal_transaction()
        \-JournalFile::submit_entry()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.cc:

int FileStore::queue_transactions(Sequencer *posr, vector&amp;lt;Transaction&amp;gt;&amp;amp; tls,
        TrackedOpRef osd_op, ThreadPool::TPHandle *handle)
{
    Context *onreadable;
    Context *ondisk;
    Context *onreadable_sync;

    /*将所有事务中的回调分类进行汇总，onreadable代表on_applied，即数据落盘后的回调；
      ondisk代表on_commit代表日志落盘后的回调；onreadable_sync代表on_applied_sync代表
      数据落盘并同步完成后的回调*/
    ObjectStore::Transaction::collect_contexts(tls, &amp;amp;onreadable, &amp;amp;ondisk, &amp;amp;onreadable_sync);
    ...

    if (journal &amp;amp;&amp;amp; journal-&amp;gt;is_writeable() &amp;amp;&amp;amp; !m_filestore_journal_trailing) {
        /*封装一个新的操作op*/
        Op *o = build_op(tls, onreadable, onreadable_sync, osd_op);
        /*准备日志块内容，内部包含操作类型和操作数据*/
        int orig_len = journal-&amp;gt;prepare_entry(o-&amp;gt;tls, &amp;amp;tbl);
        
        /*OSD中对日志主要有两使用方式：parallel和writeahead。parallel代表日志落盘和数据落盘同时发起
          writeahead代表日志先落盘，成功后再发起数据落盘。这里我们主要讨论writeahead模式*/
        if (m_filestore_journal_parallel) {
            ...
        }else if (m_filestore_journal_writeahead) {
            /*提交日志到日志队列，并封装一个回调对象C_JournalAhead*/
            _op_journal_transactions(tbl, orig_len, o-&amp;gt;op,
                new C_JournaledAhead(this, osr, o, ondisk), osd_op);
        }
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/JournalingObjectStore.cc:

void JournalingObjectStore::_op_journal_transactions(
        bufferlist&amp;amp; tbl, uint32_t orig_len, uint64_t op,
        Context *onjournal, TrackedOpRef osd_op)
{
    ...
    journal-&amp;gt;submit_entry(op, tbl, orig_len, onjournal, osd_op);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  C_JournaledAhead回调对象最终会调用FileStore::_journaled_ahead，代码如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.cc:

struct C_JournaledAhead : public Context {
    FileStore *fs;
    FileStore::OpSequencer *osr;
    FileStore::Op *o;
    Context *ondisk;

    C_JournaledAhead(FileStore *f, FileStore::OpSequencer *os, FileStore::Op *o, Context *ondisk):
        fs(f), osr(os), o(o), ondisk(ondisk) { }
    void finish(int r) override {
        fs-&amp;gt;_journaled_ahead(osr, o, ondisk);
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;3-journal_write线程工作过程&quot;&gt;&lt;strong&gt;3. journal_write线程工作过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  tp_osd_tp线程将日志提交到日志队列是通过JournalFile::submit_entry实现的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/JournalFile.cc:

void FileJournal::submit_entry(uint64_t seq, bufferlist&amp;amp; e, uint32_t orig_len,
        Context *oncommit, TrackedOpRef osd_op)
{
    ...
    /*先在compleions列表中记录回调对象*/
    completions.push_back(
        completion_item(seq, oncommit, ceph_clock_now(), osd_op));

    /*唤醒journal_write线程*/
    if (writeq.empty())
        writeq_cond.Signal();

    /*将待提交日志放入日志队列writeq中*/
    writeq.push_back(write_item(seq, e, orig_len, osd_op));
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里我们看看journal_write线程的工作原理，线程入口函数为FileJournal::write_thread_entry：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/JournalFile.cc:

void FileJournal::write_thread_entry()
{
    ...
    while (1) {
        ...

        bufferlist bl;
        /*从日志队列writeq中取出若干日志项*/
        int r = prepare_multi_write(bl, orig_ops, orig_bytes);
        ...

        /*将日志项写入到日志文件中，如果非direct io，会执行fdatasync；执行完成后将回调对象送入fn_jrn_objstore处理*/
        do_write(bl);

        /*记录日志写入完成事件*/
        complete_write(orig_ops, orig_bytes);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;4-fn_jrn_objstore回调&quot;&gt;&lt;strong&gt;4. fn_jrn_objstore回调&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  日志完全落盘执行的回调即是前文指出的FileStore::_journaled_ahead，它会触发两个并行的操作：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;通过queue_op触发tp_fstore_op的数据落盘操作；&lt;/li&gt;
    &lt;li&gt;通地Finisher::queue触发OSD模块的回调&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.cc:

void FileStore::_journaled_ahead(OpSequencer *osr, Op *o, Context *ondisk)
{
    queue_op(osr, o);
    
    if (ondisk) {
        ondisk_finishers[osr-&amp;gt;id % m_ondisk_finisher_num]-&amp;gt;queue(ondisk);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;5a-tp_fstore_op线程的数据落盘及fn_appl_fstore线程池的落盘回调&quot;&gt;&lt;strong&gt;5.(a) tp_fstore_op线程的数据落盘及fn_appl_fstore线程池的落盘回调&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  queue_op操作将落盘请求放入op_wq队列后，将由tp_fstore_op线程处理，该线程将周期性地调用_process和_process_finish函数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.h:

struct OpWQ : public ThreadPool::WorkQueue&amp;lt;OpSequencer&amp;gt; {
    FileStore *store;
    ...

    void _process(OpSequencer *osr, ThreadPool::TPHandle &amp;amp;handle) override {
        store-&amp;gt;_do_op(osr, handle);
    }
    
    void _process_finish(OpSequencer *osr) override {
        store-&amp;gt;_finish_op(osr);
    }

} op_wq;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  FileStore::_do_op负责将数据落盘，FileStore::_finish_op负责将回调对象送入fn_appl_fstore线程池进入处理。&lt;/p&gt;

&lt;h4 id=&quot;5b-fn_odsk_fstore线程池对osd模块传入的日志落盘回调的处理&quot;&gt;&lt;strong&gt;5.(b) fn_odsk_fstore线程池对OSD模块传入的日志落盘回调的处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  OSD传入的日志落盘回调对象为C_OSD_OnOpCommit，其实现代码为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/ReplicatedBackend.cc:

class C_OSD_OnOpCommit : public Context {
    ReplicatedBackend *pg;
    ReplicatedBackend::InProgressOp *op;
public:
    C_OSD_OnOpCommit(ReplicatedBackend *pg, ReplicatedBackend::InProgressOp *op) 
        : pg(pg), op(op) {}
    void finish(int) override {
        pg-&amp;gt;op_commit(op);
    }
};

void ReplicatedBackend::op_commit(InProgressOp *op)
{
    /*将当前多复本操作等待对象中减去本地OSD，代表本地复本已完成*/
    op-&amp;gt;waiting_for_commit.erase(get_parent()-&amp;gt;whoami_shard());

    /*如果等待队列为空，表示远端OSD复本操作也完了，那就可以执行复本操作全部完成后的回调*/
    if (op-&amp;gt;waiting_for_commit.empty()) {
        op-&amp;gt;on_commit-&amp;gt;complete(0); /*复本操作全部完成后将给客户端返回结果*/
        op-&amp;gt;on_commit = 0;
    }
    if (op-&amp;gt;done()) {
        assert(!op-&amp;gt;on_commit &amp;amp;&amp;amp; !op-&amp;gt;on_applied);
        in_progress_ops.erase(op-&amp;gt;tid);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/04/RBD-OSD-3&quot;&gt;【Rados Block Device】八、OSD原理分析－FIleStore模块&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Apr 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/RBD-OSD-3/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/RBD-OSD-3/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】七、OSD原理分析－OSD模块</title>
        <description>&lt;p&gt;  OSD进程从网络收到客户端的读写请求后，交由OSD模块执行核心的请求处理逻辑，本篇博文将讨论OSD模块的实现原理。&lt;/p&gt;

&lt;h3 id=&quot;osd模块中对象流程概览&quot;&gt;OSD模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  我们先整体来看一下OSD模块的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_osd.jpg&quot; height=&quot;500&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  回顾前文对SimpleMessenger的分析，我们看到ms_pipe_read线程从网络中收到请求消息后，通过fast_dispatch接口将消息分发给OSD对象进行处理。OSD对象针对接收到的每一个消息，都会将它们放入一个工作队列中(ShardedOpWQ，片式队列)。到这里，ms_pipe_read线程的分发动作就执行完了。对于一个片式队列，会有若干个处理线程，即图中的tp_osd_tp线程，每个处理线程负责处理不同分片中的消息。它们将各自分片中的消息取出后，找到每个消息对应的PG对象(Placement Group)，进而将消息封装成操作(op)转给PG对象处理。PG对象针对读操作将直接从filestore中读出内容并返回响应消息给客户端；而对于写操作，PG对象将请求以事务(Transaction)的方式提交给PGBackend对象(本文主要讨论ReplicatedBackend)，最终事务内的操作会转变成对filestore的操作(我们将在独立的博文中讨论filestore模块的实现原理)。&lt;/p&gt;

&lt;p&gt;  为什么一个请求消息要在两个线程(ms_pipe_read和tp_osd_tp)间传递处理？其实这里体现了ceph一个核心的设计理念：&lt;strong&gt;流水线&lt;/strong&gt;。将请求的处理分成多个步骤，每个步骤放在不同的线程中处理；请求从一个线程流动到下一个线程，类似工产里的流水流；这样可以大大提升处理请求的吞吐量(即每秒完成的请求数量)。那么时延呢？&lt;/p&gt;

&lt;h3 id=&quot;osd模块中类的概览&quot;&gt;OSD模块中类的概览&lt;/h3&gt;

&lt;p&gt;  下面我们来看看OSD模块中涉及的主要类及其关系：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_osd_class.jpg&quot; height=&quot;500&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;OSD是核心类，它内部包含两个Messenger对象指针，分别指向cluster网络(cluster_messenger)和public网络(client_messenger)。store指向后端对象存储池，用来进行实际的对象存取操作。内部包含一个片式队列(ShardedOpWQ)和一个处理线程池(SharedThreadPool)，OSD对象将请求消息放入片式队列中，再由不同的处理线程从队列中取出消息进行下一步处理。OSD中还包含全局的OSDMap和映射到本OSD的所有PG对象。&lt;/li&gt;
    &lt;li&gt;ShardedOpWQ类代表片式队列，number_shards是队列中总的分片数，每个分片都包含一个ShardedData，其内部有一个优先级队列用来接收请求消息(通过_enqueue操作)。每个片式队列都关联一个处理线程池，池中的每个线程都通过_process接口从对应队列中取出消息进行后续处理。&lt;/li&gt;
    &lt;li&gt;PrimaryLogPG类继承PG类，代表具体的Placement Group的一种实现。每个PrimaryLogPG对象包含一个pgbackend对象，该对象负责数据复本的处理。目前有两种数据复本的实现方式，Replicated(复制)和EC(校验码)，分别对应ReplicatedBackend和ECBackend。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  OSD模块的初始化代码位于OSD:init中，代码流程比较锁碎。这里我们给出初始化调用栈，并作一些简要说明：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;在挂载后端ObjectStore后，OSD模块调用load_pgs开始加载后端ObjectStore中保存的pg对象。这里先枚举ObjectStore中所有的pg，例如对于filestore，将查找CURRENT目录下的所有子目录(每个子目录代表一个pg)；然后打开该pg(生成具体的PG对象，如PrimaryLogPG)并读取pg状态信息。&lt;/li&gt;
    &lt;li&gt;启动osd_op_tp线程池，开始对op_shardedwq中的消息进行处理。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    \-OSD::init()
        |-OSD::load_pgs()
        |   |-ObjectStore::list_collections()
        |   |-OSD::_open_lock_pg()
        |   |-ObjectStore::open_collections()
        |   \-PG::read_state()
        \-osd_op_tp.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-请求处理过程&quot;&gt;&lt;strong&gt;2. 请求处理过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来分析一下ms_pipe_read中的请求消息分发流程，其栈心处理函数为OSD::ms_fast_dispatch：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OSD::ms_fast_dispatch()
    |-OpTracker::create_request()
    \-OSD::dispatch_session_waiting()
        \-OSD::enqueue_op()
            \-ShardedOpWQ::queue()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

void OSD::ms_fast_dispatch(Message *m)
{
    /*将消息封装成op*/
    OpRequestRef op = op_tracker.create_request&amp;lt;OpRequest, Message*&amp;gt;(m);
    ...
    
    if (m-&amp;gt;get_connection()-&amp;gt;has_features(CEPH_FEATUREMASK_RESEND_ON_SPLIT) ||
        m-&amp;gt;get_type() != CEPH_MSG_OSD_OP) {
        // queue it directly
        ...
    } else {
        // legacy client, and this is an MOSDOp (the *only* fast dispatch
        // message that didn't have an explicit spg_t); we need to map
        // them to an spg_t while preserving delivery order.

        /*将op放入当前连接的会话上下文中，待获取到OSDMap后进行处理*/
        Session *session = static_cast&amp;lt;Session*&amp;gt;(m-&amp;gt;get_connection()-&amp;gt;get_priv());
        if (session) {
            {
                Mutex::Locker l(session-&amp;gt;session_dispatch_lock);
                op-&amp;gt;get();
                session-&amp;gt;waiting_on_map.push_back(*op);
                OSDMapRef nextmap = service.get_nextmap_reserved();
                dispatch_session_waiting(session, nextmap);
                service.release_map(nextmap);
            }
            session-&amp;gt;put();
        }
    } 
}

void OSD::dispatch_session_waiting(Session *session, OSDMapRef osdmap)
{
    /*遍历session中waiting_on_map中的每个op进行处理*/
    auto i = session-&amp;gt;waiting_on_map.begin();
    while (i != session-&amp;gt;waiting_on_map.end()) {
        OpRequestRef op = &amp;amp;(*i);
        const MOSDFastDispatchOp *m = static_cast&amp;lt;const MOSDFastDispatchOp*&amp;gt;(op-&amp;gt;get_req());
        ...
        session-&amp;gt;waiting_on_map.erase(i++);
        op-&amp;gt;put();

        spg_t pgid;
        if (m-&amp;gt;get_type() == CEPH_MSG_OSD_OP) {
            /*根据消息中记录的pg(对象名称的hash值)计算实际pg(根据pg数取余)*/
            pg_t actual_pgid = osdmap-&amp;gt;raw_pg_to_pg(static_cast&amp;lt;const MOSDOp*&amp;gt;(m)-&amp;gt;get_pg());
            if (!osdmap-&amp;gt;get_primary_shard(actual_pgid, &amp;amp;pgid)) {
                continue;
            }
        } else {
            pgid = m-&amp;gt;get_spg();
        }
        /*依据实际pgid将消息放入片式队列*/
        enqueue_op(pgid, op, m-&amp;gt;get_map_epoch());
    }
    ...
}

void OSD::enqueue_op(spg_t pg, OpRequestRef&amp;amp; op, epoch_t epoch)
{
    ...
    op_shardedwq.queue(make_pair(pg, PGQueueable(op, epoch)));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接下来我们看一下tp_osd_tp线程是如何处理分片中的请求，线程处理的核心函数是ShardedOpWQ::_process，其调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ShardedOpWQ::_process()
    |-OpQueue&amp;lt;&amp;gt;::dequeue()
    |-OSD::_look_up_pg()
    \-PGQueueable::run()
        \-PrimrayLogPG::do_request()
            \-PrimaryLogPG::do_op()
                \-PrimaryLogPG::execute_ctx()
                    |-PrimaryLogPG::prepare_transaction()
                    |   \-PrimaryLogPG::do_osd_ops()
                    \-PrimaryLogPG::issue_repop()
                        \-ReplicatedBackend::submit_transaction()
                            |-ReplicatedBackend::generate_transaction()
                            |-ReplicatedBackend::issue_op()
                            \-PrimaryLogPG::queue_transactions()
                                \-ObjectStore::queue_transactions()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

void OSD::ShardedOpWQ::_process(uint32_t thread_index, heartbeat_handle_d *hb)
{
    /*通过线程号取余的方式找到每个线程片时的分片，可能存在两个线程处理一个分片的情况*/
    uint32_t shard_index = thread_index % num_shards;
    ShardData *sdata = shard_list[shard_index];
    
    /*通过锁机制同步请求的放入与取出，以及多个线程并发取出的场景*/
    sdata-&amp;gt;sdata_op_ordering_lock.Lock();
    ...

    /*取出一个操作对象到item*/
    pair&amp;lt;spg_t, PGQueueable&amp;gt; item = sdata-&amp;gt;pqueue-&amp;gt;dequeue();
    ...

    sdata-&amp;gt;sdata_op_ordering_lock.Unlock();
    ...

    /*查找item对应的pg对象并为该pg加锁，类型为PrimaryLogPG*/
    pg = osd-&amp;gt;_lookup_lock_pg(item.first);
    ...

    sdata-&amp;gt;sdata_op_ordering_lock.Lock();
    ...

    /*对于查找到的pg对象会放入一个临时的slot结构中进行同步加锁*/
    qi = slot.to_process.front();
    slot.to_process.pop_front();
    ...

    sdata-&amp;gt;sdata_op_ordering_lock.Unlock();
    ...

    /*调用实际的处理函数，最终将执行PrimaryLogPG::do_request()*/
    qi-&amp;gt;run(osd, pg, tp_handle);
    ...

    /*解锁pg*/
    pg-&amp;gt;unlock();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  每个PG在处理op时，会为当前op以及op操作的对象生成一个context，用来保存此次操作相关的信息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/PrimaryLogPG.cc:

void PrimaryLogPG::do_request(OpRequestRef&amp;amp; op, ThreadPool::TPHandle &amp;amp;handle)
{
    .../*针对op进行一系列的有效性判断*/

    switch (op-&amp;gt;get_req()-&amp;gt;get_type()) {
    /*针对不同的请求类型进行不同的处理*/
    case CEPH_MSG_OSD_OP:
        do_op(op);
        break;
    ...
    }
}

void PrimaryLogPG::do_op(OpRequestRef&amp;amp; op)
{
    .../*还是一堆锁碎的状态检查*/
    
    MOSDOp *m = static_cast&amp;lt;MOSDOp*&amp;gt;(op-&amp;gt;get_nonconst_req());
    ...
    
    /*在当前PG中查找或生成一个新的对象上下文，用来保存对象修改的相关信息*/
    ObjectContextRef obc;
    int r = find_object_context(
            oid, &amp;amp;obc, can_create,
            m-&amp;gt;has_flag(CEPH_OSD_FLAG_MAP_SNAP_CLONE),
            &amp;amp;missing_oid);
    ...
    
    /*生成一个新op上下文*/
    OpContext *ctx = new OpContext(op, m-&amp;gt;get_reqid(), &amp;amp;m-&amp;gt;ops, obc, this);
    ...

    /*执行该op上下文*/
    execute_ctx(ctx);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PG在执行op上下文时，会使用事务的方式保证多个修改操作的原子性。另外事务具有多个层级，PG中使用PGTransaction；底层ObjectStore中会使用ObjectStore::Transaction进行进一步封装。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/PrimaryLogPG.cc:

void PrimaryLogPG::execute_ctx(OpContext *ctx)
{
    OpRequestRef op = ctx-&amp;gt;op; /*上下文上包含的op*/
    const MOSDOp *m = static_cast&amp;lt;const MOSDOp*&amp;gt;(op-&amp;gt;get_req()); /*op对应的原始请求消息*/
    ObjectContextRef obc = ctx-&amp;gt;obc; /*op操作的对象上下文*/
    const hobject_t&amp;amp; soid = obc-&amp;gt;obs.oi.soid; /*对象id*/

    ctx-&amp;gt;op_t.reset(new PGTransaction); /*设置一个新的PGTransaction事务对象*/
    ...

    /*将上下文中的多个子操作放入到事务中，普通读写只有一个子操作*/
    int result = prepare_transaction(ctx);
    ...

    /*对于读操作，在prepare_transaction中将完成对象的读取，这里将直接返回响应消息给客户端*/
    if ((ctx-&amp;gt;op_t-&amp;gt;empty() || result &amp;lt; 0) &amp;amp;&amp;amp; !ctx-&amp;gt;update_log_only) {
        ...
        complete_read_ctx(result, ctx);
        return;
    }
    
    /*以下均针对写操作*/
    
    /*注册所有复本写操作均完成后的回调函数，该函数将发送响应给客户端*/
    ctx-&amp;gt;register_on_commit(...);
    ...

    /*生成一组复本操作，并将这些操作发射出去：本地复本将写到后端ObjectStore，远端复本将通过网络消息发送并等待响应*/
    ceph_tid_t rep_tid = osd-&amp;gt;get_tid();

    RepGather *repop = new_repop(ctx, obc, rep_tid);

    issue_repop(repop, ctx);
    eval_repop(repop);
    repop-&amp;gt;put();
}

int PrimaryLogPG::prepare_transaction(OpContext *ctx)
{
    ...
    int result = do_osd_ops(ctx, *ctx-&amp;gt;ops);
    ...
｝

int PrimaryLogPG::do_osd_ops(OpContext *ctx, vector&amp;lt;OSDOp&amp;gt;&amp;amp; ops)
{
    ...
    /*循环处理每个子操作*/
    for (vector&amp;lt;OSDOp&amp;gt;::iterator p = ops.begin(); p != ops.end(); ++p, ctx-&amp;gt;current_osd_subop_num++) {
        OSDOp&amp;amp; osd_op = *p;
        ceph_osd_op&amp;amp; op = osd_op.op;

        switch (op.op) {
        ...
        case CEPH_OSD_OP_READ:
            ++ctx-&amp;gt;num_read;
            result = do_read(ctx, osd_op);
            break;
        ...
        case CEPH_OSD_OP_WRITE:
            ++ctx-&amp;gt;num_write;
            t-&amp;gt;write(soid, op.extent.offset, op.extent.length, osd_op.indata, op.flags);
            break;
        ...
        }
    }
}

int PrimaryLogPG::do_read(OpContext *ctx, OSDOp&amp;amp; osd_op)
{
    ...

    /*针对读请求，通过后端同步读接口直接读取对象内容*/
    int r = pgbackend-&amp;gt;objects_read_sync(
            soid, op.extent.offset, op.extent.length, op.flags, &amp;amp;osd_op.outdata);
    ...
}

ceph/src/osd/PGTransaction.h:

/*针对写操作，会将几个关键参数放入事务的buffer_updates中*/
void write(
    const hobject_t &amp;amp;hoid,         ///&amp;lt; [in] object to write
    uint64_t off,                  ///&amp;lt; [in] off at which to write
    uint64_t len,                  ///&amp;lt; [in] len to write from bl
    bufferlist &amp;amp;bl,                ///&amp;lt; [in] bl to write will be claimed to len
    uint32_t fadvise_flags = 0     ///&amp;lt; [in] fadvise hint
) {
    auto &amp;amp;op = get_object_op_for_modify(hoid);
    op.buffer_updates.insert(
        off,
        len,
        ObjectOperation::BufferUpdate::Write{bl, fadvise_flags});
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  针对写操作的多复本操作，将会提交给ReplicatedBackend对象进行处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/PrimaryLogPG.cc:

void PrimaryLogPG::issue_repop(RepGather *repop, OpContext *ctx)
{
    ...

    /*生成复本操作全部完成后的回调对象*/
    Context *on_all_commit = new C_OSD_RepopCommit(this, repop);
    Context *on_all_applied = new C_OSD_RepopApplied(this, repop);
    ...

    /*递交给ReplicatedBackend对象处理*/
    pgbackend-&amp;gt;submit_transaction(
        soid,
        ctx-&amp;gt;delta_stats,
        ctx-&amp;gt;at_version,
        std::move(ctx-&amp;gt;op_t),
        pg_trim_to,
        min_last_complete_ondisk,
        ctx-&amp;gt;log,
        ctx-&amp;gt;updated_hset_history,
        onapplied_sync,
        on_all_applied,
        on_all_commit,
        repop-&amp;gt;rep_tid,
        ctx-&amp;gt;reqid,
        ctx-&amp;gt;op);
}

ceph/src/osd/ReplicatedBackend.cc:

void ReplicatedBackend::submit_transaction(
    const hobject_t &amp;amp;soid,
    const object_stat_sum_t &amp;amp;delta_stats,
    const eversion_t &amp;amp;at_version,
    PGTransactionUPtr &amp;amp;&amp;amp;_t,
    const eversion_t &amp;amp;trim_to,
    const eversion_t &amp;amp;roll_forward_to,
    const vector&amp;lt;pg_log_entry_t&amp;gt; &amp;amp;_log_entries,
    boost::optional&amp;lt;pg_hit_set_history_t&amp;gt; &amp;amp;hset_history,
    Context *on_local_applied_sync,
    Context *on_all_acked,
    Context *on_all_commit,
    ceph_tid_t tid,
    osd_reqid_t reqid,
    OpRequestRef orig_op)
{
    ObjectStore::Transaction op_t;
    ...
    
    /*将PGTransaction对象封装到ObjectStore:: Transaction中*/
    generate_transaction(
        t,
        coll,
        (get_osdmap()-&amp;gt;require_osd_release &amp;lt; CEPH_RELEASE_KRAKEN),
        log_entries,
        &amp;amp;op_t,
        &amp;amp;added,
        &amp;amp;removed);

    /*生成一个新的代表当前操作对象的InProgressOp*/
    InProgressOp &amp;amp;op = in_progress_ops.insert(
        make_pair(
            tid,
            InProgressOp(
                tid, on_all_commit, on_all_acked,
                orig_op, at_version)
        )
    ).first-&amp;gt;second;

    /*记录当前操作需要等待哪些OSD返回结果，包含本地OSD和远端OSD*/
    op.waiting_for_applied.insert(
        parent-&amp;gt;get_actingbackfill_shards().begin(), parent-&amp;gt;get_actingbackfill_shards().end());
    op.waiting_for_commit.insert(
        parent-&amp;gt;get_actingbackfill_shards().begin(), parent-&amp;gt;get_actingbackfill_shards().end());

    /*对于发往远端OSD的复本请求，通过网络发送CEPH_MSG_REPOP消息*/
    issue_op(
        soid,
        at_version,
        tid,
        reqid,
        trim_to,
        at_version,
        added.size() ? *(added.begin()) : hobject_t(),
        removed.size() ? *(removed.begin()) : hobject_t(),
        log_entries,
        hset_history,
        &amp;amp;op,
        op_t);

    /*对于发送本地OSD的复本请求*/
    /*先注册本地完成后的回调*/
    op_t.register_on_applied_sync(on_local_applied_sync);/*写到数据区并同步回刷之后触发*/
    op_t.register_on_applied(
        parent-&amp;gt;bless_context(new C_OSD_OnOpApplied(this, &amp;amp;op)));/*写到数据区之后触发*/
    op_t.register_on_commit(
        parent-&amp;gt;bless_context(new C_OSD_OnOpCommit(this, &amp;amp;op)));/*提交到日志区之后触发*/
    
    /*再将事务发送给后端存储池进行处理*/
    vector&amp;lt;ObjectStore::Transaction&amp;gt; tls;
    tls.push_back(std::move(op_t));

    parent-&amp;gt;queue_transactions(tls, op.op); /*最终会调用不同ObjectStore的queue_transactions函数*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  OSD模块整体分析完毕，后续我们将继续分析FileStore的实现原理。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/04/RBD-OSD-2&quot;&gt;【Rados Block Device】七、OSD原理分析－OSD模块&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Apr 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/RBD-OSD-2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/RBD-OSD-2/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】六、OSD原理分析－SimpleMessenger模块</title>
        <description>&lt;p&gt;  OSD进程通过网络对Client提供服务，因此网络层是OSD中的基础层。本篇博文将讨论ceph中传统的SimpleMessenger实现原理。&lt;/p&gt;

&lt;h3 id=&quot;simplemessenger模块中对象流程概览&quot;&gt;SimpleMessenger模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  如果将OSD进程的网络服务模式配置成SimpleMessenger，那么它采用的是POSIX标准网络接口来实现网络功能。也就是说，此时我们的OSD服务从代码实现流程上来说就是通过socket()-&amp;gt;bind()-&amp;gt;listen()-&amp;gt;accept()进行连接建立，随后再通过每个连接进行网络消息的收发。虽然SimpleMessenger在实现过程中融入一些设计模式的抽象，但是抓住以上POSIX网络编程核心流程后将便于大家理解其实现机理。&lt;/p&gt;

&lt;p&gt;  我们先整体来看一下SimpleMessenger的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_simplemessenger.jpg&quot; height=&quot;450&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们知道一个ceph集群中通常会有两个网络平面：public和cluster，那么每个OSD进程中就会对应产生两个messenger对象ms_public和ms_cluster。对于每个messenger对象(这里其实为SimpleMessenger对象)，会产生一个名为ms_accepter的线程，messenger对象通过accepter成员变量指向该线程。ms_accepter线程是在完成socket()-&amp;gt;bind()-&amp;gt;listen()动作之后产生，它的主要作用就是一直监听Client端的网络连接请求。&lt;/p&gt;

&lt;p&gt;  当某一个Client发起连接请求后，ms_accepter将调用accept()接受请求，并为该连接产生一个pipe对象。pipe对象是对TCP socket连接的封装，可以实现故障重连等可靠性增强特性。每个pipe对象会产生两个线程：一个叫ms_pipe_read线程，它一直在监听socket连接中的消息，如果发现有消息它将取出消息并将消息放入in_q中进行分发处理；另一个叫ms_pipe_write线程，它一直在等待out_q中被放入发送消息，如果发现out_q中有消息，它将把消息取出并通过socket连接将其发送出去。&lt;/p&gt;

&lt;p&gt;  在消息接收处理的过程中，in_q队列指向的其实是SimpleMessenger对象中的DispatchQueue，也就是说对于同一个messenger中的多个pipe，它们接收的消息将被放入到同一个队列中等待处理。DispatchQueue分发消息时如果发现该消息可以被快速处理(fast dispatch)时，会将该消息直接分发给SimpleMessenger，由SimpleMessenger将消息分发给其内部的多个Dispatcher对象进行最终的消息处理。这里OSD模块中的OSD对象就是属于SimpleMessenger的一个Dispatcher，OSD对消息的处理属于OSD模块的内容，我们将在后续博文中介绍。DispatchQueue如果发现该消息无法被快速处理，则会将该消息交给DispatchQueue对应的DispatchThread处理。DispatchThread线程取出消息后会传递给messenger通过ms_deliver_dispatch进行普通处理，其实最终也会交给Dispatcher(如OSD对象)处理，只不过是在DispatchThread线程中被处理(fast dispatch是在ms_pipe_read线程中被处理，请大家注意对比)。&lt;/p&gt;

&lt;h3 id=&quot;simplemessenger模块中类的概览&quot;&gt;SimpleMessenger模块中类的概览&lt;/h3&gt;

&lt;p&gt;  在了解了SimpleMessenger的实现流程后，我们再来看看它的类定义，从而理解它是如何实现抽象，以支持未来更灵活的扩展：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_simplemessenger_class.jpg&quot; height=&quot;750&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  图中标红的类(Messenger、Dispatcher、DispatchQueue、Connection)属于Messenger模块抽象类(不属于某一种网络实现模式)，不同的网络实现模式可以继承它们实现各自特有的功能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Messenger类是SimpleMessenger、AsyncMessenger、XioMessenger的父类，它描绘了messenger对象的功能框架。每个messenger对象包含两个Dispatcher链表，dispatchers代表普通处理对象，fast_dispatchers代表快速处理对象，以便实现针对不同消息类型采用不同处理方式的功能。Dispatcher对象是在初始流程中，通过add_dispatcher_header方法被加入到messenger对应的链表。create、bind、start、ready方法实现messenger对象的创建和初始化；wait等待messenger生命周期结束；shutdown关闭messenger对象。send_message实现了通过messenger对象发送一个消息的功能。ms_fast_dispatch实现消息的快速分发；ms_deliver_dispatch实现消息的普通分发；最终都将分发给messenger中的dispatchers进行处理。&lt;/li&gt;
    &lt;li&gt;Dispatcher类是接收消息的处理者。不同模块可以实现各自不同的Dispatcher，以实现对消息的不同处理逻辑。只要在初始化时通过messenger对象的add_dispatcher_*方法被加入到messenger中，便可保证该Dispatcher可以接收到消息并进行处理。&lt;/li&gt;
    &lt;li&gt;DispatchQueue类实现了一个分发队列。用户通过enqueue操作将消息放入队列，该队列可以按优先级对消息进行排序(PrioritizeQueue)。队列会产生一个专门的DispatchThread线程，由该线程负责从PrioritizedQueue中取出消息进行分发处理。此外，DispatchQueue也可通过can_fast_dispatch判断消息是否可以被快速处理，如果可以被快速处理，则直接调用fast_dispatch进行分发处理，否则调用enqueue进队列交由DispatchThread处理。&lt;/li&gt;
    &lt;li&gt;Connection类是对网络连接的抽象。子类通过实现send_message方法提供不同的网络发送方案。&lt;/li&gt;
    &lt;li&gt;SimpleMessenger类继承Messenger类实现基于POSIX网络接口的网络信使功能。它将实现Messenger类中的bind、start、ready方法，以完成网络socket的初始化。独有的Accepter对象将产生一个独立的线程监听网络连接请求。每当接受一个新连接时，都会生成一个新的Pipe对象，并通过add_accept_pipe方法将其加入到pipes列表中。SimpleMessenger包含了一个DispatchQueue来实现消息的普通分发和快速分发。&lt;/li&gt;
    &lt;li&gt;Pipe类代表一个连接会话，每个连接会产生一个reader_thread线程(入口函数为reader()方法)和一个writer_thread线程(入口函数为writer())。reader方法负责从网络层接收消息放入in_q并进行顶层分发处理；writer方法负责将out_q中消息通过网络发送。accept方法在接受连接时调用，connect方法在发起连接请求时调用。&lt;/li&gt;
    &lt;li&gt;Thread类是Common模块中的公共类，用来生成线程。create方法用来创建线程对象；entry方法为新线程的入口函数；join方法在等待子线程结束时调用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;p&gt;  有了对SimpleMessenger模块中对象、流程和类的整体认识之后，我们再结合代码来深入理解其实现细节。&lt;/p&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SimpleMessenger模块的初始化在OSD进程的main函数中完成，整体调用栈如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    |-Messenger::create()
    |   \-SimpleMessenger::SimpleMessenger()
    |-Messenger::bind() -&amp;gt; SimpleMessenger::bind()
    |   \-Accepter::bind()
    |       |-::socket()
    |       |-::bind()
    |       \-::listen()
    |-OSD::OSD()
    |-Messenger::start() -&amp;gt; SimpleMessenger::start()
    |-OSD::init()
    |   \-Messenger::add_dispatcher_head()
    |       \-Messenger::ready() -&amp;gt; SimpleMessenger::ready()
    |           \-Accepter::start()
    \-Messenger::wait() -&amp;gt; SimpleMessenger::wait()
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们先来看看ceph_osd.cc中main函数里和Messenger初始化相关的部分代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/ceph_osd.cc:

int main(int argc, const char **argv)
{
    ...

    /*这里我们假设public_msgr_type和cluster_msgr_type都为Simple*/
    std::string public_msgr_type = g_conf-&amp;gt;ms_public_type.empty() ? g_conf-&amp;gt;get_val&amp;lt;std::string&amp;gt;(&quot;ms_type&quot;) : g_conf-&amp;gt;ms_public_type;
    std::string cluster_msgr_type = g_conf-&amp;gt;ms_cluster_type.empty() ? g_conf-&amp;gt;get_val&amp;lt;std::string&amp;gt;(&quot;ms_type&quot;) : g_conf-&amp;gt;ms_cluster_type;

    /*根据不同类型创建messenger对象，这里将创建两个SimpleMessenger对象*/
    Messenger *ms_public = Messenger::create(g_ceph_context, public_msgr_type,
        entity_name_t::OSD(whoami), &quot;client&quot;,
        getpid(),
        Messenger::HAS_HEAVY_TRAFFIC |
        Messenger::HAS_MANY_CONNECTIONS);
    Messenger *ms_cluster = Messenger::create(g_ceph_context, cluster_msgr_type,
        entity_name_t::OSD(whoami), &quot;cluster&quot;,
        getpid(),
        Messenger::HAS_HEAVY_TRAFFIC |
        Messenger::HAS_MANY_CONNECTIONS);

    ...

    /*为messenger对象绑定IP地址，最终会调用Accepter::bind函数，由它调用系统的socket()、bind()和listen()函数*/
    r = ms_public-&amp;gt;bind(g_conf-&amp;gt;public_addr);
    ...
    r = ms_cluster-&amp;gt;bind(g_conf-&amp;gt;cluster_addr);

    ...

    /*将创建的ms_public和ms_cluster传递给OSD构造函数，建立一个新的OSD对象*/
    osd = new OSD(g_ceph_context,
                store,
                whoami,
                ms_cluster,
                ms_public,
                ms_hb_front_client,
                ms_hb_back_client,
                ms_hb_front_server,
                ms_hb_back_server,
                ms_objecter,
                &amp;amp;mc,
                g_conf-&amp;gt;osd_data,
                g_conf-&amp;gt;osd_journal);

    /*启动messenger对象，针对SimpleMessenger，其内部细节我们暂不用关心*/
    ms_public-&amp;gt;start();
    ...
    ms_cluster-&amp;gt;start();

    /*OSD执行初始化，下文将展开*/
    osd-&amp;gt;init();

    ...

    /*整个初始化动作完成，OSD主线程进入等待状态*/
    ms_public-&amp;gt;wait();
    ms_cluster-&amp;gt;wait();
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/Messenger.cc:

Messenger *Messenger::create(CephContext *cct, const string &amp;amp;type,
        entity_name_t name, string lname,
        uint64_t nonce, uint64_t cflags)
{
    int r = -1;
    if (type == &quot;random&quot;) {
        static std::random_device seed;
        static std::default_random_engine random_engine(seed());
        static Spinlock random_lock;

        std::lock_guard&amp;lt;Spinlock&amp;gt; lock(random_lock);
        std::uniform_int_distribution&amp;lt;&amp;gt; dis(0, 1);
        r = dis(random_engine);
    }
    if (r == 0 || type == &quot;simple&quot;)
        return new SimpleMessenger(cct, name, std::move(lname), nonce);
    else if (r == 1 || type.find(&quot;async&quot;) != std::string::npos)
        return new AsyncMessenger(cct, name, type, std::move(lname), nonce);
#ifdef HAVE_XIO
    else if ((type == &quot;xio&quot;) &amp;amp;&amp;amp;
            cct-&amp;gt;check_experimental_feature_enabled(&quot;ms-type-xio&quot;))
    return new XioMessenger(cct, name, std::move(lname), nonce, cflags);
#endif
    lderr(cct) &amp;lt;&amp;lt; &quot;unrecognized ms_type '&quot; &amp;lt;&amp;lt; type &amp;lt;&amp;lt; &quot;'&quot; &amp;lt;&amp;lt; dendl;
    return nullptr;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们再深入看看OSD初始化过程中与Messenger相关的部分代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

int OSD::init()
{
    ...

    /*将OSD对象自身加到public messenger和cluster messenger的dispatchers中*/
    client_messenger-&amp;gt;add_dispatcher_head(this);
    cluster_messenger-&amp;gt;add_dispatcher_head(this);

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/Messenger.h:

void add_dispatcher_head(Dispatcher *d) { 
    bool first = dispatchers.empty(); /*是否为添加到dispatchers链表中的第一个元素？*/
    dispatchers.push_front(d); /*加入到dispatchers*/
    if (d-&amp;gt;ms_can_fast_dispatch_any()) /*如果可以进行fast dispatch则加入到fast_dispatchers中*/
        fast_dispatchers.push_front(d);
    if (first)
        ready(); /*如果是首个加入到dispatchers中的对象，则调用messenger对象的ready()*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

void SimpleMessenger::ready()
{
    ldout(cct,10) &amp;lt;&amp;lt; &quot;ready &quot; &amp;lt;&amp;lt; get_myaddr() &amp;lt;&amp;lt; dendl;
    dispatch_queue.start(); /*拉起dispatch_queue对应的dispatch_thread*/

    lock.Lock();
    if (did_bind)
        accepter.start(); /*拉起ms_accepter线程*/
    lock.Unlock();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Accept.cc:

int Accepter::start()
{
    ldout(msgr-&amp;gt;cct,1) &amp;lt;&amp;lt; __func__ &amp;lt;&amp;lt; dendl;

    // start thread
    create(&quot;ms_accepter&quot;);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-连接建立过程&quot;&gt;&lt;strong&gt;2. 连接建立过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SimpleMessenger对象初始化完成后，将拉起一个ms_accepter线程处理Client端的连接请求，该线程入口函数为Accepter::entry。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Accepter.cc:

void *Accepter::entry()
{
    int errors = 0;
    int ch;

    struct pollfd pfd[2];
    memset(pfd, 0, sizeof(pfd));

    pfd[0].fd = listen_sd;
    pfd[0].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
    pfd[1].fd = shutdown_rd_fd;
    pfd[1].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;

    while (!done) {
        int r = poll(pfd, 2, -1); /*通过poll系统调用等待Client连接请求*/

        ...

        if (done) break;

        // accept
        sockaddr_storage ss;
        socklen_t slen = sizeof(ss);
        int sd = ::accept(listen_sd, (sockaddr*)&amp;amp;ss, &amp;amp;slen); /*收到请求后，accept该连接*/
        if (sd &amp;gt;= 0) {
            ...
            msgr-&amp;gt;add_accept_pipe(sd); /*向SimpleMessenger对象中添加pipe*/
        } else {
            ...
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

Pipe *SimpleMessenger::add_accept_pipe(int sd)
{
    lock.Lock();
    Pipe *p = new Pipe(this, Pipe::STATE_ACCEPTING, NULL);
    p-&amp;gt;sd = sd;
    p-&amp;gt;pipe_lock.Lock();
    p-&amp;gt;start_reader(); /*拉起pipe对象的ms_pipe_read线程*/
    p-&amp;gt;pipe_lock.Unlock();
    pipes.insert(p);
    accepting_pipes.insert(p);
    lock.Unlock();
    return p;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.cc:

void Pipe::start_reader()
{
    if (reader_needs_join) {
        reader_thread.join();
        reader_needs_join = false;
    }
    reader_running = true;
    reader_thread.create(&quot;ms_pipe_read&quot;, msgr-&amp;gt;cct-&amp;gt;_conf-&amp;gt;ms_rwthread_stack_bytes);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;3-消息接收与分发过程&quot;&gt;&lt;strong&gt;3. 消息接收与分发过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  每个pipe对象的ms_pipe_read线程被拉起后，会进行会话的协商过程(可以回顾下前期Client端RBD的messenger分析博文)；完成后将处于等待接收消息的状态：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.cc:

void Pipe::reader()
{
    pipe_lock.Lock();

    if (state == STATE_ACCEPTING) {
        /*执行会话协商过程，协商成功后会拉起ms_pipe_write线程*/
        accept();
    }

    while (state != STATE_CLOSED &amp;amp;&amp;amp;
            state != STATE_CONNECTING) {

        // sleep if (re)connecting
        if (state == STATE_STANDBY) {
            /*如果pipe状态为STANDBY，说明底层连接故障且暂无消息处理，则进入睡眠*/
            cond.Wait(pipe_lock);
            continue;
        }

        pipe_lock.Unlock();

        char tag = -1;
        /*先从网络连接中读取一个字节的tag*/
        if (tcp_read((char*)&amp;amp;tag, 1) &amp;lt; 0) {
            pipe_lock.Lock();
            fault(true);
            continue;
        }
        
        /*根据tag值进行不同的处理动作*/
        if (tag == CEPH_MSGR_TAG_KEEPALIVE) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_KEEPALIVE2) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_ACK) {
            ...
            continue;
        }
        else if (tag == CEPH_MSGR_TAG_MSG) {
            /*针对消息，先从网络中读取消息内容到m中*/
            Message *m = 0;
            int r = read_message(&amp;amp;m, auth_handler.get());

            pipe_lock.Lock();
            if (m-&amp;gt;get_seq() &amp;lt;= in_seq) {
                m-&amp;gt;put();
                continue;
            }
            m-&amp;gt;set_connection(connection_state.get());
            ...

            /*对消息进行预处理*/
            in_q-&amp;gt;fast_preprocess(m);
            if (delay_thread) {
                ...
            } else {
                /*如果消息可以被快速处理，则走快速处理流程；否则就enqueue到in_q中交给dispatch_thread处理*/
                if (in_q-&amp;gt;can_fast_dispatch(m)) {
                    reader_dispatching = true;
                    pipe_lock.Unlock();
                    in_q-&amp;gt;fast_dispatch(m);
                    pipe_lock.Lock();
                    reader_dispatching = false;
                    ...
                } else {
                    in_q-&amp;gt;enqueue(m, m-&amp;gt;get_priority(), conn_id);
                }            
            }
        }
        ...
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  消息的快速处理流程可以回头参考前文的对象、流程图。&lt;/p&gt;

&lt;h4 id=&quot;4-消息发送过程&quot;&gt;&lt;strong&gt;4. 消息发送过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  任何模块想通过messenger发送消息时，都可以调用Messenger::send_mesage来完成。对于SimpleMessenger，其实现体位于SimpleMessenger.h，发送是一个异步过程，发送者只会将消息放入Pipe对象的out_q中，随后由ms_pipe_write线程完成向网络协议栈的发送。&lt;/p&gt;

&lt;p&gt;  发送者的调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sender[any thread]
    \-SimpleMessenger::send_message()
        \-SimpleMessenger::_send_message()
            \-SimpleMessenger::submit_message()
                \-Pipe::_send()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.h:

int send_message(Message *m, const entity_inst_t&amp;amp; dest) override {
    return _send_message(m, dest);
}

int send_message(Message *m, Connection *con) {
    return _send_message(m, con);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

int SimpleMessenger::_send_message(Message *m, Connection *con)
{
    //set envelope
    m-&amp;gt;get_header().src = get_myname();

    if (!m-&amp;gt;get_priority()) m-&amp;gt;set_priority(get_default_send_priority());

    submit_message(m, static_cast&amp;lt;PipeConnection*&amp;gt;(con),
                con-&amp;gt;get_peer_addr(), con-&amp;gt;get_peer_type(), false);
    return 0;
}

void SimpleMessenger::submit_message(Message *m, PipeConnection *con,
                const entity_addr_t&amp;amp; dest_addr, int dest_type,
                bool already_locked)
{
    ...
    if (con) {
        Pipe *pipe = NULL;
        bool ok = static_cast&amp;lt;PipeConnection*&amp;gt;(con)-&amp;gt;try_get_pipe(&amp;amp;pipe);
        ...
        while (pipe &amp;amp;&amp;amp; ok) {
            // we loop in case of a racing reconnect, either from us or them
            pipe-&amp;gt;pipe_lock.Lock(); // can't use a Locker because of the Pipe ref
            if (pipe-&amp;gt;state != Pipe::STATE_CLOSED) {
                pipe-&amp;gt;_send(m);
                pipe-&amp;gt;pipe_lock.Unlock();
                pipe-&amp;gt;put();
                return;
            }
        }
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.h:

void _send(Message *m) {
    assert(pipe_lock.is_locked());
    out_q[m-&amp;gt;get_priority()].push_back(m);
    cond.Signal();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  ms_pipe_write线程入口函数为Pipe::writer，其调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pipe::writer()
    |-Pipe::_get_next_outgoing()
    \-Pipe::write_message()
        \-Pipe::do_sendmsg()

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void Pipe::writer()
{
    pipe_lock.Lock();
    while (state != STATE_CLOSED) {
        ...
        Message *m = _get_next_outgoing();
        ...
        const ceph_msg_header&amp;amp; header = m-&amp;gt;get_header();
        const ceph_msg_footer&amp;amp; footer = m-&amp;gt;get_footer();
        bufferlist blist = m-&amp;gt;get_payload();
        blist.append(m-&amp;gt;get_middle());
        blist.append(m-&amp;gt;get_data());
        pipe_lock.Unlock();

        write_message(header, footer, blist);
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/03/RBD-OSD-1&quot;&gt;【Rados Block Device】六、OSD原理分析－SimpleMessenger模块&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/RBD-OSD-1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/RBD-OSD-1/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】五、高精度定时器</title>
        <description>&lt;p&gt;  通过对低精度定时器的分析，我们知道这类定时器的精度是毫秒级的，也就是说存在毫秒级的误差范围。对于像IO超时错误处理这类定时任务，毫秒级的误差完全不算什么问题。然而，对于工业上的许多实时任务，毫秒级的误差是完全不可接受的。因此，基于更高精度的时间硬件(例如TSC和LAPIC Timer)，内核工程师们开发了一套全新的高精度定时器功能(传统基于时间轮的低精度定时器已经很稳定了，与其对它修修补补，还不如新建一套全新的机制)。&lt;/p&gt;

&lt;h3 id=&quot;1-高精度定时器的初始化&quot;&gt;1. 高精度定时器的初始化&lt;/h3&gt;

&lt;p&gt;  高精度定时器的初始化和低精度定时器的初始化有些类似，需要指定到期后的回调函数。然而在内部数据结构的设计上，不同于低精度定时器的时间轮，高精度定时器采用了红黑树(可以高效地实现排序、增删改等操作，内核中有比较成熟稳定的代码实现)。另外，低精度定时器的计时参照是jiffies，而高精度定时器可以采用timekeeper中的多种计时参照，如REAL TIME、MONOTONIC TIME等等。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/**
 * hrtimer_init - initialize a timer to the given clock
 * @timer:	the timer to be initialized
 * @clock_id:	the clock to be used
 * @mode:	timer mode abs/rel
 */
void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
    debug_init(timer, clock_id, mode);
    __hrtimer_init(timer, clock_id, mode);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/hrtimer.h:

/**
 * struct hrtimer - the basic hrtimer structure
 * @node:   timerqueue node, which also manages node.expires,
 *          the absolute expiry time in the hrtimers internal
 *          representation. The time is related to the clock on
 *          which the timer is based. Is setup by adding
 *          slack to the _softexpires value. For non range timers
 *          identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *          The time which was given as expiry time when the timer
 *          was armed.
 * @function:   timer expiry callback function
 * @base:   pointer to the timer base (per cpu and per clock)
 * @state:  state information (See bit values above)
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct hrtimer {
    struct timerqueue_node      node;
    ktime_t                     _softexpires;
    enum hrtimer_restart        (*function)(struct hrtimer *);
    struct hrtimer_clock_base   *base;
    unsigned long               state;
    ...
};

enum hrtimer_mode {
    HRTIMER_MODE_ABS = 0x0,		/* Time value is absolute */
    HRTIMER_MODE_REL = 0x1,		/* Time value is relative to now */
    HRTIMER_MODE_PINNED = 0x02,	/* Timer is bound to CPU */
    HRTIMER_MODE_ABS_PINNED = 0x02,
    HRTIMER_MODE_REL_PINNED = 0x03,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过上面的代码和注释，我们可以看到，高精度定时器初始化时可以指定计时参照对象(clock_id)和计时模式(采用绝对计时或相对计时)。高精度定时器内部结构中的node即是在红黑树中的挂接对象，base指向每个CPU针对不同计时参照对象的全局数据结构，其内部包含一棵红黑树。__hrtimer_init的具体实现比较简单：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
    struct hrtimer_cpu_base *cpu_base;
    int base;

    memset(timer, 0, sizeof(struct hrtimer));

    cpu_base = &amp;amp;__raw_get_cpu_var(hrtimer_bases); /*获取当前CPU的hrtimer_cpu_base对象*/

    if (clock_id == CLOCK_REALTIME &amp;amp;&amp;amp; mode != HRTIMER_MODE_ABS) /*REALTIME只支持绝对模式*/
        clock_id = CLOCK_MONOTONIC;

    base = hrtimer_clockid_to_base(clock_id); /*索引计时参照*/
    timer-&amp;gt;base = &amp;amp;cpu_base-&amp;gt;clock_base[base];
    timerqueue_init(&amp;amp;timer-&amp;gt;node); /*初始化红黑树节点*/

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-高精度定时器的启动&quot;&gt;2. 高精度定时器的启动&lt;/h3&gt;

&lt;p&gt;  初始化完成并指定回调处理函数后，我们通过hrtimer_start函数可以启动一个定时器：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/**
 * hrtimer_start - (re)start an hrtimer on the current CPU
 * @timer:  the timer to be added
 * @tim:    expiry time
 * @mode:   expiry mode: absolute (HRTIMER_MODE_ABS) or
 *          relative (HRTIMER_MODE_REL)
 *
 * Returns:
 *  0 on success
 *  1 when the timer was active
 */
int hrtimer_start(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode)
{
    return __hrtimer_start_range_ns(timer, tim, 0, mode, 1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  高精度定时器允许有一个纳秒级别的误差，由__hrtimer_start_range_ns的delta_ns参数指明：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
    unsigned long delta_ns, const enum hrtimer_mode mode, int wakeup)
{
    struct hrtimer_clock_base *base, *new_base;
    unsigned long flags;
    int ret, leftmost;

    base = lock_hrtimer_base(timer, &amp;amp;flags); /*锁定该timer对应的hrtimer_clock_base对象*/

    /* Remove an active timer from the queue: */
    ret = remove_hrtimer(timer, base);

    if (mode &amp;amp; HRTIMER_MODE_REL) {
        tim = ktime_add_safe(tim, base-&amp;gt;get_time());
        ...
    }

    hrtimer_set_expires_range_ns(timer, tim, delta_ns); /*设置定时器内部的超时时间*/

    /* Switch the timer base, if necessary: */
    new_base = switch_hrtimer_base(timer, base, mode &amp;amp; HRTIMER_MODE_PINNED);

    leftmost = enqueue_hrtimer(timer, new_base); /*将定时器加入到对应hrtimer_clock_base的红黑树中*/

    if (leftmost &amp;amp;&amp;amp; new_base-&amp;gt;cpu_base == &amp;amp;__get_cpu_var(hrtimer_bases)
        &amp;amp;&amp;amp; hrtimer_enqueue_reprogram(timer, new_base)) { /*如果当前定时器是红黑树中最早到期的定时器，则重新设置clock event device的oneshot计数。注，高精度定时器正常工作时，会将clock event device的工作模式切换到oneshot*/
        ...
    }

    unlock_hrtimer_base(timer, &amp;amp;flags);

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-切换到高精度模式&quot;&gt;3. 切换到高精度模式&lt;/h3&gt;

&lt;p&gt;  内核正常启动后首先工作在低精度模式，然而在时钟中断的处理中，内核会检测是否具备切换到高精度的条件，如果各条件均满足，则切换到高精度模式工作。时钟中断中在处理低精度时钟时，通过hrtimer_run_pending()完成切换动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    hrtimer_run_pending();

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies))
        __run_timers(base);
}

void hrtimer_run_pending(void)
{
    if (hrtimer_hres_active()) /*如果已经切换到高精度模式则返回*/
        return;

    if (tick_check_oneshot_change(!hrtimer_is_hres_enabled())) /*判断是否具备切换到高精度的条件，如时钟源精度是否满足、是否支持oneshot模式*/
        hrtimer_switch_to_hres();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  如果切换条件均满足，则通过hrtimer_switch_to_hres切换到高精度模式：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

static int hrtimer_switch_to_hres(void)
{
    int i, cpu = smp_processor_id();
    struct hrtimer_cpu_base *base = &amp;amp;per_cpu(hrtimer_bases, cpu);
    unsigned long flags;

    if (base-&amp;gt;hres_active)
        return 1;

    local_irq_save(flags);

    if (tick_init_highres()) { /*将tick模式切换到oneshot模式并重新指定中断处理函数*/
        local_irq_restore(flags);
        printk(KERN_WARNING &quot;Could not switch to high resolution &quot;
        &quot;mode on CPU %d\n&quot;, cpu);
        return 0;
    }
    base-&amp;gt;hres_active = 1;
    for (i = 0; i &amp;lt; HRTIMER_MAX_CLOCK_BASES; i++)
    base-&amp;gt;clock_base[i].resolution = KTIME_HIGH_RES;

    tick_setup_sched_timer(); /*设置一个专门的调度定时器，用来处理调度任务*/
    /* &quot;Retrigger&quot; the interrupt to get things going */
    retrigger_next_event(NULL);
    local_irq_restore(flags);
    return 1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-oneshot.c:

/**
 * tick_init_highres - switch to high resolution mode
 *
 * Called with interrupts disabled.
 */
int tick_init_highres(void)
{
    return tick_switch_to_oneshot(hrtimer_interrupt); /*高精度模式下时钟中断处理函数为hrtimer_interrupt*/
}

/**
 * tick_switch_to_oneshot - switch to oneshot mode
 */
int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))
{
    struct tick_device *td = &amp;amp;__get_cpu_var(tick_cpu_device);
    struct clock_event_device *dev = td-&amp;gt;evtdev;

    if (!dev || !(dev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_ONESHOT) ||
        !tick_device_is_functional(dev)) {
        ...
    }

    td-&amp;gt;mode = TICKDEV_MODE_ONESHOT;
    dev-&amp;gt;event_handler = handler;
    clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
    tick_broadcast_switch_to_oneshot();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;4-高精度定时器的到期处理&quot;&gt;4. 高精度定时器的到期处理&lt;/h3&gt;

&lt;p&gt;  如前所述，高精度模式下，时钟中断的处理函数已经从tick_handle_periodic切换成hrtimer_interrupt了：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/*
 * High resolution timer interrupt
 * Called with interrupts disabled
 */
void hrtimer_interrupt(struct clock_event_device *dev)
{
    struct hrtimer_cpu_base *cpu_base = &amp;amp;__get_cpu_var(hrtimer_bases);
    ktime_t expires_next, now, entry_time, delta;
    int i, retries = 0;

    BUG_ON(!cpu_base-&amp;gt;hres_active);
    cpu_base-&amp;gt;nr_events++;
    dev-&amp;gt;next_event.tv64 = KTIME_MAX;

    raw_spin_lock(&amp;amp;cpu_base-&amp;gt;lock);
    entry_time = now = hrtimer_update_base(cpu_base); /*通过时钟源更新当前系统时间*/
retry:
    expires_next.tv64 = KTIME_MAX;
    /*
     * We set expires_next to KTIME_MAX here with cpu_base-&amp;gt;lock
     * held to prevent that a timer is enqueued in our queue via
     * the migration code. This does not affect enqueueing of
     * timers which run their callback and need to be requeued on
     * this CPU.
     */
    cpu_base-&amp;gt;expires_next.tv64 = KTIME_MAX;

    for (i = 0; i &amp;lt; HRTIMER_MAX_CLOCK_BASES; i++) { /*针对不同的计时参照对象依次处理*/
        struct hrtimer_clock_base *base;
        struct timerqueue_node *node;
        ktime_t basenow;

        if (!(cpu_base-&amp;gt;active_bases &amp;amp; (1 &amp;lt;&amp;lt; i)))
            continue;

        base = cpu_base-&amp;gt;clock_base + i;
        basenow = ktime_add(now, base-&amp;gt;offset);

        while ((node = timerqueue_getnext(&amp;amp;base-&amp;gt;active))) { /*根据到期时间依次处理红黑树中的定时器*/
            struct hrtimer *timer;

            timer = container_of(node, struct hrtimer, node);

            /*
             * The immediate goal for using the softexpires is
             * minimizing wakeups, not running timers at the
             * earliest interrupt after their soft expiration.
             * This allows us to avoid using a Priority Search
             * Tree, which can answer a stabbing querry for
             * overlapping intervals and instead use the simple
             * BST we already have.
             * We don't add extra wakeups by delaying timers that
             * are right-of a not yet expired timer, because that
             * timer will have to trigger a wakeup anyway.
             */

            if (basenow.tv64 &amp;lt; hrtimer_get_softexpires_tv64(timer)) {
                /*未到期则退出while循环*/

                ktime_t expires;

                expires = ktime_sub(hrtimer_get_expires(timer), base-&amp;gt;offset);
                if (expires.tv64 &amp;lt; 0)
                    expires.tv64 = KTIME_MAX;
                if (expires.tv64 &amp;lt; expires_next.tv64)
                    expires_next = expires;
                break;
            }

            __run_hrtimer(timer, &amp;amp;basenow); /*调用到期回调函数*/
        } /*end of while*/
    } /*end of for*/

    /*
     * Store the new expiry value so the migration code can verify
     * against it.
     */
    cpu_base-&amp;gt;expires_next = expires_next;
    raw_spin_unlock(&amp;amp;cpu_base-&amp;gt;lock);

    /*下面重新设置clock event device的中断触发时间，如果成功则返回*/
    /* Reprogramming necessary ? */
    if (expires_next.tv64 == KTIME_MAX ||
            !tick_program_event(expires_next, 0)) {
        cpu_base-&amp;gt;hang_detected = 0;
        return;
    }

    /*执行到此，后续的逻辑是处理一种特殊的场景，即定时器到期回调函数执行时间过长导致下一个定时器又到期了*/

    /*
     * The next timer was already expired due to:
     * - tracing
     * - long lasting callbacks
     * - being scheduled away when running in a VM
     *
     * We need to prevent that we loop forever in the hrtimer
     * interrupt routine. We give it 3 attempts to avoid
     * overreacting on some spurious event.
     *
     * Acquire base lock for updating the offsets and retrieving
     * the current time.
     */
    raw_spin_lock(&amp;amp;cpu_base-&amp;gt;lock);
    now = hrtimer_update_base(cpu_base);
    cpu_base-&amp;gt;nr_retries++;
    if (++retries &amp;lt; 3)
        goto retry;
    /*
     * Give the system a chance to do something else than looping
     * here. We stored the entry time, so we know exactly how long
     * we spent here. We schedule the next event this amount of
     * time away.
     */
    cpu_base-&amp;gt;nr_hangs++;
    cpu_base-&amp;gt;hang_detected = 1;
    raw_spin_unlock(&amp;amp;cpu_base-&amp;gt;lock);
    delta = ktime_sub(now, entry_time);
    if (delta.tv64 &amp;gt; cpu_base-&amp;gt;max_hang_time.tv64)
        cpu_base-&amp;gt;max_hang_time = delta;
    /*
     * Limit it to a sensible value as we enforce a longer
     * delay. Give the CPU at least 100ms to catch up.
     */
    if (delta.tv64 &amp;gt; 100 * NSEC_PER_MSEC)
        expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
    else
        expires_next = ktime_add(now, delta);
    tick_program_event(expires_next, 1);
    printk_once(KERN_WARNING &quot;hrtimer: interrupt took %llu ns\n&quot;, ktime_to_ns(delta));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过分析高精度模式下的时钟中断处理函数，我们可以发现它只负责处理定时器的到期处理。那么低精调模式下的进程调度的处理逻辑去哪里了？不需要了吗？其实，在前文代码中我们看到，高精度模式下内核会给每个CPU生成一个调度定时器：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-sched.c:

/**
 * tick_setup_sched_timer - setup the tick emulation timer
 */
void tick_setup_sched_timer(void)
{
    struct tick_sched *ts = &amp;amp;__get_cpu_var(tick_cpu_sched);
    ktime_t now = ktime_get();

    /*
     * Emulate tick processing via per-CPU hrtimers:
     */
    hrtimer_init(&amp;amp;ts-&amp;gt;sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
    ts-&amp;gt;sched_timer.function = tick_sched_timer; /*调度定时器的回调函数*/

    /* Get the next period (per cpu) */
    hrtimer_set_expires(&amp;amp;ts-&amp;gt;sched_timer, tick_init_jiffy_update());

    /* Offset the tick to avert jiffies_lock contention. */
    if (sched_skew_tick) {
        u64 offset = ktime_to_ns(tick_period) &amp;gt;&amp;gt; 1;
        do_div(offset, num_possible_cpus());
        offset *= smp_processor_id();
        hrtimer_add_expires_ns(&amp;amp;ts-&amp;gt;sched_timer, offset);
    }

    for (;;) {
        hrtimer_forward(&amp;amp;ts-&amp;gt;sched_timer, now, tick_period);
        hrtimer_start_expires(&amp;amp;ts-&amp;gt;sched_timer, HRTIMER_MODE_ABS_PINNED);
        /* Check, if the timer was already in the past */
        if (hrtimer_active(&amp;amp;ts-&amp;gt;sched_timer))
            break;
        now = ktime_get();
    }
    ...
}

/*
 * We rearm the timer until we get disabled by the idle code.
 * Called with interrupts disabled.
 */
static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
{
    struct tick_sched *ts =
        container_of(timer, struct tick_sched, sched_timer);
    struct pt_regs *regs = get_irq_regs();
    ktime_t now = ktime_get();

    tick_sched_do_timer(now);

    /*
     * Do not call, when we are not in irq context and have
     * no valid regs pointer
     */
    if (regs)
    tick_sched_handle(ts, regs);

    hrtimer_forward(timer, now, tick_period);

    return HRTIMER_RESTART;
}

static void tick_sched_do_timer(ktime_t now)
{
    int cpu = smp_processor_id();

    ...
    /* Check, if the jiffies need an update */
    if (tick_do_timer_cpu == cpu)
        tick_do_update_jiffies64(now);
}

static void tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)
{
    ...
    update_process_times(user_mode(regs));
    profile_tick(CPU_PROFILING);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  由此可见，调度定时器按tick_period周期性触发(暂不考虑动态时钟nohz特性)，每次到期后和处理逻辑和低精度模式下的逻辑类似。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/高精度定时器/&quot;&gt;【时间子系统】五、高精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】四、低精度定时器</title>
        <description>&lt;p&gt;  通过定时器，我们可以控制计算机在将来指定的某个时刻执行特定的动作。传统的定时器，以时钟滴答(jiffy)作为计时单位，因此它的精度较低(例如HZ=1000时，精度为1毫秒)，我们也称之为低精度定时器。&lt;/p&gt;

&lt;h3 id=&quot;1-初始化定时器&quot;&gt;1. 初始化定时器&lt;/h3&gt;

&lt;p&gt;  我们在概述中介绍过，内核中通过init_timer对定时器进行初始化，定时器中最关键的三个信息是：到期时间、到期处理函数、到期处理函数的参数。init_timer宏及定时器结构struct timer_list(取名struct timer可能更合适)的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timer.h:

#define init_timer(timer)                       \
    __init_timer((timer), 0)

#define __init_timer(_timer, _flags)            \
    init_timer_key((_timer), (_flags), NULL, NULL)

struct timer_list {
    /*
     * All fields that change during normal runtime grouped to the
     * same cacheline
     */
    struct list_head entry; /*用于将当前定时器挂到CPU的tvec_base链表中*/
    unsigned long expires; /*定时器到期时间*/
    struct tvec_base *base; /*定时器所属的tvec_base*/

    void (*function)(unsigned long); /*到期处理函数*/
    unsigned long data; /*到期处理函数的参数*/

    int slack; /*允许的偏差值*/

    ...
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  init_timer_key实现时，会将定时器指向执行初始化动作的CPU的tvec_base结构。内核为每个CPU分配一个struct tvec_base对象，用来记录每个CPU上定时器相关的全局信息(我们将在下一节详细说明)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * init_timer_key - initialize a timer
 * @timer: the timer to be initialized
 * @flags: timer flags
 * @name: name of the timer
 * @key: lockdep class key of the fake lock used for tracking timer
 *       sync lock dependencies
 *
 * init_timer_key() must be done to a timer prior calling *any* of the
 * other timer functions.
 */
void init_timer_key(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    debug_init(timer);
    do_init_timer(timer, flags, name, key);
}

static void do_init_timer(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    struct tvec_base *base = __raw_get_cpu_var(tvec_bases);

    timer-&amp;gt;entry.next = NULL;
    timer-&amp;gt;base = (void *)((unsigned long)base | flags);
    timer-&amp;gt;slack = -1;
    ...
}

struct tvec_base {
    spinlock_t lock; /*同步当前tvec_base的链表操作*/
    struct timer_list *running_timer; /*正在运行(到期触发)的定时器*/
    unsigned long timer_jiffies; /*用于判断定时器是否到期的当前时间，通常和系统的jiffies值相等*/
    unsigned long next_timer; /*下一个到期的定时器的到期时间*/
    unsigned long active_timers; /*激活的定时器的个数*/
    struct tvec_root tv1; /*tv1~tv5是用于保存已添加定时器的链表，也称为时间轮*/
    struct tvec tv2;
    struct tvec tv3;
    struct tvec tv4;
    struct tvec tv5;
} ____cacheline_aligned;

/*
 * per-CPU timer vector definitions:
 */
#define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
#define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
#define TVN_SIZE (1 &amp;lt;&amp;lt; TVN_BITS)
#define TVR_SIZE (1 &amp;lt;&amp;lt; TVR_BITS)
#define TVN_MASK (TVN_SIZE - 1)
#define TVR_MASK (TVR_SIZE - 1)
#define MAX_TVAL ((unsigned long)((1ULL &amp;lt;&amp;lt; (TVR_BITS + 4*TVN_BITS)) - 1))

struct tvec {
    struct list_head vec[TVN_SIZE];
};

struct tvec_root {
    struct list_head vec[TVR_SIZE];
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-添加定时器&quot;&gt;2. 添加定时器&lt;/h3&gt;

&lt;p&gt;  add_timer将定时器添加到执行CPU的tvec_base的时间轮链表中。内核根据定时器到期时间与当前时间jiffies的差值(值越小说明到期时间越早)，将定时器分别挂到五个级别的链表数组，级别越低链表到期时间越早，如下表所示：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;链表数组&lt;/th&gt;
      &lt;th&gt;时间差&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;tv1&lt;/td&gt;
      &lt;td&gt;0-255(2^8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv2&lt;/td&gt;
      &lt;td&gt;256–16383(2^14)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv3&lt;/td&gt;
      &lt;td&gt;16384–1048575(2^20)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv4&lt;/td&gt;
      &lt;td&gt;1048576–67108863(2^26)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv5&lt;/td&gt;
      &lt;td&gt;67108864–4294967295(2^32)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  其中tv1的数组大小为TVR_SIZE， tv2 tv3 tv4 tv5的数组大小为TVN_SIZE，根据CONFIG_BASE_SMALL配置项的不同，它们有不同的大小。默认情况下，没有使能CONFIG_BASE_SMALL，TVR_SIZE的大小是256，TVN_SIZE的大小则是64，当需要节省内存空间时，也可以使能CONFIG_BASE_SMALL，这时TVR_SIZE的大小是64，TVN_SIZE的大小则是16，以下的讨论我都是基于没有使能CONFIG_BASE_SMALL的情况。当有一个新的定时器要加入时，系统根据定时器到期的jiffies值和timer_jiffies字段的差值来决定该定时器被放入tv1至tv5中的哪一个数组中，最终，系统中所有的定时器的组织结构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/timer_2.jpg&quot; height=&quot;350&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从add_timer代码实现上看，最终会调用__internal_add_timer并根据时间差将定时器加入到合适的链表中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void
__internal_add_timer(struct tvec_base *base, struct timer_list *timer)
{
    unsigned long expires = timer-&amp;gt;expires;
    unsigned long idx = expires - base-&amp;gt;timer_jiffies; /*idx即为时间差*/
    struct list_head *vec;

    if (idx &amp;lt; TVR_SIZE) {
        int i = expires &amp;amp; TVR_MASK; /*以超时时间(而非时间差idx)作为索引寻找对应的链表，方便后续的超时处理*/
        vec = base-&amp;gt;tv1.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; TVR_BITS) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv2.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 2 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv3.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 3 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + 2 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv4.vec + i;
    } else if ((signed long) idx &amp;lt; 0) {
        /*
         * Can happen if you add a timer with expires == jiffies,
         * or you set a timer to go off in the past
         */
        vec = base-&amp;gt;tv1.vec + (base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK);
    } else {
        int i;
        /* If the timeout is larger than MAX_TVAL (on 64-bit
         * architectures or with CONFIG_BASE_SMALL=1) then we
         * use the maximum timeout.
         */
        if (idx &amp;gt; MAX_TVAL) {
            idx = MAX_TVAL;
            expires = idx + base-&amp;gt;timer_jiffies;
        }
        i = (expires &amp;gt;&amp;gt; (TVR_BITS + 3 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv5.vec + i;
    }
    /*
     * Timers are FIFO:
     */
    list_add_tail(&amp;amp;timer-&amp;gt;entry, vec);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-触发定时器&quot;&gt;3. 触发定时器&lt;/h3&gt;

&lt;p&gt;  在时钟中断部分，我们提到过每次中断处理时都会调用run_local_timers进行本地定时器的处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/*
 * Called by the local, per-CPU timer interrupt on SMP.
 */
void run_local_timers(void)
{
    ...
    raise_softirq(TIMER_SOFTIRQ); /*最终在中断返回时进入软中断处理函数run_timer_softirq*/
}

/*
 * This function runs timers and the timer-tq in bottom half context.
 */
static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    ...

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) /*实际当前时间晚于base中记录的当前时间，说明需要更新base中时间或者有定时器到期*/
        __run_timers(base);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  定时器的到期处理逻辑中，总是先处理tv1中的定时器，如果tv1中所有的链表为空，再从tv2中移动链表并重新添加到tv1中；如果tv1和tv2中为空，再从tv3中移动链表重新添加到tv1和tv2中；依此类推。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * __run_timers - run all expired timers (if any) on this CPU.
 * @base: the timer vector to be processed.
 *
 * This function cascades all vectors and executes all expired timer
 * vectors.
 */
static inline void __run_timers(struct tvec_base *base)
{
    struct timer_list *timer;

    spin_lock_irq(&amp;amp;base-&amp;gt;lock);
    while (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) {
        struct list_head work_list;
        struct list_head *head = &amp;amp;work_list;
        int index = base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK; /*以base中的当前时间为索引取出已到期的定时器*/

        /*
         * Cascade timers:
         */
        /*如果低级链表为空，则从高级别链表中移动添加到低级别中*/
        if (!index &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv2, INDEX(0))) &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv3, INDEX(1))) &amp;amp;&amp;amp;
            !cascade(base, &amp;amp;base-&amp;gt;tv4, INDEX(2)))
                cascade(base, &amp;amp;base-&amp;gt;tv5, INDEX(3));
        ++base-&amp;gt;timer_jiffies; /*累加base中当前时间*/
        list_replace_init(base-&amp;gt;tv1.vec + index, &amp;amp;work_list);
        /*处理已到期的定时期的回调函数*/
        while (!list_empty(head)) {
            void (*fn)(unsigned long);
            unsigned long data;
            bool irqsafe;

            timer = list_first_entry(head, struct timer_list,entry);
            fn = timer-&amp;gt;function;
            data = timer-&amp;gt;data;
            irqsafe = tbase_get_irqsafe(timer-&amp;gt;base);

            timer_stats_account_timer(timer);

            base-&amp;gt;running_timer = timer;
            detach_expired_timer(timer, base);

            if (irqsafe) {
                spin_unlock(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock(&amp;amp;base-&amp;gt;lock);
            } else {
                spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock_irq(&amp;amp;base-&amp;gt;lock);
            }
        }
    }
    base-&amp;gt;running_timer = NULL;
    spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
}

#define INDEX(N) ((base-&amp;gt;timer_jiffies &amp;gt;&amp;gt; (TVR_BITS + (N) * TVN_BITS)) &amp;amp; TVN_MASK)

static int cascade(struct tvec_base *base, struct tvec *tv, int index)
{
    /* cascade all the timers from tv up one level */
    struct timer_list *timer, *tmp;
    struct list_head tv_list;

    list_replace_init(tv-&amp;gt;vec + index, &amp;amp;tv_list);

    /*
     * We are removing _all_ timers from the list, so we
     * don't have to detach them individually.
     */
    list_for_each_entry_safe(timer, tmp, &amp;amp;tv_list, entry) {
        BUG_ON(tbase_get_base(timer-&amp;gt;base) != base);
        /* No accounting, while moving them */
        __internal_add_timer(base, timer);
    }

    return index;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;【时间子系统】四、低精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】三、时钟中断－定时基础</title>
        <description>&lt;p&gt;  时钟中断是各种定时器(timer)能够正常工作的前提，同时它和进程调度(tick事件)也密不可分，因此在分析定时器原理前，我们先来深入了解一下时钟中断的原理。&lt;/p&gt;

&lt;h3 id=&quot;1-中断初始化&quot;&gt;1. 中断初始化&lt;/h3&gt;

&lt;p&gt;  时钟中断涉及时钟事件设备(Clock Event Device)等多个概念，我们先通过分析初始化流程来理解这些概念。&lt;/p&gt;

&lt;h4 id=&quot;11-bsp初始化阶段&quot;&gt;&lt;strong&gt;1.1. BSP初始化阶段&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  时钟中断的初始化发生在启动CPU(BSP)上，由start_kernel函数作为总体入口。在完成IO-APIC中断控制器的相关初始化动作后，由late_time_init作为初始化入口。针对x86架构，该函数的实现体为x86_late_time_init：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/time.c:

void __init time_init(void)
{
    late_time_init = x86_late_time_init;
}

static __init void x86_late_time_init(void)
{
    x86_init.timers.timer_init(); /*指向hpet_time_init*/
    ...
}

void __init hpet_time_init(void)
{
    if (!hpet_enable())
        setup_pit_timer();
    setup_default_timer_irq();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  在我们的示例架构i440fx下，hpet没有使能，因此系统将使用PIT作为启动CPU(BSP)的本地tick设备(tick事件发生源)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/i8253.c:

void __init setup_pit_timer(void)
{
    clockevent_i8253_init(true); /*PIT芯片代号为8253*/
    global_clock_event = &amp;amp;i8253_clockevent;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PIT本质上是一种全局时钟事件设备，也就是说它不和某一个CPU绑定，这和后文将介绍的LAPIC Timer不同。然而在BSP初始化过程中，它将暂时被用作BSP的本地tick设备。后续在完成SMP的初始化后，每个CPU都有各自不同的本地tick设备(即本地LAPIC Timer)。tick设备的作用就是周期性(由内核配置参数HZ控制，例始HZ=1000代表每秒产生1000个tick中断)地产生时钟中断，CPU在处理中断的过程中可以决定是否需要进行进程调度。&lt;/p&gt;

&lt;p&gt;  每个时钟事件设备可以有两种工作模式：单次模式(oneshot)和周期模式(periodic)。工作在单次模式时，每次设置完到期时间后，时钟事件设备只会产生一次中断；而工作在周期模式时，时钟事件设备会以设定频率周期性地产生中断。单次模式相比周期模式具备更强的灵活性，我们可以动态控制时钟中断的间隔，从而实现像动态时钟(nohz)之类的高级特性(我们将在后续博文专题介绍)。从下面的代码中，我们可以看出PIT可以同时支持oneshot和periodic两种模式，并在初始化时指定其亲和CPU为当前执行CPU(即BSP)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/driver/clocksource/i8253.c:

/*
 * On UP the PIT can serve all of the possible timer functions. On SMP systems
 * it can be solely used for the global tick.
 */
struct clock_event_device i8253_clockevent = {
    .name           = &quot;pit&quot;,
    .features       = CLOCK_EVT_FEAT_PERIODIC,
    .set_mode       = init_pit_timer,
    .set_next_event = pit_next_event,
};

void __init clockevent_i8253_init(bool oneshot)
{
    if (oneshot)
        i8253_clockevent.features |= CLOCK_EVT_FEAT_ONESHOT;
    /*
     * Start pit with the boot cpu mask. x86 might make it global
     * when it is used as broadcast device later.
     */
    i8253_clockevent.cpumask = cpumask_of(smp_processor_id());

    clockevents_config_and_register(&amp;amp;i8253_clockevent, PIT_TICK_RATE,
            0xF, 0x7FFF);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PIT是时钟事件设备的一种具体硬件实现，从软件抽象层来说，各种时钟事件设备都会调度内核的clockevents中的注册函数进行注册：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
/**
 * clockevents_config_and_register - Configure and register a clock event device
 * @dev:	device to register
 * @freq:	The clock frequency
 * @min_delta:	The minimum clock ticks to program in oneshot mode
 * @max_delta:	The maximum clock ticks to program in oneshot mode
 *
 * min/max_delta can be 0 for devices which do not support oneshot mode.
 */
void clockevents_config_and_register(struct clock_event_device *dev,
    u32 freq, unsigned long min_delta, unsigned long max_delta)
{
    dev-&amp;gt;min_delta_ticks = min_delta;
    dev-&amp;gt;max_delta_ticks = max_delta;
    clockevents_config(dev, freq); /*根据内部计数器频率计算相关转换参数*/
    clockevents_register_device(dev);
}

void clockevents_register_device(struct clock_event_device *dev)
{
    unsigned long flags;

    ...

    raw_spin_lock_irqsave(&amp;amp;clockevents_lock, flags);

    list_add(&amp;amp;dev-&amp;gt;list, &amp;amp;clockevent_devices); /*将当前设备加入到全局clockevent_devices链表中*/
    tick_check_new_device(dev); /*检测当前设备是否适合作当前执行CPU的本地tick设备或全局broadcast设备*/
    clockevents_notify_released(); /*对于被释放的设备，重新加入全局列表并作tick_check_new_device检测*/

    raw_spin_unlock_irqrestore(&amp;amp;clockevents_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  如上代码所示，新注册一个事钟设备时都会对其进行检测，以判断其是否适合作为本地tick设备(由struct tick_device定义，它是对struct clock_event_device的封装)。如果新的设备适合作本地tick设备，那将替换原有的tcik设备(如果在存的话)。被替换的老设备将有机会重新加入全局clockevent_devices链表并进行检测，此时的检测主要是判定它是否适合作为广播(broadcast)设备。广播设备的作用是为了当某些本地tick设备随CPU进入节电状态而停止工作时，能够再次发生中断以唤醒进入节电状态的CPU继续进行工作。这种情况下本地tick设备是无能为力的，因为它也随CPU进入睡眠状态了。这里我们只需要理解广播设备的作用，不用太深挖其内部实现细节：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

void tick_check_new_device(struct clock_event_device *newdev)
{
    struct clock_event_device *curdev;
    struct tick_device *td;
    int cpu;
    unsigned long flags;

    raw_spin_lock_irqsave(&amp;amp;tick_device_lock, flags);

    cpu = smp_processor_id(); /*取当前执行CPU*/
    if (!cpumask_test_cpu(cpu, newdev-&amp;gt;cpumask)) /*判断当前CPU是否在新设备的CPU掩码位中*/
        goto out_bc; /*不在，则转而判断新设备是否可作为bc(broadcast)设备*/

    td = &amp;amp;per_cpu(tick_cpu_device, cpu); /*取出当前CPU的本地tick设备*/
    curdev = td-&amp;gt;evtdev; /*本地tick设备所封装的当前时钟事件设备，可能为空*/

    /* cpu local device ? */
    if (!tick_check_percpu(curdev, newdev, cpu)) /*判断新设备是否更适合作本地tick设备*/
        goto out_bc; /*如果不合适则进行bc判定*/

    /* Preference decision */
    if (!tick_check_preferred(curdev, newdev)) /*判断新设备是否符合偏好，如oneshot优先等*/
        goto out_bc;

    if (!try_module_get(newdev-&amp;gt;owner))
        return;

    /*如果执行到这里，说明新设备newdev相比老设备curdev更适合作本地tick设备，将进行替换操作*/

    /*
     * Replace the eventually existing device by the new
     * device. If the current device is the broadcast device, do
     * not give it back to the clockevents layer !
     */
    if (tick_is_broadcast_device(curdev)) { /*如果老设备是一个广播设备将对其进行关闭*/
        clockevents_shutdown(curdev);
        curdev = NULL;
    }
    clockevents_exchange_device(curdev, newdev); /*进行交换*/
    tick_setup_device(td, newdev, cpu, cpumask_of(cpu)); /*重新设定新设备为本地tick设备*/
    if (newdev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_ONESHOT)
        tick_oneshot_notify();

    raw_spin_unlock_irqrestore(&amp;amp;tick_device_lock, flags);
    return;

out_bc:
    /*
     * Can the new device be used as a broadcast device ?
     */
    tick_install_broadcast_device(newdev);
    raw_spin_unlock_irqrestore(&amp;amp;tick_device_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于新设定的本地tick设备(没有老设备进行替换时)，初始时内核总是将其设为周期模式(periodic)。随着系统的运行，当外部条件成熟后，在时钟中断的处理过程中会将它的模式切换到单次模式(oneshot)以支持更高级功能。这部分切换我们将在高精度时钟部分进行分析。这里我们看看tick_setup_device的基本动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

static void tick_setup_device(struct tick_device *td,
        struct clock_event_device *newdev, int cpu,
        const struct cpumask *cpumask)
{
    ktime_t next_event;
    void (*handler)(struct clock_event_device *) = NULL;

    /*
     * First device setup ?
     */
    if (!td-&amp;gt;evtdev) {
        /*
         * If no cpu took the do_timer update, assign it to
         * this cpu:
         */
        if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
            if (!tick_nohz_full_cpu(cpu))
                tick_do_timer_cpu = cpu; /*动态时钟未打开情况下，初始化过程中首个注册本地tick设备的CPU将负责在中断处理时完成jiffies更新*/
            else
                tick_do_timer_cpu = TICK_DO_TIMER_NONE;
            tick_next_period = ktime_get(); /*下次tick事件发生时间，这里初始化为当前时间*/
            tick_period = ktime_set(0, NSEC_PER_SEC / HZ); /*tick的时间间隔*/
        }

        /*
         * Startup in periodic mode first.
         */
        td-&amp;gt;mode = TICKDEV_MODE_PERIODIC; /*首次注册tick设备时，将其设为periodic模式*/
    } else {
        handler = td-&amp;gt;evtdev-&amp;gt;event_handler;
        next_event = td-&amp;gt;evtdev-&amp;gt;next_event;
        td-&amp;gt;evtdev-&amp;gt;event_handler = clockevents_handle_noop;
    }

    td-&amp;gt;evtdev = newdev;

    /*
     * When the device is not per cpu, pin the interrupt to the
     * current cpu:
     */
    if (!cpumask_equal(newdev-&amp;gt;cpumask, cpumask))
        irq_set_affinity(newdev-&amp;gt;irq, cpumask);

    /*
     * When global broadcasting is active, check if the current
     * device is registered as a placeholder for broadcast mode.
     * This allows us to handle this x86 misfeature in a generic
     * way. This function also returns !=0 when we keep the
     * current active broadcast state for this CPU.
     */
    if (tick_device_uses_broadcast(newdev, cpu))
    return;

    if (td-&amp;gt;mode == TICKDEV_MODE_PERIODIC)
        tick_setup_periodic(newdev, 0);
    else
        tick_setup_oneshot(newdev, handler, next_event);
}

void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
    tick_set_periodic_handler(dev, broadcast); /*设置dev-&amp;gt;event_handler为tick_handle_periodic*/

    ...

    if ((dev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_PERIODIC) &amp;amp;&amp;amp;
            !tick_broadcast_oneshot_active()) {
        clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC); /*设置时钟事件设备的工作模式为周期模式，内部将调用set_mode函数*/
    } else {
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  回到上层初始化流程，完成将PIT设置为BSP的本地tick设备后，内核在setup_default_timer_irq中完成中断处理函数的设定并使能中断信号。之后BSP在初始化过程中有会周期性地收到PIT产生的0号时钟中断，并进行中断处理。对于PIT时钟中断的处理我们将在下一节展开：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
static struct irqaction irq0  = {
    .handler    = timer_interrupt,
    .flags      = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,
    .name       = &quot;timer&quot;
};

void __init setup_default_timer_irq(void)
{
    setup_irq(0, &amp;amp;irq0); /*设备中断处理对象并使能中断信号，0号中断即时钟中断*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;12-smp初始化阶段&quot;&gt;&lt;strong&gt;1.2. SMP初始化阶段&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在x86 SMP系统中，每个CPU的Local APIC中都有一个高精度的时钟事件设备(LAPIC Timer)，因此在BSP初始化的最后阶段及AP的初始化过程中，都会调用setup_APIC_timer进行LAPIC Timer的初始化：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/apic/apic.c:

static void __cpuinit setup_APIC_timer(void)
{
    struct clock_event_device *levt = &amp;amp;__get_cpu_var(lapic_events); /*每个CPU对应的lapic timer*/

    if (this_cpu_has(X86_FEATURE_ARAT)) { /*ARAT: Always Run Apic Timer，intel实现的特性；timer不随CPU睡眠而停止*/
        lapic_clockevent.features &amp;amp;= ~CLOCK_EVT_FEAT_C3STOP;
        /* Make LAPIC timer preferrable over percpu HPET */
        lapic_clockevent.rating = 150;
    }

    memcpy(levt, &amp;amp;lapic_clockevent, sizeof(*levt));
    levt-&amp;gt;cpumask = cpumask_of(smp_processor_id());

    if (this_cpu_has(X86_FEATURE_TSC_DEADLINE_TIMER)) {
        ...
    } else
        clockevents_register_device(levt);
}

/*
 * The local apic timer can be used for any function which is CPU local.
 */
static struct clock_event_device lapic_clockevent = {
    .name       = &quot;lapic&quot;,
    .features   = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT
                    | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_DUMMY,/*DUMMY标志在BSP初始化时将会被清除*/
    .shift      = 32,
    .set_mode   = lapic_timer_setup,
    .set_next_event	= lapic_next_event,
    .broadcast  = lapic_timer_broadcast,
    .rating     = 100,
    .irq        = -1,
};
static DEFINE_PER_CPU(struct clock_event_device, lapic_events);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  setup_APIC_timer函数内部同样是调用clockevents_register_device进行注册，对于BSP它将使用lapic timer替换PIT作为本地tick设备，而PIT将设为广播设备；对于AP，将直接使用lapic timer作为本地tick设备。注意，对于lapic timer的处理函数入口为smp_apic_timer_interrupt，它是在中断系统初始化过程(start_kernel-&amp;gt;init_IRQ-&amp;gt;apic_intr_init)中设定的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/irqinit.c:

static void __init apic_intr_init(void)
{
    ...
    /*apic_timer_interrupt将跳转到smp_apic_timer_interrupt*/
    alloc_intr_gate(LOCAL_TIMER_VECTOR, apic_timer_interrupt);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-周期性中断处理&quot;&gt;2. 周期性中断处理&lt;/h3&gt;

&lt;h4 id=&quot;21-pit中断处理&quot;&gt;&lt;strong&gt;2.1. PIT中断处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  BSP完成对PIT的初始化并使能中断信号后，BSP便可周期性地接收到来自PIT的中断，它对该中断的处理句柄是：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/time.c:

/*
 * Default timer interrupt handler for PIT/HPET
 */
static irqreturn_t timer_interrupt(int irq, void *dev_id)
{
    global_clock_event-&amp;gt;event_handler(global_clock_event);
    return IRQ_HANDLED;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里的global_clock_event即是i8253_clockevent，它最初工作在周期模式下，相应的处理函数为tick_handle_periodic：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

/*
 * Event handler for periodic ticks
 */
void tick_handle_periodic(struct clock_event_device *dev)
{
    int cpu = smp_processor_id();
    ktime_t next;

    /*实际的周期性处理逻辑*/
    tick_periodic(cpu);

    /*对于周期模式的时钟事件设备直接返回，无须设置下次到期时间*/
    if (dev-&amp;gt;mode != CLOCK_EVT_MODE_ONESHOT)
        return;
    
    /*对于单次模式的设备，如果要实现周期性中断，则在每次中断处理中要设置下次到期时间*/
    /*
     * Setup the next period for devices, which do not have
     * periodic mode:
     */
    next = ktime_add(dev-&amp;gt;next_event, tick_period);
    for (;;) {
        if (!clockevents_program_event(dev, next, false))
            return;
        /*
         * Have to be careful here. If we're in oneshot mode,
         * before we call tick_periodic() in a loop, we need
         * to be sure we're using a real hardware clocksource.
         * Otherwise we could get trapped in an infinite
         * loop, as the tick_periodic() increments jiffies,
         * when then will increment time, posibly causing
         * the loop to trigger again and again.
         */
        if (timekeeping_valid_for_hres())
            tick_periodic(cpu);
        next = ktime_add(next, tick_period);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  tick_periodic负责实际的处理逻辑，它主要完成对jiffies和xtime(墙上时间)的周期性更新，并对进程进行运行计时和调度：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

/*
 * Periodic tick
 */
static void tick_periodic(int cpu)
{
    if (tick_do_timer_cpu == cpu) {
        /*如果当前CPU负责计时更新，则调用do_timer进行更新*/
        write_seqlock(&amp;amp;jiffies_lock);

        /* Keep track of the next tick event */
        tick_next_period = ktime_add(tick_next_period, tick_period);

        do_timer(1);
        write_sequnlock(&amp;amp;jiffies_lock);
    }

    /*更新进程运行时间并做调度判断*/
    update_process_times(user_mode(get_irq_regs()));
    profile_tick(CPU_PROFILING);
}

/*
 * Must hold jiffies_lock
 */
void do_timer(unsigned long ticks)
{
    jiffies_64 += ticks;
    update_wall_time(); /*周期性地更新墙上时间*/
    calc_global_load(ticks);
}

void update_process_times(int user_tick)
{
    struct task_struct *p = current;
    int cpu = smp_processor_id();

    /* Note: this timer irq context must be accounted for as well. */
    account_process_tick(p, user_tick); /*当前进程运行时间统计*/
    run_local_timers(); /*检查本地定时器，我们将在定时器部分分析*/
    ...
    scheduler_tick(); /*调度检测*/
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;22-lapic-timer中断处理&quot;&gt;&lt;strong&gt;2.2. LAPIC Timer中断处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SMP初始化完成后，所有CPU的本地tick设备变更为LAPIC Timer，虽然其工作模式仍然是周期性模式，但中断处理函数入口变更为smp_apic_timer_interrupt：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/apic/apic.c:

void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
{
    struct pt_regs *old_regs = set_irq_regs(regs);

    /*
     * NOTE! We'd better ACK the irq immediately,
     * because timer handling can be slow.
     */
    ack_APIC_irq();
    /*
     * update_process_times() expects us to have done irq_enter().
     * Besides, if we don't timer interrupts ignore the global
     * interrupt lock, which is the WrongThing (tm) to do.
     */
    irq_enter();
    exit_idle();
    local_apic_timer_interrupt();
    irq_exit();

    set_irq_regs(old_regs);
}

/*
 * The guts of the apic timer interrupt
 */
static void local_apic_timer_interrupt(void)
{
    int cpu = smp_processor_id();
    struct clock_event_device *evt = &amp;amp;per_cpu(lapic_events, cpu);

    /*
     * Normally we should not be here till LAPIC has been initialized but
     * in some cases like kdump, its possible that there is a pending LAPIC
     * timer interrupt from previous kernel's context and is delivered in
     * new kernel the moment interrupts are enabled.
     *
     * Interrupts are enabled early and LAPIC is setup much later, hence
     * its possible that when we get here evt-&amp;gt;event_handler is NULL.
     * Check for event_handler being NULL and discard the interrupt as
     * spurious.
     */
    if (!evt-&amp;gt;event_handler) {
        pr_warning(&quot;Spurious LAPIC timer interrupt on cpu %d\n&quot;, cpu);
        /* Switch it off */
        lapic_timer_setup(CLOCK_EVT_MODE_SHUTDOWN, evt);
        return;
    }

    /*
     * the NMI deadlock-detector uses this.
     */
    inc_irq_stat(apic_timer_irqs);

    evt-&amp;gt;event_handler(evt);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  对于周期模式的LAPIC Timer，其event_hander仍然为tick_handle_periodic，因此核心处理逻辑和PIT是一样的。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/时钟中断/&quot;&gt;【时间子系统】三、时钟中断－定时基础&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E6%97%B6%E9%92%9F%E4%B8%AD%E6%96%AD/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E6%97%B6%E9%92%9F%E4%B8%AD%E6%96%AD/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】二、计时原理－timekeeper与clocksource</title>
        <description>&lt;p&gt;  本篇博文我们将深入分析一下内核是如何使用计时硬件对应用提供服务的。&lt;/p&gt;

&lt;h3 id=&quot;1-内核表示时间数据结构&quot;&gt;1. 内核表示时间数据结构&lt;/h3&gt;

&lt;p&gt;  内核中对时间的表示有多种形式，可以使用在不同的应用场景。我们在时间概述中看到的gettimeofday的示例中，采用的数据结构是struct timeval，它的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/time.h:

struct timeval {
    __kernel_time_t         tv_sec;     /* seconds */
    __kernel_suseconds_t    tv_usec;    /* microseconds */
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  从上面的定义中，我们可以看到struct timeval记录了当前时间的秒数和毫秒数，精度就是毫秒。那么这里的秒数和毫秒数是相对哪个时间点(epoch)而言的呢？按照UNIX系统的习惯，记录时间的秒数和毫秒数是相对1970年1月1日00:00:00 +0000(UTC)而言的。另外，记录秒数的__kernel_time_t和记录毫秒的__kernel_suseconds_t在64位系统中都是long型的。&lt;/p&gt;

&lt;p&gt;  除了struct timeval，内核中还定义了精度更高的struct timespec，它的精度是纳秒：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/time.h:

struct timespec {
    __kernel_time_t tv_sec;     /* seconds */
    long            tv_nsec;    /* nanoseconds */
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  此外，为了兼容各种系统架构，内核也定义了ktime_t类型，在64位机器中对应long，时间表示单位是纳秒：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ktime.h:

union ktime {
    s64	tv64;
#if BITS_PER_LONG != 64 &amp;amp;&amp;amp; !defined(CONFIG_KTIME_SCALAR)
    struct {
#ifdef __BIG_ENDIAN
        s32	sec, nsec;
#else
        s32	nsec, sec;
#endif
    } tv;
#endif
};

typedef union ktime ktime_t;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-内核时间类别&quot;&gt;2. 内核时间类别&lt;/h3&gt;

&lt;p&gt;  时间概述中示例程序使用的gettimeofday将返回实时间(real time，或叫墙上时间wall time)，代表现实生活中使用的时间。除了墙上时间，内核也提供了线性时间(monotonic time，它不可调整，随系统运行线性增加，但不包括休眠时间)、启动时间(boot time，它也不可调整，并包括了休眠时间)等多种时间类型，以使应用在不同场景(获取不同类型时间的用户态方法是clock_gettime)，下表汇总了各类时间的要素点：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;时间类别&lt;/th&gt;
      &lt;th&gt;精度&lt;/th&gt;
      &lt;th&gt;可手动调整&lt;/th&gt;
      &lt;th&gt;受NTP调整影响&lt;/th&gt;
      &lt;th&gt;时间起点&lt;/th&gt;
      &lt;th&gt;受闰秒影响&lt;/th&gt;
      &lt;th&gt;系统暂停时是否可工作&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC_RAW&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME_COARSE&lt;/td&gt;
      &lt;td&gt;tick&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC_COARSE&lt;/td&gt;
      &lt;td&gt;tick&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BOOTTIME&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;machine start&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME_ALARM&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BOOTTIME_ALARM&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;machine start&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TAI&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  关于闰秒，我们需要先理解什么是原子秒？原子秒提出的背景是人们对于”秒”的精确定义追求。多长时间可以算作1秒？这是一个很难准确回答的问题。但是后来科学家发现铯133原子在能量跃迁时辐射的电磁波振荡频率非常稳定，因此就被用来定义时间的基本单位：秒，即原子秒。通过原子秒延展出来的时间轴就是TAI(International Atomic Time)。原子时间虽然精准，但是对人类来说不太友好，它和传统的地球自转和公转的周期性自然现象存在时间差。在这样的背景下，UTC(Coordinated Universal Time)被提出。它使用原子秒作为计时单位，但又会适当调整以适应人们的日常生活。这个调整的时间差就是闰秒。TAI和UTC在1972进行了校准，两者相差10秒，从此后到2017年，又调整了27次，因此TAI比UTC快了37秒。&lt;/p&gt;

&lt;h3 id=&quot;3-深入do_gettimeofday&quot;&gt;3. 深入do_gettimeofday&lt;/h3&gt;

&lt;p&gt;  用户态gettimeofday接口在内核中是通过do_gettimeofday实现的，从调用层次上看，它可以分为timekeeper和clocksource两层。&lt;/p&gt;

&lt;h4 id=&quot;31-timekeeper&quot;&gt;&lt;strong&gt;3.1 timekeeper&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  timekeeper是内核中负责计时功能的核心对象，它通过使用当前系统中最优的clocksource来提供时间服务：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/**
 * do_gettimeofday - Returns the time of day in a timeval
 * @tv:		pointer to the timeval to be set
 *
 * NOTE: Users should be converted to using getnstimeofday()
 */
void do_gettimeofday(struct timeval *tv)
{
    struct timespec now;

    getnstimeofday(&amp;amp;now); /*获取纳秒精度的当前时间*/
    tv-&amp;gt;tv_sec = now.tv_sec;
    tv-&amp;gt;tv_usec = now.tv_nsec/1000;
}

/**
 * __getnstimeofday - Returns the time of day in a timespec.
 * @ts:		pointer to the timespec to be set
 *
 * Updates the time of day in the timespec.
 * Returns 0 on success, or -ve when suspended (timespec will be undefined).
 */
int __getnstimeofday(struct timespec *ts)
{
    struct timekeeper *tk = &amp;amp;timekeeper; /*系统全局对象timekeeper*/
    unsigned long seq;
    s64 nsecs = 0;

    do {
        seq = read_seqcount_begin(&amp;amp;timekeeper_seq); /*以顺序锁来同步各个任务对timekeeper的读写操作*/

        ts-&amp;gt;tv_sec = tk-&amp;gt;xtime_sec; /*获取最近更新的墙上时间的秒数(墙上时间会周期性地被更新，将在定时原理部分讨论)*/
        nsecs = timekeeping_get_ns(tk); /*获取当前墙上时间相对(tk-&amp;gt;xtime_sec, 0)的纳秒时间间隔*/

    } while (read_seqcount_retry(&amp;amp;timekeeper_seq, seq));

    ts-&amp;gt;tv_nsec = 0;
    timespec_add_ns(ts, nsecs);/*累加前面获取的纳秒时间间隔以得到正确的当前墙上时间；有可能导致秒数进位*/

    ...
    return 0;
}

static inline s64 timekeeping_get_ns(struct timekeeper *tk)
{
    cycle_t cycle_now, cycle_delta;
    struct clocksource *clock;
    s64 nsec;

    /*通过当前最优clocksource获取当前时间计数cycle；不同的clocksource可以提供不同的read实现*/
    /* read clocksource: */
    clock = tk-&amp;gt;clock;
    cycle_now = clock-&amp;gt;read(clock);
    
    /*通过clocksource中的当前计数值与最近一次更新墙上时间时获取的值的差值来计算时间间隔*/

    /* calculate the delta since the last update_wall_time: */    
    cycle_delta = (cycle_now - clock-&amp;gt;cycle_last) &amp;amp; clock-&amp;gt;mask;

    /*tk-&amp;gt;mult和tk-&amp;gt;shift是用来进行将cycle数值转成纳秒的转换参数，参见clocksource中的说明*/
    nsec = cycle_delta * tk-&amp;gt;mult + tk-&amp;gt;xtime_nsec; /*tk-&amp;gt;xtime_nsec是最近更新的墙上时间的秒纳数左移tk-&amp;gt;shift后的值*/
    nsec &amp;gt;&amp;gt;= tk-&amp;gt;shift;

    /* If arch requires, add in get_arch_timeoffset() */
    return nsec + get_arch_timeoffset();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timekeeper_internal.h:

/* Structure holding internal timekeeping values. */
struct timekeeper {
    /* Current clocksource used for timekeeping. */
    struct clocksource	*clock;
    /* NTP adjusted clock multiplier */
    u32			mult;
    /* The shift value of the current clocksource. */
    u32			shift;
    /* Number of clock cycles in one NTP interval. */
    cycle_t			cycle_interval;
    /* Last cycle value (also stored in clock-&amp;gt;cycle_last) */
    cycle_t			cycle_last;
    /* Number of clock shifted nano seconds in one NTP interval. */
    u64			xtime_interval;
    /* shifted nano seconds left over when rounding cycle_interval */
    s64			xtime_remainder;
    /* Raw nano seconds accumulated per NTP interval. */
    u32			raw_interval;

    /* Current CLOCK_REALTIME time in seconds */
    u64			xtime_sec;
    /* Clock shifted nano seconds */
    u64			xtime_nsec;

    /* Difference between accumulated time and NTP time in ntp
     * shifted nano seconds. */
    s64			ntp_error;
    /* Shift conversion between clock shifted nano seconds and
     * ntp shifted nano seconds. */
    u32			ntp_error_shift;

    /*
     * wall_to_monotonic is what we need to add to xtime (or xtime corrected
     * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
     * at zero at system boot time, so wall_to_monotonic will be negative,
     * however, we will ALWAYS keep the tv_nsec part positive so we can use
     * the usual normalization.
     *
     * wall_to_monotonic is moved after resume from suspend for the
     * monotonic time not to jump. We need to add total_sleep_time to
     * wall_to_monotonic to get the real boot based time offset.
     *
     * - wall_to_monotonic is no longer the boot time, getboottime must be
     * used instead.
     */
    struct timespec		wall_to_monotonic;
    /* Offset clock monotonic -&amp;gt; clock realtime */
    ktime_t			offs_real;
    /* time spent in suspend */
    struct timespec		total_sleep_time;
    /* Offset clock monotonic -&amp;gt; clock boottime */
    ktime_t			offs_boot;
    /* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
    struct timespec		raw_time;
    /* The current UTC to TAI offset in seconds */
    s32			tai_offset;
    /* Offset clock monotonic -&amp;gt; clock tai */
    ktime_t			offs_tai;

};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;32-clocksource&quot;&gt;&lt;strong&gt;3.2 clocksource&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  内核通过clocksource对象来描述物理计时设备，x86架构下最常见的计时设备是tsc，我们来看看tsc对应的clocksource:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/tsc.c:

static struct clocksource clocksource_tsc = {
    .name                   = &quot;tsc&quot;,
    .rating                 = 300,
    .read                   = read_tsc,
    .resume                 = resume_tsc,
    .mask                   = CLOCKSOURCE_MASK(64),
    .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
                              CLOCK_SOURCE_MUST_VERIFY,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/clocksource.h:

/**
 * struct clocksource - hardware abstraction for a free running counter
 *	Provides mostly state-free accessors to the underlying hardware.
 *	This is the structure used for system time.
 *
 * @name:		ptr to clocksource name
 * @list:		list head for registration
 * @rating:		rating value for selection (higher is better)
 *			To avoid rating inflation the following
 *			list should give you a guide as to how
 *			to assign your clocksource a rating
 *			1-99: Unfit for real use
 *				Only available for bootup and testing purposes.
 *			100-199: Base level usability.
 *				Functional for real use, but not desired.
 *			200-299: Good.
 *				A correct and usable clocksource.
 *			300-399: Desired.
 *				A reasonably fast and accurate clocksource.
 *			400-499: Perfect
 *				The ideal clocksource. A must-use where
 *				available.
 * @read:		returns a cycle value, passes clocksource as argument
 * @enable:		optional function to enable the clocksource
 * @disable:		optional function to disable the clocksource
 * @mask:		bitmask for two's complement
 *			subtraction of non 64 bit counters
 * @mult:		cycle to nanosecond multiplier
 * @shift:		cycle to nanosecond divisor (power of two)
 * @max_idle_ns:	max idle time permitted by the clocksource (nsecs)
 * @maxadj:		maximum adjustment value to mult (~11%)
 * @flags:		flags describing special properties
 * @archdata:		arch-specific data
 * @suspend:		suspend function for the clocksource, if necessary
 * @resume:		resume function for the clocksource, if necessary
 * @cycle_last:		most recent cycle counter value seen by ::read()
 */
struct clocksource {
    /*
     * Hotpath data, fits in a single cache line when the
     * clocksource itself is cacheline aligned.
     */
    cycle_t (*read)(struct clocksource *cs);
    cycle_t cycle_last;
    cycle_t mask;
    u32 mult;
    u32 shift;
    u64 max_idle_ns;
    u32 maxadj;
#ifdef CONFIG_ARCH_CLOCKSOURCE_DATA
    struct arch_clocksource_data archdata;
#endif

    const char *name;
    struct list_head list;
    int rating;
    int (*enable)(struct clocksource *cs);
    void (*disable)(struct clocksource *cs);
        unsigned long flags;
    void (*suspend)(struct clocksource *cs);
    void (*resume)(struct clocksource *cs);

    /* private: */
#ifdef CONFIG_CLOCKSOURCE_WATCHDOG
    /* Watchdog related data, used by the framework */
    struct list_head wd_list;
    cycle_t cs_last;
    cycle_t wd_last;
#endif
} ____cacheline_aligned;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  上面代码的注释部分已经清楚地解析了tsc是一种精度良好的clocksource，我们可以使用read_tsc(本质是通过rdtsc指令)来获取当前tsc计数值。cycle_last表示最近一次从clocksource中获取的cycle计数值；mask表示当前clocksource中计数器的有效位数；mult和shift用来计算从cycle到纳秒的转换；max_idle_ns表示当前clocksource允许的最长时间更新间隔，因为如果CPU长期不更新时间，将会导致再次获取到的cycle计数值过大，使得转换成纳秒时发生溢出错误。从理论计算上说，将cycle转换成纳秒的公式是”cycle * 每秒纳秒数 / 频率”，但是由于内核无法进行浮点运算，只能通过一种变通的方法来计算，即”cycle * mult » shift”，这里的mult和shif就是基于频率、计算精度和最大表示范围计算而得的。&lt;/p&gt;

&lt;h3 id=&quot;4-计时初始化&quot;&gt;4. 计时初始化&lt;/h3&gt;

&lt;p&gt;  最后我们再来看看内核计时功能的初始化过程，了解一下计时功能是如何一步步生效的。timekeeper的初始化是在内核启动过程start_kernel中调用timekeeping_init进行：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/*
 * timekeeping_init - Initializes the clocksource and common timekeeping values
 */
void __init timekeeping_init(void)
{
    struct timekeeper *tk = &amp;amp;timekeeper;
    struct clocksource *clock;
    unsigned long flags;
    struct timespec now, boot, tmp;

    /*x86架构下，persistent clock为系统RTC时钟源，我们先从中获取当前时间，精度为秒*/
    read_persistent_clock(&amp;amp;now);

    if (!timespec_valid_strict(&amp;amp;now)) {
        pr_warn(&quot;WARNING: Persistent clock returned invalid value!\n&quot;
                &quot;         Check your CMOS/BIOS settings.\n&quot;);
        now.tv_sec = 0;
        now.tv_nsec = 0;
    } else if (now.tv_sec || now.tv_nsec)
        persistent_clock_exist = true;

    /*x86架构下，没有boot clock，所以boot time为0*/
    read_boot_clock(&amp;amp;boot);
    if (!timespec_valid_strict(&amp;amp;boot)) {
        pr_warn(&quot;WARNING: Boot clock returned invalid value!\n&quot;
                &quot;         Check your CMOS/BIOS settings.\n&quot;);
        boot.tv_sec = 0;
        boot.tv_nsec = 0;
    }

    raw_spin_lock_irqsave(&amp;amp;timekeeper_lock, flags);
    write_seqcount_begin(&amp;amp;timekeeper_seq);
    ntp_init();

    /*获取系统默认时钟源，x86架构中即为jiffies*/
    clock = clocksource_default_clock();
    if (clock-&amp;gt;enable)
    clock-&amp;gt;enable(clock);
    /*将jiffy设备timekeeper中的时钟源并设定内部相关变量*/
    tk_setup_internals(tk, clock);

    /*将当前时间设定为tk的墙上时间，注其中tk-&amp;gt;xtime_sec为当前秒数，tk-&amp;gt;xtime_nsec为纳秒左移shift位后的值*/
    tk_set_xtime(tk, &amp;amp;now);
    /*raw_time设为0*/
    tk-&amp;gt;raw_time.tv_sec = 0;
    tk-&amp;gt;raw_time.tv_nsec = 0;
    /*boot time设为墙上时间*/
    if (boot.tv_sec == 0 &amp;amp;&amp;amp; boot.tv_nsec == 0)
        boot = tk_xtime(tk);

    /*将monotonic time减去wall time的时间偏移记录下来*/
    set_normalized_timespec(&amp;amp;tmp, -boot.tv_sec, -boot.tv_nsec);
    tk_set_wall_to_mono(tk, tmp);

    /*将sleep time初始化为零*/
    tmp.tv_sec = 0;
    tmp.tv_nsec = 0;
    tk_set_sleep_time(tk, tmp);

    /*备份当前timekeeper*/
    memcpy(&amp;amp;shadow_timekeeper, &amp;amp;timekeeper, sizeof(timekeeper));

    write_seqcount_end(&amp;amp;timekeeper_seq);
    raw_spin_unlock_irqrestore(&amp;amp;timekeeper_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  内核jiffies变量代表时间滴答，是对同期性tick事件的记数，因此可以将它视为一个最为简单的时钟源。它和一般时钟源的不同之处在于它没有实际的计时设备与之对应，完全是记录在计算机内存中；另外它的精度和系统tick数相关。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/jiffies.c:

struct clocksource * __init __weak clocksource_default_clock(void)
{
    return &amp;amp;clocksource_jiffies;
}

static struct clocksource clocksource_jiffies = {
    .name		= &quot;jiffies&quot;,
    .rating		= 1, /* lowest valid rating*/
    .read		= jiffies_read,
    .mask		= 0xffffffff, /*32bits*/
    .mult		= NSEC_PER_JIFFY &amp;lt;&amp;lt; JIFFIES_SHIFT, /* details above */
    .shift		= JIFFIES_SHIFT,
};

static cycle_t jiffies_read(struct clocksource *cs)
{
    return (cycle_t) jiffies;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于tsc时钟源，在内核对模块进行初始化时，会注册tsc时钟，并通知timekeeper将时钟源从jiffies切换到tsc:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int __init init_tsc_clocksource(void)
{
    ...
    if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE)) {
        /*注册一个频率为tsc_khz的时钟源，最终调用__clocksource_register_scale实现*/
        clocksource_register_khz(&amp;amp;clocksource_tsc, tsc_khz);
    return 0;
    }
    ...
}
/*
 * We use device_initcall here, to ensure we run after the hpet
 * is fully initialized, which may occur at fs_initcall time.
 */
device_initcall(init_tsc_clocksource);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/clocksource.c:

/**
 * __clocksource_register_scale - Used to install new clocksources
 * @cs:		clocksource to be registered
 * @scale:	Scale factor multiplied against freq to get clocksource hz
 * @freq:	clocksource frequency (cycles per second) divided by scale
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 *
 * This *SHOULD NOT* be called directly! Please use the
 * clocksource_register_hz() or clocksource_register_khz helper functions.
 */
int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
{

    /*首先根据时钟源的频率计算合适的mult和shift，以及最大更新延时max_idle_ns*/
    /* Initialize mult/shift and max_idle_ns */
    __clocksource_updatefreq_scale(cs, scale, freq);

    /* Add clocksource to the clcoksource list */
    mutex_lock(&amp;amp;clocksource_mutex);
    clocksource_enqueue(cs); /*加入到全局clocksource_list*/
    clocksource_enqueue_watchdog(cs); /*加入到时钟源监控中，如果发现当前时钟源精度下降会重新选择更优的时钟源*/
    clocksource_select(); /*选择最优的时钟源，内部会调用timekeeping_notify通知timerkeeper*/
    mutex_unlock(&amp;amp;clocksource_mutex);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/**
* timekeeping_notify - Install a new clock source
* @clock:		pointer to the clock source
*
* This function is called from clocksource.c after a new, better clock
* source has been registered. The caller holds the clocksource_mutex.
*/
void timekeeping_notify(struct clocksource *clock)
{
    struct timekeeper *tk = &amp;amp;timekeeper;

    if (tk-&amp;gt;clock == clock)
        return;
    /*注意，这里会暂停所有CPU的运行，并选定一个默认的CPU(0号核)执行change_clocksource。
      这是因为时钟源是计时的基础，在进行时钟源切换时系统将无法提供正确的时间服务。只有当切换
      完成后系统才可恢复运行。*/
    stop_machine(change_clocksource, clock, NULL);
    tick_clock_notify();
}

/**
 * change_clocksource - Swaps clocksources if a new one is available
 *
 * Accumulates current time interval and initializes new clocksource
 */
static int change_clocksource(void *data)
{
    struct timekeeper *tk = &amp;amp;timekeeper;
    struct clocksource *new, *old;
    unsigned long flags;

    new = (struct clocksource *) data;

    raw_spin_lock_irqsave(&amp;amp;timekeeper_lock, flags);
    write_seqcount_begin(&amp;amp;timekeeper_seq);

    timekeeping_forward_now(tk);
    if (!new-&amp;gt;enable || new-&amp;gt;enable(new) == 0) {
        old = tk-&amp;gt;clock;
        tk_setup_internals(tk, new);
        if (old-&amp;gt;disable)
            old-&amp;gt;disable(old);
    }
    timekeeping_update(tk, true, true);

    write_seqcount_end(&amp;amp;timekeeper_seq);
    raw_spin_unlock_irqrestore(&amp;amp;timekeeper_lock, flags);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/计时/&quot;&gt;【时间子系统】二、计时原理－timekeeper与clocksource&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%AE%A1%E6%97%B6/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%AE%A1%E6%97%B6/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】一、概述</title>
        <description>&lt;p&gt;  除了计算、存储、网络等核心子系统外，计算机内部还包含时间子系统，它对系统的运行起着重要作用。&lt;/p&gt;

&lt;h3 id=&quot;什么是计算机的时间子系统&quot;&gt;什么是计算机的时间子系统？&lt;/h3&gt;

&lt;p&gt;  计算机内的时间子系统包含多种时间设备，我们可以把这些设备分为两大类：&lt;strong&gt;计时&lt;/strong&gt;设备和&lt;strong&gt;定时通知&lt;/strong&gt;设备。&lt;/p&gt;

&lt;p&gt;  我们可以将计时设备理解为生活中所见的”墙上挂钟”或者”手表”，它们为系统提供了时间(时刻)。常见的计时设备有TSC(Time Stamp Counter)、RTC(Real Time Clock)、ACPI_PM：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;TSC是每个CPU内部的一个计数器，它按CPU主频以固定频率递增，例如一个2400Mhz的CPU，计数器每秒逐一增加2400M个计数值。计数器在CPU启动时初始化为零，假设我们已知CPU启动时刻，那么只要把当前计算器的值除以频率再加上启动时刻，就可以得知当前时间了。&lt;/li&gt;
    &lt;li&gt;RTC是位于CMOS电路中的一个计时设备，它与TSC相比的优点是有独立的电池供电，即使计算机下电，RTC计时器仍可以继续工作；缺点是计数频率较低，因此时间精度较差。通常我们初始时将RTC计数器的值设置为当前时刻相对于1970年1月1日的时间差，因此通过RTC提供的寄存器接口，我们可以直接获取到当前时间。&lt;/li&gt;
    &lt;li&gt;ACPI_PM通常是南桥中的APCI电源管理模块提供的计时设备，其精度较低，通常不推荐使用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  我们可以将定时通知设备理解为生活中所见的”闹钟”，它们周期性地或者在一定时间间隔后向系统通知到期事件。常见的通知设备有Local APIC Timer、PIT(Programmable Interval Timer)、HPET(High Precision Event Timer)。初始时我们向时间通知设备的计数器中写入一个到期计数值，然后时间通知设备按固定频率递减计数器中的值，当计数器值为零时便通过中断向CPU通知事件发生。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Local APIC Timer是每个CPU的本地中断控制器(APIC)内部的定时设备，精度较高，是系统正常运行时采用的通知设备。&lt;/li&gt;
    &lt;li&gt;PIT是CPU之外的独立定时通知设备，属全局设备，精度较低，通常不使用。&lt;/li&gt;
    &lt;li&gt;HPET也是全局定时通知设备，精度较高，需要系统中含专属硬件。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  计算机用户是无法直接使用这些时间设备的，必须通过运行在CPU上的内核程序，应用程序或者用户才能最终获取计时和时间通知服务。因此我们可以将计算机时间子系统分为硬件和软件两部分：各种时间设备属于硬件部分，内核使能这些硬件的模块(也称内核时间子系统)属于软件部分。内核中将计时设备称为&lt;strong&gt;时钟源(Clock Source)&lt;/strong&gt;，将定时通知设备称为&lt;strong&gt;时钟事件设备(Clock Event Device)&lt;/strong&gt;。时间子系统整体结构如下图所示(感谢droidphone的分享，&lt;a href=&quot;http://blog.csdn.net/droidphone/article/details/8017604&quot;&gt;原文链接&lt;/a&gt;)，后续我们将深入内核分析其实现原理。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/time_1.jpg&quot; height=&quot;280&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;h3 id=&quot;为什么需要时间子系统&quot;&gt;为什么需要时间子系统？&lt;/h3&gt;

&lt;p&gt;  基于各种时间设备和内核时间子系统模块，应用程序可以实现计时和到期通知(&lt;strong&gt;定时器&lt;/strong&gt;)功能：例如我们桌面应用中的日历程序就使用了计时功能来提供实时时间，又例如邮件客户端的定时接收功能就使用了定时器。此外，内核自己的进程调度功能也依赖于内核的定时器，从而进行周期性的调度决策。&lt;/p&gt;

&lt;p&gt;  下面我们给出了两段示例程序来展示时间子系统的基本使用方法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//用户态计时功能示例程序

#include&amp;lt;stdio.h&amp;gt;
#include&amp;lt;sys/time.h&amp;gt;
#include&amp;lt;unistd.h&amp;gt;

int main()
{
    struct  timeval    tv;
    struct  timezone   tz;

    gettimeofday(&amp;amp;tv,&amp;amp;tz); /*获取当前时间*/

    printf(“tv_sec:%d\n”,tv.tv_sec);
    printf(“tv_usec:%d\n”,tv.tv_usec);

    printf(“tz_minuteswest:%d\n”,tz.tz_minuteswest);
    printf(“tz_dsttime:%d\n”,tz.tz_dsttime);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//用户态定时器参考setitime，这里以内核使用定时器示例

/*1. 初始化定时器结构*/
init_timer(&amp;amp;wb_timer);

/*2. 定时器超时函数*/
wb_timer.function = wb_timer_function; 

/*3.或者初始化定时器和超时函数作为一步(data作为fn的参数)*/
setup_timer(timer, fn, data)    

/*4. 添加定时器*/
add_timer(&amp;amp;buttons_timer); 

/*5. 设置定时器超时时间 1\100 s（修改一次超时时间只会触发一次定时器*/
mod_timer(&amp;amp;buttons_timer, jiffies+HZ/100); 

/*6.删除定时器*/
del_timer(&amp;amp;timer);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;内核如何实现时间子系统&quot;&gt;内核如何实现时间子系统？&lt;/h3&gt;

&lt;p&gt;  后续我们将以一系统博文深入分析内核时间子系统的实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/01/计时/&quot;&gt;计时原理－timekeeper与clocksource&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/时钟中断/&quot;&gt;时钟中断－定时基础&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;低精度定时器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/高精度定时器/&quot;&gt;高精度定时器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;动态时钟－nohz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/时间子系统概述/&quot;&gt;【时间子系统】一、概述&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 19 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E6%97%B6%E9%97%B4%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E6%97%B6%E9%97%B4%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【计算子系统】进程管理之二：进程替换</title>
        <description>&lt;p&gt;  本篇讨论进程替换(exec)，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是进程替换为什么需要它&quot;&gt;什么是进程替换？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  进程替换是用新的代码和数据替换当前进程已有代码和数据，从而开始执行新的业务逻辑。&lt;/p&gt;

&lt;p&gt;  通过fork创建出来的进程是继承父进程的代码和数据，如果想要进程执行一些新的任务，那就得从磁盘程序中加载新的代码和数据并替换当前进程已有的代码和数据。&lt;/p&gt;

&lt;h3 id=&quot;如何实现进程替换&quot;&gt;如何实现进程替换？&lt;/h3&gt;

&lt;h4 id=&quot;1-exec用户态示例代码&quot;&gt;&lt;strong&gt;1. exec用户态示例代码&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来看看在用户态程序中是如何实现替换的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exec_test.c:

#include &amp;lt;stdio.h&amp;gt;  
#include &amp;lt;unistd.h&amp;gt;  

/*示例函数fork一个新进程并在其中执行ps命令*/
int main()  
{
    /*构造命令行参数ps_argv和环境变量ps_envp*/
    char *const ps_argv[] ={&quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL};  
    char *const ps_envp[] ={&quot;PATH=/bin:/usr/bin&quot;, &quot;TERM=console&quot;, NULL};

    if(fork()==0){ 
        /*执行execve系统调用，第一个参数表示可执行文件的位置，第二个表示命令行参数，第三个表示环境变量。
          这里注意exec是一个函数簇，其有6种类似的系统调用：
          (1)6种调用的前4个字符相同，均是exec;
          (2)第5位有v和l两种，v表示向量表示法，l表示逐个列举;
          (3)第6位有e和p两种，e表示带环境变量，p表示可执行程序以文件名方式查找，而不是路径查找;*/
        if(execve(&quot;/bin/ps&quot;, ps_argv, ps_envp) &amp;lt; 0)  {  
            perror(&quot;execve error!&quot;);  
            return -1 ;  
        }  
    }  
    return 0 ;  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-elf可执行文件格式解析&quot;&gt;&lt;strong&gt;2. ELF可执行文件格式解析&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  通过上面的例子，我们看到应用程序通过调用execve系统调用实现执行程序的替换。Linux平台上主流的可执行文件格式是ELF(Executable and Linkable Format，类似Windows平台上的exe文件格式)，如果想深入分析execve调用功能，那我们就得了解ELF文件格式，详细规范说明&lt;a href=&quot;http://www.skyfree.org/linux/references/ELF_Format.pdf&quot;&gt;点此&lt;/a&gt;进入。&lt;/p&gt;

&lt;h5 id=&quot;21-示例程序&quot;&gt;&lt;strong&gt;2.1. 示例程序&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  真正理解ELF格式后，我们能够将C语言源文件与ELF二进制文件进行对应，这样可以提升对底层系统问题的定位能力。因此为方便大家入门，这里以一个简单的C语言程序为例，来逐一对应ELF中的各部分：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/tmp_analyze_elf/main.c:

#include &amp;lt;stdio.h&amp;gt;

void say_hello(char *who)
{
    printf(&quot;hello, %s!\n&quot;, who);
}

char *my_name = &quot;wb&quot;;

int main()
{
    say_hello(my_name);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们将其编译生成名为app的可执行程序并运行：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;linux-XqAfQZ:~/tmp_analyze_elf # &lt;strong&gt;gcc -o&lt;/strong&gt; app main.c&lt;br /&gt;
linux-XqAfQZ:~/tmp_analyze_elf # &lt;strong&gt;./app&lt;/strong&gt;&lt;br /&gt;
hello, wb!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;22-elf整体布局&quot;&gt;&lt;strong&gt;2.2. ELF整体布局&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  ELF格式可以表达三种类型的二进制对象文件(object files)：可重定位文件(relocatable file，就是大家平常见到的.o文件)、可执行文件(executable file, 例上述示例代码生成的app文件)、共享库文件(shared object files，就是.so文件，用来做动态链接)。可重定位文件用在编译和链接阶段；可执行文件用在程序运行阶段；共享库则同时用在编译链接和运行阶段。在不同阶段，我们可以用不同视角来理解ELF文件，整体布局如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_1.jpg&quot; height=&quot;300&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从上图可见，ELF格式文件整体可分为四大部分：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;ELF Header, ELF头部，定义全局性信息；&lt;/li&gt;
    &lt;li&gt;Program Header Table， 描述段(Segment)信息的数组，每个元素对应一个段；通常包含在可执行文件中，可重定文件中可选(通常不包含)；&lt;/li&gt;
    &lt;li&gt;Segment and Section，段(Segment)由若干区(Section)组成；段在运行时被加载到进程地址空间中，包含在可执行文件中；区是段的组成单元，包含在可执行文件和可重定位文件中；&lt;/li&gt;
    &lt;li&gt;Section Header Table，描述区(Section)信息的数组，每个元素对应一个区；通常包含在可重定位文件中，可执行文件中为可选(通常包含)；&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;23-elf-header实例解析&quot;&gt;&lt;strong&gt;2.3. ELF Header实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  ELF规范中对ELF Header中各字段的定义如下所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_2.jpg&quot; height=&quot;300&quot; width=&quot;350&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  接下来我们通过readelf -h命令来看看示例程序app中的ELF Header内容，显示结果如下图：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-header.jpg&quot; height=&quot;380&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;e_ident&lt;/strong&gt;含前16个字节，又可细分成class、data、version等字段，具体含义不用太关心，只需知道前4个字节点包含”ELF”关键字，这样可以判断当前文件是否是ELF格式；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_type&lt;/strong&gt;表示具体ELF类型，可重定位文件/可执行文件/共享库文件，显然这里是一个可执行文件；&lt;strong&gt;e_machine&lt;/strong&gt;表示执行的机器平台，这里是x86_64；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_version&lt;/strong&gt;表示文件版本号，这里的1表示初始版本号；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_entry&lt;/strong&gt;对应”Entry point address”，程序入口函数地址，通过进程虚拟地址空间地址(0x400440)表达；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phoff&lt;/strong&gt;对应“Start of program headers”，表示program header table在文件内的偏移位置，这里是从第64号字节(假设初始为0号字节)开始；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shoff&lt;/strong&gt;对应”Start of section headers”，表示section header table在文件内的偏移位置，这里是从第4472号字节开始，靠近文件尾部；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_flags&lt;/strong&gt;表示与CPU处理器架构相关的信息，这里为零；&lt;strong&gt;e_ehsize&lt;/strong&gt;对应”Size of this header”，表示本ELF header自身的长度，这里为64个字节，回看前面的e_phoff为64，说明ELF header后紧跟着program header table；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phentsize&lt;/strong&gt;对应“Size of program headers”，表示program header table中每个元素的大小，这里为56个字节；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phnum&lt;/strong&gt;对应”Number of program headers”，表示program header table中元素个数，这里为9个；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shentsize&lt;/strong&gt;对应”Size of section headers”，表示section header table中每个元素的大小，这里为64个字节；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shnum&lt;/strong&gt;对应”Number of section headers”，表示section header table中元素的个数，这里为30个；&lt;/li&gt;
    &lt;li&gt;最后， &lt;strong&gt;e_shstrndx&lt;/strong&gt;对应”Section header string table index”，表示描述各section字符名称的string table在section header table中的下标，详见后文对string table的介绍。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;24-program-header-table实例解析&quot;&gt;&lt;strong&gt;2.4. Program Header Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  Program Header Table是一个数组，每个元素叫Program Header，规范对其结构定义如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_3.jpg&quot; height=&quot;200&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  同样我们用readelf -l命令查看示例程序的program header table：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-program-header1.jpg&quot; height=&quot;500&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  上图截取了readelf命令返回的上半部，我们重点看下前面几项：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;PHDR&lt;/strong&gt;，此类型header元素描述了program header table自身的信息。从这里的内容看出，示例程序的program header table在文件中的偏移(Offset)为0x40，即64号字节处；该段映射到进程空间的虚拟地址(VirtAddr)为0x400040；PhysAddr暂时不用，其保持和VirtAddr一致；该段占用的文件大小FileSiz为00x1f8；运行时占用进程空间内存大小MemSiz也为0x1f8；Flags标记表示该段的读写权限，这里”R E”表示可读可执行，说明本段属于代码段；Align对齐为8，表明本段按8字节对齐。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;INTERP&lt;/strong&gt;，此类型header元素描述了一个特殊内存段，该段内存记录了动态加载解析器的访问路径字符串。示例程序中，该段内存位于文件偏移0x238处，即紧跟program header table；映射的进程虚拟地址空间地址为0x400238；文件长度和内存映射长度均为0x1c，即28个字符，具体内容为”/lib64/ld-linux-x86-64.so.2”；段属性为只读，并按字节对齐；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;LOAD&lt;/strong&gt;，此类型header元素描述了可加载到进程空间的代码段或数据段：第三项为代码段，文件内偏移为0，映射到进程地址0x400000处，代码段长度为0x764个字节，属性为只读可执行，段地址按2M边界对齐；第四段为数据段，文件内偏移为0xe10，映射到进程地址为0x600e10处(按2M对齐移动)，文件大小为0x230，内存大小为0x238(因为其内部包含了8字节的bss段，即未初始化数据段，该段内容不占文件空间，但在运行时需要为其分配空间并清零)，属性为读写，段地址也按2M边界对齐。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;DYNAMIC&lt;/strong&gt;，此类型header元素描述了动态加载段，其内部通常包含了一个名为”.dynamic”的动态加载区；这也是一个数组，每个元素描述了与动态加载相关的各方面信息，我们将在动态加载中介绍。该段是从文件偏移0xe28处开始，长度为0x1d0，并映射到进程的0x600e28；可见该段和上一个数据段是有重叠的。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  readelf命令返回内容的下半部分给出了各段(segment)和各区(section)之间的包含关系，如下图所示。INTERP段只包含了”.interp”区；代码段包含”.interp”、”.plt”、”.text”等区；数据段包含”.dynamic”、”.data”、”.bss”等区；DYNAMIC段包含”.dynamic”区。从这里可以看出，有些区被包含在多个段中。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-program-header2.jpg&quot; height=&quot;100&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;25-section-header-table实例解析&quot;&gt;&lt;strong&gt;2.5. Section Header Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  针对各区的描述信息由Section Header Table提供，该数组中每个元素的定义如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_4.jpg&quot; height=&quot;200&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  下面我们再通过readelf -S命令看看示例程序中section header table的内容，如下图所示。示例程序共生成30个区，Name表示每个区的名字，Type表示每个区的功能，Address表示每个区的进程映射地址，Offset表示文件内偏移，Size表示区的大小，EntSize表示区中每个元素的大小(如果该区为一个数组的话，否则该值为0)，Flags表示每个区的属性(参见图中最后的说明)，Link和Info记录不同类型区的相关信息(不同类型含义不同，具体参见规范)，Align表示区的对齐单位。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-section-header1.jpg&quot; height=&quot;600&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-section-header2.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;26-string-table实例解析&quot;&gt;&lt;strong&gt;2.6. String Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  从上述Section Header Table示例中，我们看到有一种类型为STRTAB的区(在Section Header Table中的下标为6,27,29)。此类区叫做String Table，其作用是集中记录字符串信息，其它区在需要使用字符串的时候，只需要记录字符串起始地址在该String Table表中的偏移即可，而无需包含整个字符串内容。&lt;/p&gt;

&lt;p&gt;  我们使用readelf -x读出下标27区的详细内容观察：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-strtable1.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  红框内为该区实际内容，左侧为区内偏移地址，后侧为对应内容的字符表示。我们可以发现，这里其实是一堆字符串，这些字符串对应的就是各个区的名字。因此section header table中每个元素的Name字段其实是这个string table的索引。再回头看看ELF header中的e_shstrndx，它的值正好就是27，指向了当前的string table。&lt;/p&gt;

&lt;p&gt;  同理再来看下29区的内容，如下图所示。这里我们看到了”main”、”say_hello”字符串，这些是我们在示例中源码中定义的符号，由此可以29区是应用自身的String Table，记录了应用使用的字符串。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-strtable2.jpg&quot; height=&quot;700&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;27-symbol-table实例解析&quot;&gt;&lt;strong&gt;2.7. Symbol Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  Section Header Table中，还有一类SYMTAB(DYNSYM)区，该区叫符号表。符号表中的每个元素对应一个符号，记录了每个符号对应的实际数值信息，通常用在重定位过程中或问题定位过程中，进程执行阶段并不加载符号表。符号表中每个元素定义如下：name表示符号对应的源码字符串，为对应String Table中的索引；value表示符号对应的数值；size表示符号对应数值的空间占用大小；info表示符号的相关信息，如符号类型(变量符号、函数符号)；shndx表示与该符号相关的区的索引，例如函数符号与对应的代码区相关。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_5.jpg&quot; height=&quot;160&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们用readelf -s读出示例程序中的符号表，如下图所示。如红框中内容所示，我们示例程序定义的main函数符号对应的数值为0x400554，其类型为FUNC，大小为26字节，对应的代码区在Section Header Table中的索引为13；say_hello函数符号对应数值为0x400530，其类型为FUNC，大小为36字节，对应的代码区也为13。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-symtable1.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-symtable2.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;28-代码段实例解析&quot;&gt;&lt;strong&gt;2.8. 代码段实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  在理解了String Table和Symbol Table的作用后，我们通过objdump反汇编来理解一下.text代码段：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-main-code.jpg&quot; height=&quot;300&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  这里截取了与示例程序相关部分，我们看到0x400530和0x400554两处各定义一个函数，其符号分别为say_hello和main，这部分信息实际是通过符号表解析而来的；在涉及到内存地址的指令中，除了对数据段地址的引用是通过绝对地址进行的之外，对于代码段地址的引用都是以相对地址的方式进行的，这样做的好处是在二进制文件的重定位过程中，我们不用修改指令的访问地址(因为相对地址保持不变)；最后，我们看到对于库函数printf的访问指向了代码段地址0x400410，那么这个地址处放的是printf函数么？要回答这个问题就涉及动态链接，我们将在下文专题分析。&lt;/p&gt;

&lt;h5 id=&quot;29-动态链接dynamic-linking&quot;&gt;&lt;strong&gt;2.9. 动态链接(Dynamic Linking)&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  基于模块化设计思路，我们在应用开发时会将基础的、共用的功能抽取出来，设计成可共享的库。应用程序在编译时只是建立了与共享库的联系，并不将其包含在内；运行时，由系统负责加载所需的共享库。这就是动态链接，如此一来，既可以节省磁盘和内存的空间占用(相同功能在磁盘和内存中均只存在一份)，也可以方便基础模块自身的优化改进。&lt;/p&gt;

&lt;p&gt;  要实现动态链接，需要解决两个大的问题：(1)共享库内部的函数的地址访问需要与加载地址无关，因为不同的程序可能将库加载到不同的地址处；回顾2.8中的代码分析，我们看到这个可以通过相对寻址的方式解决。(2)调用共享库的应用程序如何能够获知共享库的加载地址并准确对其进行调用？&lt;/p&gt;

&lt;p&gt;  ELF规范对问题2的解决方法给出明确思路：系统中需要有一个Program Interpreter配合内核完成进程执行上下文的准备。Program Interpreter可以是一个可执行程序，也可以是一个共享库；在Linux x86_64平台下，这个解析器就是/lib64/ld-ld-linux-x86-64.so.2，就是由INTERP段指明的。解析器和内核需要配合完成以下动作：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;内核加载可执行程序和解析器到进程空间，之后将控制权交给解析器；&lt;/li&gt;
    &lt;li&gt;解析器加载共享库到进程空间；&lt;/li&gt;
    &lt;li&gt;解析器进行重定位；&lt;/li&gt;
    &lt;li&gt;解析器将控制权交给进程正常执行。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  解析器工作时需要从DYNAMIC段中获取信息，并通过GOT(Global Offset Table)记录解析后的库函数地址；应用程序通过PLT(Procedure Linkage Table)中的代码间接访问GOT，并最终完成向目标库函数的跳转。&lt;/p&gt;

&lt;p&gt;  结合我们的示例程序，我们通过readelf -d获取DYNAMIC段中的信息，如下图所示。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  重点看一下红框中标出的两行，NEEDED表示当前可执行程序依赖的共享库，这里只有libc.so.6一项；PLTGOT表示当前程序调用共享库时使用的GOT表的地址为0x601000，回看前文的Section Header Table，可知对应区的索引为23。我们接着可以看看该区的内容：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic-got.jpg&quot; height=&quot;100&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从23区的section header中可知该区为一个数组，每个元素大小为8字节。结合规范中的说明可知，GOT中的前三个元素有着特殊作用：第一个元素转换成地址为0x600e28，即为DYNAMIC段的映射地址；第二元素和第三个元素会在解析器获得控制权后被放置动态解析函数的参数和入口地址，其作用将在后续说明PLT功能是说明。从第四个元素起，每个元素代表一个调用地址，依次为0x400416、0x400426、0x400436，这些地址对应什么内容呢？&lt;/p&gt;

&lt;p&gt;  我们通过objdump来看看.plt段的内容：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic-plt.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  plt包含几段相似的汇编指令，回顾前文，.text代码段的say_hello在调用printf时访问的函数地址即为0x40010，对应plt中第二段汇编的第一条指令。这是一条jump指令，跳转到0x601018即GOT表的第4个元素。前文分析时指出，GOT中第4个元素为0x400416，正好对应jump指令的下一条指令。感觉上转了一圈却只是跳到了下一条指令处，有点多余，那么我们接着往下分析。后续的push指令把0压入了栈中(代表每个调用函数的索引，这里printf索引值为0),然后跳转到plt表中的第一段汇编指令处。这里把GOT表的第二个元素压力栈中，然后跳转到GOT表中第三个元素指定的函数。这个函数是解析器的动态解析函数，它的作用是针对栈中的调用函数索引，找到调用函数的实际加载地址，并将该地址填入GOT表中对应的元素位置(这里就是将printf的实际加载地址填入0x601018处(即GOT表中的第4个元素))，然后跳转到printf处执行。当应用程序再次调用printf时，跳转到plt中对应的函数后，那里的jump指令将根据GOT表中被解析器更新后的地址直接跳转到printf处开始执行，这样就不用解析器的干预了，从而达到了动态跳转的目的。&lt;/p&gt;

&lt;p&gt;  我们对动态调用做个总结：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;需要进行动态调用的可执行程序在编译时会自动生成DYNAMIC段、GOT表和PLT表；&lt;/li&gt;
    &lt;li&gt;对动态库的每个函数调用都会在GOT(从第4个元素开始)和PLT(从第二段汇编指令开始)中生成一项；&lt;/li&gt;
    &lt;li&gt;解析器在获得控制权后会在GOT第2个元素和第3个元素放置解析函数的参数和入口地址；PLT的第一段汇编指令会将GOT第2个元素压栈并跳转到第3个元素指定的函数位置；&lt;/li&gt;
    &lt;li&gt;进行过一次动态调用后，GOT中对应的元素中就记录了库函数的实际加载地址，后续的调用就可以进行直接跳转。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;3-深入内核sys_execve&quot;&gt;&lt;strong&gt;3. 深入内核sys_execve&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们在2.9节中介绍过，内核和解析器相互配合完了进程执行空间的替换，下面我们深入内核代码来看看sys_execve的实现原理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/fs/exec.c:

SYSCALL_DEFINE3(execve,
        const char __user *, filename,
        const char __user *const __user *, argv,
        const char __user *const __user *, envp)
{
    struct filename *path = getname(filename);
    int error = PTR_ERR(path);
    if (!IS_ERR(path)) {
        error = do_execve(path-&amp;gt;name, argv, envp);
        putname(path);
    }
    return error;
}

int do_execve(const char *filename,
            const char __user *const __user *__argv,
    const char __user *const __user *__envp)
{
    struct user_arg_ptr argv = { .ptr.native = __argv };
    struct user_arg_ptr envp = { .ptr.native = __envp };
    return do_execve_common(filename, argv, envp);
}

/*
 * sys_execve() executes a new program.
 */
static int do_execve_common(const char *filename,
                    struct user_arg_ptr argv,
                    struct user_arg_ptr envp)
{
    struct linux_binprm *bprm;
    struct file *file;
    struct files_struct *displaced;
    bool clear_in_exec;
    int retval;
    const struct cred *cred = current_cred();

    ...

    retval = -ENOMEM;
    bprm = kzalloc(sizeof(*bprm), GFP_KERNEL);
    if (!bprm)
        goto out_files;

    ...

    /*打开可执行文件*/
    file = open_exec(filename);
    ...

    bprm-&amp;gt;file = file;
    bprm-&amp;gt;filename = filename;
    bprm-&amp;gt;interp = filename;

    /*初始化将替换当前进程的mm_struct*/
    retval = bprm_mm_init(bprm);
    ...

    /*计算命令行参数和环境变量个数*/
    bprm-&amp;gt;argc = count(argv, MAX_ARG_STRINGS);
    ...
    bprm-&amp;gt;envc = count(envp, MAX_ARG_STRINGS);
    ...

    /*准备bprm中相关信息并读取可执行文件头部*/
    retval = prepare_binprm(bprm);
    ...

    /*复制信息到用户栈空间*/
    retval = copy_strings_kernel(1, &amp;amp;bprm-&amp;gt;filename, bprm);
    ...
    bprm-&amp;gt;exec = bprm-&amp;gt;p;
    retval = copy_strings(bprm-&amp;gt;envc, envp, bprm);
    ...
    retval = copy_strings(bprm-&amp;gt;argc, argv, bprm);
    ...

    /*在内核中搜索能够加载当前可执行文件的二进制解析驱动，这里是elf驱动，
      最终会调用elf_load_binary*/
    retval = search_binary_handler(bprm);
    ...

    return retval;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/fs/binfmt_elf.c:

static int load_elf_binary(struct linux_binprm *bprm)
{
    struct file *interpreter = NULL; /* to shut gcc up */
    unsigned long load_addr = 0, load_bias = 0;
    int load_addr_set = 0;
    char * elf_interpreter = NULL;
    unsigned long error;
    struct elf_phdr *elf_ppnt, *elf_phdata;
    unsigned long elf_bss, elf_brk;
    int retval, i;
    unsigned int size;
    unsigned long elf_entry;
    unsigned long interp_load_addr = 0;
    unsigned long start_code, end_code, start_data, end_data;
    unsigned long reloc_func_desc __maybe_unused = 0;
    int executable_stack = EXSTACK_DEFAULT;
    unsigned long def_flags = 0;
    struct pt_regs *regs = current_pt_regs();
    struct {
        struct elfhdr elf_ex; /*记录当前可执行程序的elf header*/
        struct elfhdr interp_elf_ex; /*记录程序解析器的elf header*/
    } *loc;

    loc = kmalloc(sizeof(*loc), GFP_KERNEL);
    if (!loc) {
        retval = -ENOMEM;
        goto out_ret;
    }

    /* Get the exec-header */
    loc-&amp;gt;elf_ex = *((struct elfhdr *)bprm-&amp;gt;buf); /*当前可执行程序头部128字节已经读到buff中*/

    retval = -ENOEXEC;
    /* First of all, some simple consistency checks */
    if (memcmp(loc-&amp;gt;elf_ex.e_ident, ELFMAG, SELFMAG) != 0) /*较验头部魔术字*/
        goto out;

    if (loc-&amp;gt;elf_ex.e_type != ET_EXEC &amp;amp;&amp;amp; loc-&amp;gt;elf_ex.e_type != ET_DYN) /*校验可执行文件类型*/
        goto out;
    ...
    if (!bprm-&amp;gt;file-&amp;gt;f_op || !bprm-&amp;gt;file-&amp;gt;f_op-&amp;gt;mmap)
        goto out;

    /* Now read in all of the header information */
    if (loc-&amp;gt;elf_ex.e_phentsize != sizeof(struct elf_phdr)) /*校验program header大小*/
        goto out;
    if (loc-&amp;gt;elf_ex.e_phnum &amp;lt; 1 ||
            loc-&amp;gt;elf_ex.e_phnum &amp;gt; 65536U / sizeof(struct elf_phdr)) /*校验program header 个数*/
        goto out;
    size = loc-&amp;gt;elf_ex.e_phnum * sizeof(struct elf_phdr);
    retval = -ENOMEM;
    elf_phdata = kmalloc(size, GFP_KERNEL);
    if (!elf_phdata)
        goto out;

    /*根据e_phoff记录的文件偏移读取program header table*/
    retval = kernel_read(bprm-&amp;gt;file, loc-&amp;gt;elf_ex.e_phoff,
            (char *)elf_phdata, size);
    if (retval != size) {
        if (retval &amp;gt;= 0)
            retval = -EIO;
        goto out_free_ph;
    }

    elf_ppnt = elf_phdata;
    elf_bss = 0;
    elf_brk = 0;

    start_code = ~0UL;
    end_code = 0;
    start_data = 0;
    end_data = 0;

    /*从program header table中找出INTERP段获取程序解析器的访问路径并加载其头部*/
    for (i = 0; i &amp;lt; loc-&amp;gt;elf_ex.e_phnum; i++) {
        if (elf_ppnt-&amp;gt;p_type == PT_INTERP) {
            ...
            elf_interpreter = kmalloc(elf_ppnt-&amp;gt;p_filesz, GFP_KERNEL);
            ...
            retval = kernel_read(bprm-&amp;gt;file, elf_ppnt-&amp;gt;p_offset, elf_interpreter,
                elf_ppnt-&amp;gt;p_filesz);
            ...
            interpreter = open_exec(elf_interpreter);
            ...
            retval = kernel_read(interpreter, 0, bprm-&amp;gt;buf, BINPRM_BUF_SIZE);

            loc-&amp;gt;interp_elf_ex = *((struct elfhdr *)bprm-&amp;gt;buf);
            break;
        }
        elf_ppnt++;
    }

    ...
    /* Some simple consistency checks for the interpreter */
    if (elf_interpreter) {
        ...
    }

    /* Flush all traces of the currently running executable */
    retval = flush_old_exec(bprm); /*替换当前进程的mm_struct*/
    if (retval)
        goto out_free_dentry;

    ...

    setup_new_exec(bprm);

    ...
    retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack);
    if (retval &amp;lt; 0) {
        send_sig(SIGKILL, current, 0);
        goto out_free_dentry;
    }

    current-&amp;gt;mm-&amp;gt;start_stack = bprm-&amp;gt;p;

    /* Now we do a little grungy work by mmapping the ELF image into
       the correct location in memory. */
    for(i = 0, elf_ppnt = elf_phdata; i &amp;lt; loc-&amp;gt;elf_ex.e_phnum; i++, elf_ppnt++) {
        int elf_prot = 0, elf_flags;
        unsigned long k, vaddr;
        unsigned long total_size = 0;

        if (elf_ppnt-&amp;gt;p_type != PT_LOAD) /*只处理LOAD段*/
            continue;

        ...
        /*根据段的读写属性设置elf_prot*/
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_R)
            elf_prot |= PROT_READ;
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_W)
            elf_prot |= PROT_WRITE;
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_X)
            elf_prot |= PROT_EXEC;

        elf_flags = MAP_PRIVATE | MAP_DENYWRITE | MAP_EXECUTABLE;

        vaddr = elf_ppnt-&amp;gt;p_vaddr; /*段映射的进程虚拟地址，例如示例程的代码段映射到0x400000*/
        if (loc-&amp;gt;elf_ex.e_type == ET_EXEC || load_addr_set) {
            elf_flags |= MAP_FIXED;
        } else if (loc-&amp;gt;elf_ex.e_type == ET_DYN) {
            ...
        }

        /*将LOAD段映射到进程的vaddr处*/
        error = elf_map(bprm-&amp;gt;file, load_bias + vaddr, elf_ppnt,
                elf_prot, elf_flags, total_size);
        ...

        if (!load_addr_set) {
            load_addr_set = 1;
            load_addr = (elf_ppnt-&amp;gt;p_vaddr - elf_ppnt-&amp;gt;p_offset); /*段起始映射位置*/
            if (loc-&amp;gt;elf_ex.e_type == ET_DYN) {
                ...
            }
        }
        /*下面这段代码用来计算各段的加载位置，针对示例程序：
           start_code = 0x400000;
           end_code = 0x40764;
           start_data = 0x600e10;
           end_data = 0x601040;
           elf_bss = 0x601040;
           elf_brk = 0x601048; */
        k = elf_ppnt-&amp;gt;p_vaddr;
        if (k &amp;lt; start_code)
            start_code = k;
        if (start_data &amp;lt; k)
            start_data = k;

        ...
        k = elf_ppnt-&amp;gt;p_vaddr + elf_ppnt-&amp;gt;p_filesz;

        if (k &amp;gt; elf_bss)
            elf_bss = k;
        if ((elf_ppnt-&amp;gt;p_flags &amp;amp; PF_X) &amp;amp;&amp;amp; end_code &amp;lt; k)
            end_code = k;
        if (end_data &amp;lt; k)
            end_data = k;
        k = elf_ppnt-&amp;gt;p_vaddr + elf_ppnt-&amp;gt;p_memsz;
        if (k &amp;gt; elf_brk)
            elf_brk = k;
    }

    loc-&amp;gt;elf_ex.e_entry += load_bias;
    elf_bss += load_bias;
    elf_brk += load_bias;
    start_code += load_bias;
    end_code += load_bias;
    start_data += load_bias;
    end_data += load_bias;

    /* Calling set_brk effectively mmaps the pages that we need
     * for the bss and break sections.  We must do this before
     * mapping in the interpreter, to make sure it doesn't wind
     * up getting placed where the bss needs to go.
     */
    retval = set_brk(elf_bss, elf_brk); /*为bss映射匿名页并清零*/
    ...

    if (elf_interpreter) {
        unsigned long interp_map_addr = 0;
        
        /*elf_entry记录了execve调用返回后将执行的函数入口，这里指向程序解析器linux-ld*/
        elf_entry = load_elf_interp(&amp;amp;loc-&amp;gt;interp_elf_ex, interpreter, 
                &amp;amp;interp_map_addr, load_bias);
        if (!IS_ERR((void *)elf_entry)) {
            interp_load_addr = elf_entry;
            elf_entry += loc-&amp;gt;interp_elf_ex.e_entry;
        }
        ...
    }

    kfree(elf_phdata);

    set_binfmt(&amp;amp;elf_format);

    /*在用户态栈中放入elf解析获得的相关信息，供用户态程序使用*/
    retval = create_elf_tables(bprm, &amp;amp;loc-&amp;gt;elf_ex, load_addr, interp_load_addr);
    ...
    /* N.B. passed_fileno might not be initialized? */
    current-&amp;gt;mm-&amp;gt;end_code = end_code;
    current-&amp;gt;mm-&amp;gt;start_code = start_code;
    current-&amp;gt;mm-&amp;gt;start_data = start_data;
    current-&amp;gt;mm-&amp;gt;end_data = end_data;
    current-&amp;gt;mm-&amp;gt;start_stack = bprm-&amp;gt;p;
    ...

    /*修改regs指向的栈寄存器，使得execve调用返回到用户空间时，入口函数为elf_entry，栈指向bprm-&amp;gt;p*/
    start_thread(regs, elf_entry, bprm-&amp;gt;p);
    retval = 0;
out:
    kfree(loc);
out_ret:
    return retval;

/* error cleanup */
out_free_dentry:
    allow_write_access(interpreter);
    if (interpreter)
        fput(interpreter);
out_free_interp:
    kfree(elf_interpreter);
out_free_ph:
    kfree(elf_phdata);
goto out;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，我们已经将进程生命周期中最基本的fork和exec分析完成，内容较多，建议大家多思考多理解，打好基础。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/进程替换/&quot;&gt;【计算子系统】进程管理之二：进程替换&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】五、Client内核RBD驱动分析－网络信使messenger</title>
        <description>&lt;h3 id=&quot;4-libcephko中messenger模块分析&quot;&gt;4. libceph.ko中messenger模块分析&lt;/h3&gt;

&lt;p&gt;  messenger模块(信使)是libceph中相对比较独立的部分，旨在为上层各种网络客户端(如mon_client、osd_client)提供稳定可靠、有序的网络服务。messenger构建在网络TCP协议之上，虽然TCP协议本身是面向连接且可靠的网络协议，但是TCP连接有可能断开(broken，非长连接情况下会发生；长连接存在底层网络故障排除后上层应用感知不及时的问题)。为解决TCP短连接不可靠的问题，messenger通过间隔性地重连方案(back off，当有消息要发送时)或者等待方案(stand by，当无消息要发送时)来解决。同时会给每个发送的消息带上唯一的序列号(seq)，用以区分是否为重复消息。&lt;/p&gt;

&lt;p&gt;  messenger模块内部架构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_6.jpg&quot; height=&quot;400&quot; width=&quot;700&quot; /&gt;  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;从发送流程来看(图中实线)，应用程序(如rbd命令)调用ceph_con_send进行消息发送；该函数会唤醒发送连接对应的工作任务；在工作任务进程中，通过try_write函数调用kernel_sendmsg访问底层网络协议栈，最终触发网卡发包；&lt;/li&gt;
    &lt;li&gt;从接收流程来看(图中虚线)，网卡收包后通过中断和协议栈回调唤醒工作任务；工作任务通过try_read调用kernel_recvmsg从网络协议栈中读取消息内容；最后调用连接初始化时指定的消息回调函数对消息进行处理。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;41-正常网络收发的处理&quot;&gt;&lt;strong&gt;4.1. 正常网络收发的处理&lt;/strong&gt;&lt;/h4&gt;

&lt;h5 id=&quot;411-socket连接阶段&quot;&gt;&lt;strong&gt;4.1.1 socket连接阶段&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  每当上层通过ceph_con_init创建一个ceph_connection对象(对底层TCP连接的封装，含有自身的状态变化)并调用ceph_con_open打开该连接后(连接状态变为PREOPEN)，就会在内核工作队列中添加一项新的任务，并对该任务进行一次调度，其入口函数为con_work，代码框架如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con); /*socket连接初始化阶段不进行任何工作*/
        ...
        ret = try_write(con); /*建立socket连接并设定socket回调函数*/
        ...
        break;
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  conection worker首次被调度时进入socket连接阶段，完成的主要工作是建立TCP socket连接，并将ceph_connection的状态置为CONNECTING：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static int try_write(struct ceph_connection *con)
{
    ...

    if (con-&amp;gt;state == CON_STATE_PREOPEN) { /*首次被调度时，连接状态为PREOPEN*/
        BUG_ON(con-&amp;gt;sock);
        con-&amp;gt;state = CON_STATE_CONNECTING; /*执行完下面这些动作后状态更新为CONNECTING*/

        con_out_kvec_reset(con); /*重置当前连接con的发送缓冲区，对应con-&amp;gt;out_kvec_* */
        prepare_write_banner(con); /*将客户端的banner内容放入发送缓冲区*/
        prepare_read_banner(con); /*准备接收服务端的banner内容*/

        BUG_ON(con-&amp;gt;in_msg);
        con-&amp;gt;in_tag = CEPH_MSGR_TAG_READY; /*重置消息接收状态为READY，表示可接收任意消息类别，不同类别由CEPH_MSGR_TAG_*进行区分 */
        ret = ceph_tcp_connect(con); /*创建并连接socket，同时指定协议栈的回调函数*/
        if (ret &amp;lt; 0) {
            con-&amp;gt;error_msg = &quot;connect error&quot;;
            goto out;
        }
    }

more_kvec:
    ...
    if (con-&amp;gt;out_kvec_left) { 
        /*调用底层kernel_sendmsg将当前连接con发送缓冲中的内容进行发送*/
        ret = write_partial_kvec(con);
        if (ret &amp;lt;= 0)
            goto out;
    }
    ...
}

static int ceph_tcp_connect(struct ceph_connection *con)
{
    struct sockaddr_storage *paddr = &amp;amp;con-&amp;gt;peer_addr.in_addr;
    struct socket *sock;
    int ret;

    BUG_ON(con-&amp;gt;sock);
    ret = sock_create_kern(con-&amp;gt;peer_addr.in_addr.ss_family, SOCK_STREAM,
        IPPROTO_TCP, &amp;amp;sock);
    ...
    sock-&amp;gt;sk-&amp;gt;sk_allocation = GFP_NOFS;

    set_sock_callbacks(sock, con);

    con_sock_state_connecting(con);
    ret = sock-&amp;gt;ops-&amp;gt;connect(sock, (struct sockaddr *)paddr, sizeof(*paddr),
        O_NONBLOCK);
    ...
    con-&amp;gt;sock = sock;
    return 0;
}

static void set_sock_callbacks(struct socket *sock,
    struct ceph_connection *con)
{
    struct sock *sk = sock-&amp;gt;sk;
    sk-&amp;gt;sk_user_data = con;
    sk-&amp;gt;sk_data_ready = ceph_sock_data_ready; /*协议栈收到包后会回调该函数*/
    sk-&amp;gt;sk_write_space = ceph_sock_write_space;
    sk-&amp;gt;sk_state_change = ceph_sock_state_change; /*协议栈改变socket状态时会回调该函数*/
}

static void ceph_sock_data_ready(struct sock *sk, int count_unused)
{
    struct ceph_connection *con = sk-&amp;gt;sk_user_data;

    ...
    queue_con(con); /*再次唤醒工作任务*/
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;412-协商阶段&quot;&gt;&lt;strong&gt;4.1.2 协商阶段&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  socket连接阶段最后，向服务端发送了客户端的banner，并等待服务端回复banner。当收到服务端回复banner后，connection worker再次被唤醒，此时主要完成客户端与服务端的信息协商：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con); /*读取服务端返回的banner并进入协商阶段*/
        ...
        ret = try_write(con); /*向服务端发送协商信息*/
        ...
        break;
    }
    ...
}

static int try_read(struct ceph_connection *con)
{
    ...
    if (con-&amp;gt;state == CON_STATE_CONNECTING) {
        ret = read_partial_banner(con); /*读取服务端的banner信息*/
        if (ret &amp;lt;= 0)
            goto out;
        ret = process_banner(con); /*对banner进行校验*/
        if (ret &amp;lt; 0)
            goto out;

        con-&amp;gt;state = CON_STATE_NEGOTIATING; /*校验通过后进入协商阶段*/

        /*
         * Received banner is good, exchange connection info.
         * Do not reset out_kvec, as sending our banner raced
         * with receiving peer banner after connect completed.
         */
        ret = prepare_write_connect(con); /*准备全局连接号等协商信息*/
        if (ret &amp;lt; 0)
            goto out;
        prepare_read_connect(con); /*准备接收服务端返回的协商信息*/

        /* Send connection info before awaiting response */
        goto out;
    }
    ...
｝
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;413-正常打开阶段&quot;&gt;&lt;strong&gt;4.1.3 正常打开阶段&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  收到服务端的协商消息后，connection worker再次被唤醒，进行服务端协商消息的处理并进入正常打开阶段可收发消息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con); /*读取服务端协商信息并进入正常打开阶段，可正常接收消息*/
        ...
        ret = try_write(con); /*可正常发送消息*/
        ...
        break;
    }
    ...
}

static int try_read(struct ceph_connection *con)
{
    ...
    if (con-&amp;gt;state == CON_STATE_NEGOTIATING) {
        ret = read_partial_connect(con); /*读取服务端协商信息到in_reply中*/
        if (ret &amp;lt;= 0)
            goto out;
        ret = process_connect(con); /*处理协商消息并进入正常打开阶段*/
        if (ret &amp;lt; 0)
            goto out;
        goto more;
    }
    ...
}

static int process_connect(struct ceph_connection *con)
{
    ...
    switch (con-&amp;gt;in_reply.tag) { /*in_reply中记录服务端返回的协商信息*/
        ...
    case CEPH_MSGR_TAG_SEQ:
    case CEPH_MSGR_TAG_READY:
        ...

        /*记录服务端返回的协商消息并将连接状态置为OPEN*/
    
        con-&amp;gt;state = CON_STATE_OPEN;
        con-&amp;gt;auth_retry = 0;    /* we authenticated; clear flag */
        con-&amp;gt;peer_global_seq = le32_to_cpu(con-&amp;gt;in_reply.global_seq);
        con-&amp;gt;connect_seq++;
        con-&amp;gt;peer_features = server_feat;
        ...
        con-&amp;gt;delay = 0;      /* reset backoff memory */

        if (con-&amp;gt;in_reply.tag == CEPH_MSGR_TAG_SEQ) {
            prepare_write_seq(con);
            prepare_read_seq(con);
        } else {
            prepare_read_tag(con);
        }
        break;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;414-消息收发过程&quot;&gt;&lt;strong&gt;4.1.4 消息收发过程&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  客户端与服务端进行正常消息收发时，总是先在网络连接上发送一个字节的tag，再发送实际的消息。原因是两者可以通过tag来明确消息的具体类别和解析格式。网络流上的数据大体如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_message.jpg&quot; height=&quot;100&quot; width=&quot;700&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们先来看看消息的接收：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static int try_read(struct ceph_connection *con)
{
    ...

    if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_READY) { /*READY代表准备好接收具体的tag内容*/
        /*
         * what's next?
         */
        ret = ceph_tcp_recvmsg(con-&amp;gt;sock, &amp;amp;con-&amp;gt;in_tag, 1); /*从网络中接收一个字节的tag*/
        if (ret &amp;lt;= 0)
            goto out;

        /*根据tag内容准备接收后续的消息*/
        switch (con-&amp;gt;in_tag) {
        case CEPH_MSGR_TAG_MSG:
            prepare_read_message(con);
            break;
        case CEPH_MSGR_TAG_ACK:
            prepare_read_ack(con);
            break;
        case CEPH_MSGR_TAG_CLOSE:
            con_close_socket(con);
            con-&amp;gt;state = CON_STATE_CLOSED;
            goto out;
        default:
            goto bad_tag;
        }
    }

    /*如果tag为MSG，则表示后续为一个实际的message消息，开始接收消息内容并调用回调函数*/
    if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_MSG) {
        ret = read_partial_message(con); /*注意，这里通过回调alloc_msg找到ceph_osd_request中分配好的r_reply*/
        ...
        if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_READY)
            goto more;
        process_message(con);
        if (con-&amp;gt;state == CON_STATE_OPEN)
            prepare_read_tag(con);
        goto more;
    }

    /*如果tag为ACK或SEQ，则表示后续为一个服务端返回的确认号，可以释放已确认的发送消息*/
    if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_ACK ||
        con-&amp;gt;in_tag == CEPH_MSGR_TAG_SEQ) {
        /*
         * the final handshake seq exchange is semantically
         * equivalent to an ACK
         */
        ret = read_partial_ack(con);
        ...
        process_ack(con);
        goto more;
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接下来再来看看消息的发送：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static int try_write(struct ceph_connection *con)
{
more:
    ...
more_kvec:    
    if (con-&amp;gt;out_kvec_left) { /*将发送缓冲区中的内容通过网络进行发送*/
        ret = write_partial_kvec(con);
        if (ret &amp;lt;= 0)
            goto out;
    }

    /* msg pages? */
    if (con-&amp;gt;out_msg) { /*如果已选定当前发送消息out_msg且还未完成发送，则发送消息包含的数据*/
        if (con-&amp;gt;out_msg_done) {
            ceph_msg_put(con-&amp;gt;out_msg);
            con-&amp;gt;out_msg = NULL;   /* we're done with this one */
            goto do_next;
        }

        ret = write_partial_message_data(con);
        if (ret == 1)
            goto more_kvec;  /* we need to send the footer, too! */
        if (ret == 0)
            goto out;
        if (ret &amp;lt; 0) {
            dout(&quot;try_write write_partial_message_data err %d\n&quot;, ret);
            goto out;
        }
    }

do_next:
    if (con-&amp;gt;state == CON_STATE_OPEN) {
        /* is anything else pending? */
        if (!list_empty(&amp;amp;con-&amp;gt;out_queue)) {
            /*如果发送队列out_queue中有待发送的消息，则取出一个消息放到out_msg中并发送其头部内容(消息中包含的数据在前面代码中发送)*/
            prepare_write_message(con);
            goto more;
        }
        if (con-&amp;gt;in_seq &amp;gt; con-&amp;gt;in_seq_acked) {
            /*如果当前收到消息的序列号大于已经确认的序列号，则准备发送新的确认号*/
            prepare_write_ack(con);
            goto more;
        }
        ...
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  最后我们来看看ceph_msg的结构定义，发送者负责将请求内容填入ceph_msg以供网络层发送；网络层接收消息后也先填入ceph_msg再交由上层应用处理响应：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/messenger.h:

struct ceph_msg {
    struct ceph_msg_header hdr;	/* header */
    struct ceph_msg_footer footer;	/* footer */
    struct kvec front;              /* unaligned blobs of message */
    struct ceph_buffer *middle;

    size_t				data_length;
    struct list_head		data;
    struct ceph_msg_data_cursor	cursor;

    struct ceph_connection *con;
    struct list_head list_head;	/* links for connection lists */

    struct kref kref;
    bool front_is_vmalloc;
    bool more_to_follow;
    bool needs_out_seq;
    int front_alloc_len;
    unsigned long ack_stamp;        /* tx: when we were acked */

    struct ceph_msgpool *pool;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/msgr.h:

struct ceph_msg_header {
    __le64 seq;       /* message seq# for this session */
    __le64 tid;       /* transaction id */
    __le16 type;      /* message type */
    __le16 priority;  /* priority.  higher value == higher priority */
    __le16 version;   /* version of message encoding */

    __le32 front_len; /* bytes in main payload */
    __le32 middle_len;/* bytes in middle payload */
    __le32 data_len;  /* bytes of data payload */
    __le16 data_off;  /* sender: include full offset;
                        receiver: mask against ~PAGE_MASK */

    struct ceph_entity_name src;
    __le32 reserved;
    __le32 crc;       /* header crc32c */
} __attribute__ ((packed));
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/ceph_fs.h:

/*
 * message types
 */

/* misc */
#define CEPH_MSG_SHUTDOWN               1
#define CEPH_MSG_PING                   2

/* client &amp;lt;-&amp;gt; monitor */
#define CEPH_MSG_MON_MAP                4
#define CEPH_MSG_MON_GET_MAP            5
#define CEPH_MSG_STATFS                 13
#define CEPH_MSG_STATFS_REPLY           14
#define CEPH_MSG_MON_SUBSCRIBE          15
#define CEPH_MSG_MON_SUBSCRIBE_ACK      16
#define CEPH_MSG_AUTH			17
#define CEPH_MSG_AUTH_REPLY		18
#define CEPH_MSG_MON_GET_VERSION        19
#define CEPH_MSG_MON_GET_VERSION_REPLY  20

/* client &amp;lt;-&amp;gt; mds */
#define CEPH_MSG_MDS_MAP                21

#define CEPH_MSG_CLIENT_SESSION         22
#define CEPH_MSG_CLIENT_RECONNECT       23

#define CEPH_MSG_CLIENT_REQUEST         24
#define CEPH_MSG_CLIENT_REQUEST_FORWARD 25
#define CEPH_MSG_CLIENT_REPLY           26
#define CEPH_MSG_CLIENT_CAPS            0x310
#define CEPH_MSG_CLIENT_LEASE           0x311
#define CEPH_MSG_CLIENT_SNAP            0x312
#define CEPH_MSG_CLIENT_CAPRELEASE      0x313

/* pool ops */
#define CEPH_MSG_POOLOP_REPLY           48
#define CEPH_MSG_POOLOP                 49


/* osd */
#define CEPH_MSG_OSD_MAP                41
#define CEPH_MSG_OSD_OP                 42
#define CEPH_MSG_OSD_OPREPLY            43
#define CEPH_MSG_WATCH_NOTIFY           44
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;42-tcp连接故障后的处理&quot;&gt;&lt;strong&gt;4.2. TCP连接故障后的处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  messenger模块在两种情况下可以获知TCP连接故障：一种是底层网络协议栈通知socket状态改变；另外一种是在通过socket进行网络收发包时，返回错误信息。&lt;/p&gt;

&lt;h5 id=&quot;421-网络协议栈通过回调上报故障&quot;&gt;&lt;strong&gt;4.2.1 网络协议栈通过回调上报故障&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  在第一种情况下，网络协议栈在发现TCP连接故障后，通过调用回调ceph_sock_state_change函数通知messenger底层socket处于关闭状态。ceph_sock_state_change函数在更新socket状态后会重新调度connection work：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void ceph_sock_state_change(struct sock *sk)
{
    struct ceph_connection *con = sk-&amp;gt;sk_user_data;

    switch (sk-&amp;gt;sk_state) {
    case TCP_CLOSE: /*发现底层socket已关闭*/
        dout(&quot;%s TCP_CLOSE\n&quot;, __func__);
    case TCP_CLOSE_WAIT:
        dout(&quot;%s TCP_CLOSE_WAIT\n&quot;, __func__);
        con_sock_state_closing(con);
        con_flag_set(con, CON_FLAG_SOCK_CLOSED); /*将连接状态置为关闭*/
        queue_con(con); /*重新唤醒工作任务*/
        break;
    case TCP_ESTABLISHED:
        ...
        break;
    default:	/* Everything else is uninteresting */
        break;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  工作任务被调度后，发现连接状态被关闭，进入故障处理流程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        if ((fault = con_sock_closed(con))) {
            /*工作任务被调度时首先判断连接是否处于关闭状态，如果已关则跳转出循环进行故障处理*/
            dout(&quot;%s: con %p SOCK_CLOSED\n&quot;, __func__, con);
            break;
        }
        ...
        ret = try_read(con); 
        ...
        ret = try_write(con); 
        ...
        break;
    }

    /*下面是故障处理逻辑*/
    if (fault)
        con_fault(con);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;422-通过网络收发函数返回结果获知连接故障&quot;&gt;&lt;strong&gt;4.2.2 通过网络收发函数返回结果获知连接故障&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  每个连接的工作任务在收发过程中，如果底层函数返回错误，也可获知网络连接故障：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con);
        if (ret &amp;lt; 0) { /*收包时底层返回错误*/
            if (ret == -EAGAIN)
                continue;
            con-&amp;gt;error_msg = &quot;socket error on read&quot;;
            fault = true;
            break;
        }
        
        ret = try_write(con); 
        if (ret &amp;lt; 0) { /*发包时底层返回错误*/
            if (ret == -EAGAIN)
                continue;
            con-&amp;gt;error_msg = &quot;socket error on write&quot;;
            fault = true;
        }

        break;
    }

    /*下面是故障处理逻辑*/
    if (fault)
        con_fault(con);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;423-socket连接故障处理逻辑&quot;&gt;&lt;strong&gt;4.2.3 socket连接故障处理逻辑&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  连接故障的处理有两种方式：一种是back off，即延时重连；别一种是stand by，即等待有新消息时再重试：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
static void con_fault(struct ceph_connection *con)
{
    ...
    con_close_socket(con); /*关闭底层socket*/

    ...
    if (con-&amp;gt;in_msg) {
        /*如果有新接收到消息，则释放该消息*/
        BUG_ON(con-&amp;gt;in_msg-&amp;gt;con != con);
        con-&amp;gt;in_msg-&amp;gt;con = NULL;
        ceph_msg_put(con-&amp;gt;in_msg);
        con-&amp;gt;in_msg = NULL;
        con-&amp;gt;ops-&amp;gt;put(con);
    }

    /*对于已经发送但还未收到对方确认的消息，我们需要在网络重连后对它们进行重发*/
    /* Requeue anything that hasn't been acked */
    list_splice_init(&amp;amp;con-&amp;gt;out_sent, &amp;amp;con-&amp;gt;out_queue);

    /* If there are no messages queued or keepalive pending, place
     * the connection in a STANDBY state */
    if (list_empty(&amp;amp;con-&amp;gt;out_queue) &amp;amp;&amp;amp;
            !con_flag_test(con, CON_FLAG_KEEPALIVE_PENDING)) {

        /*如果发送队列为空且不需要发送心跳消息时，将当前连接置为stand by状态*/

        dout(&quot;fault %p setting STANDBY clearing WRITE_PENDING\n&quot;, con);
        con_flag_clear(con, CON_FLAG_WRITE_PENDING);
        con-&amp;gt;state = CON_STATE_STANDBY;
    } else {
        
        /*如果还有待发送的消息，那么偿试等待一段时间后重连；等待时间每次翻倍*/

        /* retry after a delay. */
        con-&amp;gt;state = CON_STATE_PREOPEN;
        if (con-&amp;gt;delay == 0)
            con-&amp;gt;delay = BASE_DELAY_INTERVAL;
        else if (con-&amp;gt;delay &amp;lt; MAX_DELAY_INTERVAL)
            con-&amp;gt;delay *= 2;
        con_flag_set(con, CON_FLAG_BACKOFF);
        queue_con(con);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  当连接处理stand by状态时，如果有新的消息通过ceph_con_send发送，其内部会清除stand by状态并重置为PREOPEN以偿试进行重连。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/RBD-client-4/&quot;&gt;【Rados Block Device】五、Client内核RBD驱动分析－网络信使messenger&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/RBD-client-4/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/RBD-client-4/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
  </channel>
</rss>
