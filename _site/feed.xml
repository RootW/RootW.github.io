<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>吴斌</title>
    <description>欢迎来到我的技术博客~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 21 May 2018 11:21:43 +0800</pubDate>
    <lastBuildDate>Mon, 21 May 2018 11:21:43 +0800</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>【SPDK】四、reactor线程</title>
        <description>&lt;p&gt;  reactor线程是SPDK中负责实际业务处理逻辑的单元，它们在vhsot服务启动时创建，直到服务停止。目前还不支持reactor线程的动态增减。&lt;/p&gt;

&lt;h3 id=&quot;reactor线程总流程&quot;&gt;reactor线程总流程&lt;/h3&gt;

&lt;p&gt;  我们顺着vhost进程的代码执行顺序来看看总体流程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/app/vhost/vhost.c:

int
main(int argc, char *argv[])
{
    struct spdk_app_opts opts = {};
    int rc;

    /* 首先进行参数解析，解析后的结果保存于opts中 */

    vhost_app_opts_init(&amp;amp;opts);

    if ((rc = spdk_app_parse_args(argc, argv, &amp;amp;opts, &quot;f:S:&quot;,
        vhost_parse_arg, vhost_usage)) !=
        SPDK_APP_PARSE_ARGS_SUCCESS) {
        exit(rc);
    }

    ...

    /* 接着根据配置文件指明的物理核启动reactors线程(主线程最终也成为一个reactor)。
        这些reactors线程会执行轮循函数，直到外部将服务状态置为退出 */

    /* Blocks until the application is exiting */
    rc = spdk_app_start(&amp;amp;opts, vhost_started, NULL, NULL);

    /* 所有reactor线程退出后，进行资源清理 */
    spdk_app_fini();

    return rc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  上述整体流程中最为重要的便是spdk_app_start函数，该函数内部调用了DPDK关于系统CPU、内存、PCI设备管理等通用性服务代码，这里我们尽可能以理解其功能为主而不做深入的代码分析：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/event/app.c:

int
spdk_app_start(struct spdk_app_opts *opts, spdk_event_fn start_fn,
void *arg1, void *arg2)
{
    struct spdk_conf	*config = NULL;
    int			rc;
    struct spdk_event	*app_start_event;

    ...

    /* 将配置文件中的内容导入到config对象中 */
    config = spdk_app_setup_conf(opts-&amp;gt;config_file);
    ...
    spdk_app_read_config_file_global_params(opts);

    ...

    /* 调用DPDK系统服务：
        (1)通过内核sysfs获取物理CPU信息，并通过配置文件指定的运行核，在各个核上启动服务线程；
        各服务线程启动后因为在等待主线程给它们发送需要执行的任务而处于睡眠状态；
        (2)基于大页内存创建内存池以供其它模块使用；
        (3)初始化PCI设备枚举服务，可以实现类似内核的设备发现及驱动初始化流程。SPDK基于此并借
        助内核uio或vfio驱动实现全用户态的PCI驱动 */
     /* 完成DPDK的初始化后，SPDK会建立一张由vva(vhost virtual address)到pa(physical address)
        的内存映射表g_vtophys_map。每当有新的内存映射到vhost中时，都需要调用spdk_mem_register在该
        表中注册新的映射关系。设计该表的原因是当SPDK向物理设备发送DMA请求时，需要向设备提供pa而非vva */
    if (spdk_app_setup_env(opts) &amp;lt; 0) {
        ...
    }

    /* 这里为reactors分配相应的内存 */
    /*
     * If mask not specified on command line or in configuration file,
     *  reactor_mask will be 0x1 which will enable core 0 to run one
     *  reactor.
     */
    if ((rc = spdk_reactors_init(opts-&amp;gt;max_delay_us)) != 0) {
        ...
    }

    ...

    /* 设置一些全局变量 */
    memset(&amp;amp;g_spdk_app, 0, sizeof(g_spdk_app));
    g_spdk_app.config = config;
    g_spdk_app.shm_id = opts-&amp;gt;shm_id;
    g_spdk_app.shutdown_cb = opts-&amp;gt;shutdown_cb;
    g_spdk_app.rc = 0;
    g_init_lcore = spdk_env_get_current_core();
    g_app_start_fn = start_fn;
    g_app_start_arg1 = arg1;
    g_app_start_arg2 = arg2;
    app_start_event = spdk_event_allocate(g_init_lcore, start_rpc, (void *)opts-&amp;gt;rpc_addr, NULL);

    /* 初始化SPDK的各个子系统，如bdev、vhost均为子系统。但这里需注意一点，此处仅是产生了一个初始化事件，事件的处理要在
        reactor线程正式进入轮循函数后才开始 */
    spdk_subsystem_init(app_start_event);

    /* 从此处开始，各个线程(包括主线程)开始执行_spdk_reactor_run，线程名也正式变更为reactor_X；
        直到所有线程均退出_spdk_reactor_run后，主线程才会返回 */
    /* This blocks until spdk_app_stop is called */
    spdk_reactors_start();

    return g_spdk_app.rc;
    ...    
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  再看一下spdk_reactors_start：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/event/reactor.c:

void
spdk_reactors_start(void)
{
    struct spdk_reactor *reactor;
    uint32_t i, current_core;
    int rc;

    g_reactor_state = SPDK_REACTOR_STATE_RUNNING;
    g_spdk_app_core_mask = spdk_cpuset_alloc();

    /* 针对主线程之外的其它核上的线程，通过发送通知使它们开始执行_spdk_reactor_run */
    current_core = spdk_env_get_current_core();
    SPDK_ENV_FOREACH_CORE(i) {
        if (i != current_core) {
            reactor = spdk_reactor_get(i);
            rc = spdk_env_thread_launch_pinned(reactor-&amp;gt;lcore, _spdk_reactor_run, reactor);
            ...
        }
        spdk_cpuset_set_cpu(g_spdk_app_core_mask, i, true);
    }

    /* 主线程也会执行_spdk_reactor_run */
    /* Start the master reactor */
    reactor = spdk_reactor_get(current_core);
    _spdk_reactor_run(reactor);

    /* 主线程退出后会等待其它核上的线程均退出 */
    spdk_env_thread_wait_all();

    /* 执行到此处，说明vhost服务进程即将退出 */
    g_reactor_state = SPDK_REACTOR_STATE_SHUTDOWN;
    spdk_cpuset_free(g_spdk_app_core_mask);
    g_spdk_app_core_mask = NULL;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;轮循函数_spdk_reactor_run&quot;&gt;轮循函数_spdk_reactor_run&lt;/h3&gt;

&lt;p&gt;  通过对vhost代码流程的分析，我们看到vhost中所有线程最终都会调用_spdk_reactor_run，该函数是一个死循环，由此实现轮循逻辑：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/event/reactor.c:

static int
_spdk_reactor_run(void *arg)
{
    struct spdk_reactor	*reactor = arg;
    struct spdk_poller	*poller;
    uint32_t		event_count;
    uint64_t		idle_started, now;
    uint64_t		spin_cycles, sleep_cycles;
    uint32_t		sleep_us;
    uint32_t		timer_poll_count;
    char			thread_name[32];

    /* 重新命名线程名，reactor_[核号] */
    snprintf(thread_name, sizeof(thread_name), &quot;reactor_%u&quot;, reactor-&amp;gt;lcore);

    /* 创建SPDK线程对象：
        (1)线程间通过_spdk_reactor_send_msg发送消息，本质是向接收方的event队列中添加事件；
        (2)线程通过_spdk_reactor_start_poller和_spdk_reactor_stop_poller启动和停止poller；
        (3)IO Channel等线程相关对象也会记录到线程对象中 */
    if (spdk_allocate_thread(_spdk_reactor_send_msg,
            _spdk_reactor_start_poller,
            _spdk_reactor_stop_poller,
            reactor, thread_name) == NULL) {
        return -1;
    }
    
    /* spin_cycles代表最短轮循时间 */
    spin_cycles = SPDK_REACTOR_SPIN_TIME_USEC * spdk_get_ticks_hz() / SPDK_SEC_TO_USEC;
    /* sleep_cycles代表最长睡眠时间 */
    sleep_cycles = reactor-&amp;gt;max_delay_us * spdk_get_ticks_hz() / SPDK_SEC_TO_USEC;
    idle_started = 0;
    timer_poll_count = 0;

    /* 轮循的死循环正式开始 */
    while (1) {
        bool took_action = false;

        /* 首先，每个reactor线程通过DPDK的无锁队列实现了一个事件队列；这里从事件队列中取出事件并调用事件
            的处理函数。例如，vhost的子系统的初始化即是在spdk_subsystem_init中产生了一个verify事件并
            添加到主线程reactor的事件队列中，该事件处理函数为spdk_subsystem_verify */
        event_count = _spdk_event_queue_run_batch(reactor);
        if (event_count &amp;gt; 0) {
            took_action = true;
        }

        /* 接着，每个reactor线程从active_pollers链表头部取出一个poller并调用其fn函数。poller代表一次
            具体的处理动作，例如处理某个vhost_blk设备的所有IO环中的请求，又或者处理后端NVMe某个queue 
            pair中的所有响应 */
        poller = TAILQ_FIRST(&amp;amp;reactor-&amp;gt;active_pollers);
        if (poller) {
            TAILQ_REMOVE(&amp;amp;reactor-&amp;gt;active_pollers, poller, tailq);
            poller-&amp;gt;state = SPDK_POLLER_STATE_RUNNING;
            poller-&amp;gt;fn(poller-&amp;gt;arg);
            if (poller-&amp;gt;state == SPDK_POLLER_STATE_UNREGISTERED) {
                free(poller);
            } else {
                poller-&amp;gt;state = SPDK_POLLER_STATE_WAITING;
                TAILQ_INSERT_TAIL(&amp;amp;reactor-&amp;gt;active_pollers, poller, tailq);
            }
            took_action = true;
        }

        /* 最后，reactor线程还实现了定时器逻辑，这里判断是否有定时器到期；如果确有定时器到期则执行其回调并将
            其放到定时器队列尾部 */
        if (timer_poll_count &amp;gt;= SPDK_TIMER_POLL_ITERATIONS) {
            poller = TAILQ_FIRST(&amp;amp;reactor-&amp;gt;timer_pollers);
            if (poller) {
                now = spdk_get_ticks();

                if (now &amp;gt;= poller-&amp;gt;next_run_tick) {
                    TAILQ_REMOVE(&amp;amp;reactor-&amp;gt;timer_pollers, poller, tailq);
                    poller-&amp;gt;state = SPDK_POLLER_STATE_RUNNING;
                    poller-&amp;gt;fn(poller-&amp;gt;arg);
                    if (poller-&amp;gt;state == SPDK_POLLER_STATE_UNREGISTERED) {
                        free(poller);
                    } else {
                        poller-&amp;gt;state = SPDK_POLLER_STATE_WAITING;
                        _spdk_poller_insert_timer(reactor, poller, now);
                    }
                    took_action = true;
                }
            }
            timer_poll_count = 0;
        } else {
            timer_poll_count++;
        }

        /* 下面的逻辑主要用来决定轮循线程是否可以睡眠一会 */

        if (took_action) {
            /* We were busy this loop iteration. Reset the idle timer. */
            idle_started = 0;
        } else if (idle_started == 0) {
            /* We were previously busy, but this loop we took no actions. */
            idle_started = spdk_get_ticks();
        }

        /* Determine if the thread can sleep */
        if (sleep_cycles &amp;amp;&amp;amp; idle_started) {
            now = spdk_get_ticks();
            if (now &amp;gt;= (idle_started + spin_cycles)) { /* 保证轮循线程最少已执行了spin_cycles */
                sleep_us = reactor-&amp;gt;max_delay_us;

                poller = TAILQ_FIRST(&amp;amp;reactor-&amp;gt;timer_pollers);
                if (poller) {
                    /* There are timers registered, so don't sleep beyond
                     * when the next timer should fire */
                    if (poller-&amp;gt;next_run_tick &amp;lt; (now + sleep_cycles)) {
                        if (poller-&amp;gt;next_run_tick &amp;lt;= now) {
                            sleep_us = 0;
                        } else {
                            sleep_us = ((poller-&amp;gt;next_run_tick - now) *
                                SPDK_SEC_TO_USEC) / spdk_get_ticks_hz();
                        }
                    }
                }

                if (sleep_us &amp;gt; 0) {
                    usleep(sleep_us);
                }

                /* After sleeping, always poll for timers */
                timer_poll_count = SPDK_TIMER_POLL_ITERATIONS;
            }
        }

        if (g_reactor_state != SPDK_REACTOR_STATE_RUNNING) {
            break;
        }
    } /* 死循环结束 */

    ...
    spdk_free_thread();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，reactor线程整体执行逻辑已分析完成，后续我们将以verify_event为线索开始分析各个子系统的初始化过程。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-reactors-init/&quot;&gt;【SPDK】四、reactor线程&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 21 May 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/SPDK-reactors-init/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/SPDK-reactors-init/</guid>
        
        <category>SPDK</category>
        
        
      </item>
    
      <item>
        <title>【SPDK】三、IO流程代码解析</title>
        <description>&lt;p&gt;  在分析SPDK数据面代码之前，需要我们对qemu中实现的IO环以及virtio前后端驱动的实现有所了解(后续我计划出专门的博文来介绍qemu)。这里我们仍以SPDK前端配置vhost-blk，后端对接NVMe SSD为例(有关NVMe驱动涉及较多规范细节，这里也不作过于深入的讨论，感兴趣的读者可以结合NVMe规范展开阅读)进行分析。&lt;/p&gt;

&lt;h3 id=&quot;总流程&quot;&gt;总流程&lt;/h3&gt;

&lt;p&gt;  前文在分析SPDK IO栈时已经大致分析了IO处理的调用层次，在此我们进一步打开内部实现细节，更细致地分析一下IO处理流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/spdk/ioanalyze.jpg&quot; height=&quot;600&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  首先，从虚拟机视角来说，它看到的是一个virtio-blk-pci设备，该pci设备内部包含一条virtio总线，其上又连接了virtio-blk设备。qemu在对虚拟机用户呈现这个virtio-blk-pci设备时，采用的具体设备类型是vhost-user-blk-pci(这是virtio-blk-pci设备的一种后端实现方式。另外两种是：vhost-blk-pci，由内核实现后端；普通virtio-blk-pci，由qemu实现后端处理)，这样便可与用户态的SPDK vhost进程建立连接。SPDK vhost进程内部对于虚拟机所见的virtio-blk-pci设备也有一个对象来表示它，这就是spdk_vhost_blk_dev。该对象指向一个bdev对象和一个io channel对象，bdev对象代表真正的后端块存储(这里对应NVMe SSD上的一个namespace)，io channel代表当前线程访问存储的独立通道(对应NVMe SSD的一个Queue Pair)。这两个对象在驱动层会进一步扩展新的成员变量，用来表示驱动层可见的一些详细信息。&lt;/p&gt;

&lt;p&gt;  其次，当虚拟机往IO环中放入IO请求后，便立刻被vhost进程中的某个reactor线程轮循到该请求(轮循过种中执行函数为vdev_worker)。reactor线程取出请求后，会将其映成一个任务(spdk_vhost_blk_task)。对于读写请求，会进一步走到bdev层，将任务封状成一个bdev_io对象(类似内核的bio)。bdev_io继续往驱动层递交，它会扩展为适配具体驱动的io对象，例如针对NVMe驱动，bdev_io将扩展成nvme_bdev_io对象。NVMe驱动会根据nvme_bdev_io对象中的请求内容在当前reactor线程对应的QueuePair中生成一个新的请求项，并通知NVMe控制器有新的请求产生。&lt;/p&gt;

&lt;p&gt;  最后，当物理NVMe控制器完成IO请求后，会往QueuePair中添加IO响应。该响应信息也会很快被reactor线程轮循到(轮循执行函数为bdev_nvme_poll)。reactor取出响应后，根据其id找到对应的nvme_bdev_io，进一步关联到对应的bdev_io，再调用bdev_io中的记录的回调函数。vhost-blk下发请求时注册的回调函数为blk_request_complete_cb，回调参数为当前的spdk_vhost_blk_task对象。在blk_request_complete_cb中会往虚拟机IO环中放入IO响应，并通过虚拟中断通知虚拟机IO完成。&lt;/p&gt;

&lt;h3 id=&quot;io请求下发流程代码解析&quot;&gt;IO请求下发流程代码解析&lt;/h3&gt;

&lt;p&gt;  vhost进程通过vdev_worker函数以轮循方式处理虚拟机下发的IO请求，调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vdev_worker()
    \-process_vq()
        |-spdk_vhost_vq_avail_ring_get()
        \-process_blk_request()
            |-blk_iovs_setup()
            \-spdk_bdev_readv()/spdk_bdev_writev()
                \-spdk_bdev_io_submit()
                    \-bdev-&amp;gt;fn_table-&amp;gt;submit_request()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  下面我们先来分析一下vhost-blk层的具体代码实现：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/vhost/vhost-blk.c:

/* reactor线程会采用轮循方式周期性地调用vdev_worker函数来处理虚拟机下发的请求 */
static int
vdev_worker(void *arg)
{
    /* arg在注册轮循函数时指定，代表当前操作的vhost-blk对象 */
    struct spdk_vhost_blk_dev *bvdev = arg; 
    uint16_t q_idx;

    /* vhost-blk对象bvdev中含有一个抽象的spdk_vhost_dev对象，其内部记录所有vhost_dev类别对象
        均含有的公共内容，max_queues代表当前vhost_dev对象共有多少个IO环，virtqueue[]数组记录了
        所有的IO环信息 */
    for (q_idx = 0; q_idx &amp;lt; bvdev-&amp;gt;vdev.max_queues; q_idx++) {
        /* 根据IO环的个数，依次处理每个环中的请求 */
        process_vq(bvdev, &amp;amp;bvdev-&amp;gt;vdev.virtqueue[q_idx]);
    }

    ...

}

/* 处理IO环中的所有请求 */
static void
process_vq(struct spdk_vhost_blk_dev *bvdev, struct spdk_vhost_virtqueue *vq)
{
    struct spdk_vhost_blk_task *task;
    int rc;
    uint16_t reqs[32];
    uint16_t reqs_cnt, i;

    /* 先给出一些关于IO环的知识：
            (1) 简单来说，每个IO环分成descriptor数组、avail数组和used数组三个部分，数组元素个数均为环的最大请求个数。
            (2) descriptor数组元素代表一段虚拟机内存，每个IO请求至少包含三段，请求头部段、数据段(至少一个)和响应段。
                请求头部包含请求类型(读或写)、访问偏移，数据段代表实际的数据存放位置，响应段记录请求处理结果。一般来说，
                每个IO请求在descriptor中至少要占据三个元素；不过当配置了indirect特性后，一个IO请求只占用一项，只不过
                该项指向的内存段又是一个descriptor数组，该数组元素个数为IO请求实际所需内存段。
            (3) avail数组用来记录已下发的IO请求，数组元素内容为IO请求在descriptor数组中的下标，该下标可作为请求的id。
            (4) used数组用来记录已完成的IO响应，数组元素内容同样为IO在descritpror数组中的下标。
    */

    /* 从IO环的avail数组中中取出一批请求，将请求id放入reqs数组中；每次将环取空或者最多取32个请求 */
    reqs_cnt = spdk_vhost_vq_avail_ring_get(vq, reqs, SPDK_COUNTOF(reqs));
    ...

    /* 依次对reqs数组中的请求进行处理 */
    for (i = 0; i &amp;lt; reqs_cnt; i++) {
        ...
        
        /* 以请求id作为下标，找到对应的task对象。注，初始化时，会按IO环的最大请求个数来申请tasks数组 */
        task = &amp;amp;((struct spdk_vhost_blk_task *)vq-&amp;gt;tasks)[reqs[i]];
        ...

        bvdev-&amp;gt;vdev.task_cnt++; /* 作统计计数 */

        task-&amp;gt;used = true; /* 代表tasks数组中该项正在被使用 */
        task-&amp;gt;iovcnt = SPDK_COUNTOF(task-&amp;gt;iovs); /* iovs数组将来会记录IO请求中数据段的内存映射信息 */
        task-&amp;gt;status = NULL; /* 将来指向IO响应段，用来给虚拟机返回IO处理结果 */
        task-&amp;gt;used_len = 0;

        /* 将IO环中请求的详细信息记录到task中，并递交给bdev层处理 */
        rc = process_blk_request(task, bvdev, vq);
        ...
    }
}

static int
process_blk_request(struct spdk_vhost_blk_task *task, struct spdk_vhost_blk_dev *bvdev,
struct spdk_vhost_virtqueue *vq)
{
    const struct virtio_blk_outhdr *req;
    struct iovec *iov;
    uint32_t type;
    uint32_t payload_len;
    int rc;

    /* 将IO环descriptor数组中记录的请求内存段(以gpa表示，即Guest Physical Address)映成vhost进程中的
        虚拟地址(vva, vhost virtual address)，并保存到task的iovs数组中 */
    if (blk_iovs_setup(&amp;amp;bvdev-&amp;gt;vdev, vq, task-&amp;gt;req_idx, task-&amp;gt;iovs, &amp;amp;task-&amp;gt;iovcnt, &amp;amp;payload_len)) {
        ...
    }

    /* 第一个请求内存段为请求头部，即struct virtio_blk_outhdr，记录请求类型、访问位置信息 */
    iov = &amp;amp;task-&amp;gt;iovs[0];
    ...
    req = iov-&amp;gt;iov_base;

    /* 最后一个请求内存段用来保存请求处理结果 */
    iov = &amp;amp;task-&amp;gt;iovs[task-&amp;gt;iovcnt - 1];
    ...
    task-&amp;gt;status = iov-&amp;gt;iov_base;

    /* 除去一头一尾，中间的请求内存段为数据段 */
    payload_len -= sizeof(*req) + sizeof(*task-&amp;gt;status);
    task-&amp;gt;iovcnt -= 2;

    type = req-&amp;gt;type;
    
    switch (type) {
    case VIRTIO_BLK_T_IN:
    case VIRTIO_BLK_T_OUT:

        /*  对于读写请求，调用bdev读写接口，并注册请求完成后的回调函数为blk_request_complete_cb */
        if (type == VIRTIO_BLK_T_IN) {
            task-&amp;gt;used_len = payload_len + sizeof(*task-&amp;gt;status);
            rc = spdk_bdev_readv(bvdev-&amp;gt;bdev_desc, bvdev-&amp;gt;bdev_io_channel,
                    &amp;amp;task-&amp;gt;iovs[1], task-&amp;gt;iovcnt, req-&amp;gt;sector * 512,
                    payload_len, blk_request_complete_cb, task);
        } else if (!bvdev-&amp;gt;readonly) {
            task-&amp;gt;used_len = sizeof(*task-&amp;gt;status);
            rc = spdk_bdev_writev(bvdev-&amp;gt;bdev_desc, bvdev-&amp;gt;bdev_io_channel,
                    &amp;amp;task-&amp;gt;iovs[1], task-&amp;gt;iovcnt, req-&amp;gt;sector * 512,
                    payload_len, blk_request_complete_cb, task);
        } else {
            SPDK_DEBUGLOG(SPDK_LOG_VHOST_BLK, &quot;Device is in read-only mode!\n&quot;);
            rc = -1;
        }
        break;
    case VIRTIO_BLK_T_GET_ID:
        ...
        break;
    default:
        ...
        return -1;
    }   

    return 0;
}

static int
blk_iovs_setup(struct spdk_vhost_dev *vdev, struct spdk_vhost_virtqueue *vq, uint16_t req_idx,
                struct iovec *iovs, uint16_t *iovs_cnt, uint32_t *length)
{
    struct vring_desc *desc, *desc_table;
    uint16_t out_cnt = 0, cnt = 0;
    uint32_t desc_table_size, len = 0;
    int rc;

    /* 从IO环descriptor数组中获取请求对应的所有内存段信息，并映射成vva地址 */
    rc = spdk_vhost_vq_get_desc(vdev, vq, req_idx, &amp;amp;desc, &amp;amp;desc_table, &amp;amp;desc_table_size);
    ...

    while (1) {
        ...
        len += desc-&amp;gt;len;

        out_cnt += spdk_vhost_vring_desc_is_wr(desc);

        rc = spdk_vhost_vring_desc_get_next(&amp;amp;desc, desc_table, desc_table_size);
        if (rc != 0) {
            ...
            return -1;
        } else if (desc == NULL) {
            break;
        }
    }

    ...

    *length = len;
    *iovs_cnt = cnt;
    return 0;
}

int
spdk_vhost_vq_get_desc(struct spdk_vhost_dev *vdev, struct spdk_vhost_virtqueue *virtqueue,
                    uint16_t req_idx, struct vring_desc **desc, struct vring_desc **desc_table,
                    uint32_t *desc_table_size)
{
    
    *desc = &amp;amp;virtqueue-&amp;gt;vring.desc[req_idx];

    if (spdk_vhost_vring_desc_is_indirect(*desc)) {
        assert(spdk_vhost_dev_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC));
        *desc_table_size = (*desc)-&amp;gt;len / sizeof(**desc);
        
        /* 将IO环中记录的gpa地址转换成vhost的虚拟地址，qemu和vhost之间的内存映射关系管理我们将在管理面分析时讨论 */
        *desc_table = spdk_vhost_gpa_to_vva(vdev, (*desc)-&amp;gt;addr, sizeof(**desc) * *desc_table_size);
        *desc = *desc_table;
        if (*desc == NULL) {
            return -1;
        }

        return 0;
    }

    *desc_table = virtqueue-&amp;gt;vring.desc;
    *desc_table_size = virtqueue-&amp;gt;vring.size;

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接着，我们看一下bdev层对IO请求的处理，以读请求为例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/bdev/bdev.c:

int
spdk_bdev_readv(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
                struct iovec *iov, int iovcnt,
                uint64_t offset, uint64_t nbytes,
                spdk_bdev_io_completion_cb cb, void *cb_arg)
{
    uint64_t offset_blocks, num_blocks;

    ...
    
    /* 将字节转换成块进行实际的IO操作 */
    return spdk_bdev_readv_blocks(desc, ch, iov, iovcnt, offset_blocks, num_blocks, cb, cb_arg);
}

int spdk_bdev_readv_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
                            struct iovec *iov, int iovcnt,
                            uint64_t offset_blocks, uint64_t num_blocks,
                            spdk_bdev_io_completion_cb cb, void *cb_arg)
{
    struct spdk_bdev *bdev = desc-&amp;gt;bdev;
    struct spdk_bdev_io *bdev_io;
    struct spdk_bdev_channel *channel = spdk_io_channel_get_ctx(ch);

    /* io channel是一个线程强相关对象，不同的线程对应不同的channel，
        这里spdk_bdev_channel包含一个线程独立的缓存池，先从中申请bdev_io内存(免锁)，
        如果申请不到，再到全局的mempool中申请内存 */
    bdev_io = spdk_bdev_get_io(channel);
    ...

    /*  将接口参数记录到bdev_io中，并继续递交 */
    bdev_io-&amp;gt;ch = channel;
    bdev_io-&amp;gt;type = SPDK_BDEV_IO_TYPE_READ;
    bdev_io-&amp;gt;u.bdev.iovs = iov;
    bdev_io-&amp;gt;u.bdev.iovcnt = iovcnt;
    bdev_io-&amp;gt;u.bdev.num_blocks = num_blocks;
    bdev_io-&amp;gt;u.bdev.offset_blocks = offset_blocks;
    spdk_bdev_io_init(bdev_io, bdev, cb_arg, cb);

    spdk_bdev_io_submit(bdev_io);
    return 0;
}

static void
spdk_bdev_io_submit(struct spdk_bdev_io *bdev_io)
{
    struct spdk_bdev *bdev = bdev_io-&amp;gt;bdev;

    if (bdev_io-&amp;gt;ch-&amp;gt;flags &amp;amp; BDEV_CH_QOS_ENABLED) { /* 开启了bdev的qos特性时走该流程 */
        ...
    } else {
        _spdk_bdev_io_submit(bdev_io); /* 直接递交 */
    }
}

static void
_spdk_bdev_io_submit(void *ctx)
{
    struct spdk_bdev_io *bdev_io = ctx;
    struct spdk_bdev *bdev = bdev_io-&amp;gt;bdev;
    struct spdk_bdev_channel *bdev_ch = bdev_io-&amp;gt;ch;
    struct spdk_io_channel *ch = bdev_ch-&amp;gt;channel; /* 底层驱动对应的io channel */
    struct spdk_bdev_module_channel	*module_ch = bdev_ch-&amp;gt;module_ch;

    bdev_io-&amp;gt;submit_tsc = spdk_get_ticks();
    bdev_ch-&amp;gt;io_outstanding++;
    module_ch-&amp;gt;io_outstanding++;
    bdev_io-&amp;gt;in_submit_request = true;
    if (spdk_likely(bdev_ch-&amp;gt;flags == 0)) {
        if (spdk_likely(TAILQ_EMPTY(&amp;amp;module_ch-&amp;gt;nomem_io))) {
            /* 不同的驱动在生成bdev对象时会注册不同的fn_table，这里将调用驱动注册的submit_request函数 */
            bdev-&amp;gt;fn_table-&amp;gt;submit_request(ch, bdev_io);
        } else {
            bdev_ch-&amp;gt;io_outstanding--;
            module_ch-&amp;gt;io_outstanding--;
            TAILQ_INSERT_TAIL(&amp;amp;module_ch-&amp;gt;nomem_io, bdev_io, link);
        }
    } else if (bdev_ch-&amp;gt;flags &amp;amp; BDEV_CH_RESET_IN_PROGRESS) {
        ...
    } else if (bdev_ch-&amp;gt;flags &amp;amp; BDEV_CH_QOS_ENABLED) {
        ...
    } else {
        ...
    }
    bdev_io-&amp;gt;in_submit_request = false;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  最后，我们来看一下bdev的NVMe驱动的处理逻辑：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/bdev/bdev_nvme.c:

static const struct spdk_bdev_fn_table nvmelib_fn_table = {
    .destruct           = bdev_nvme_destruct,
    .submit_request		= bdev_nvme_submit_request,
    .io_type_supported	= bdev_nvme_io_type_supported,
    .get_io_channel		= bdev_nvme_get_io_channel,
    .dump_info_json		= bdev_nvme_dump_info_json,
    .write_config_json	= bdev_nvme_write_config_json,
    .get_spin_time		= bdev_nvme_get_spin_time,
};

static void
bdev_nvme_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
{
    int rc = _bdev_nvme_submit_request(ch, bdev_io);

    if (spdk_unlikely(rc != 0)) {
        if (rc == -ENOMEM) {
            spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_NOMEM);
        } else {
            spdk_bdev_io_complete(bdev_io, SPDK_BDEV_IO_STATUS_FAILED);
        }
    }
}

static int
_bdev_nvme_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
{
    /* 将ch扩展成具体的nvme_io_channel，其对应一个queue parir */
    struct nvme_io_channel *nvme_ch = spdk_io_channel_get_ctx(ch);
    if (nvme_ch-&amp;gt;qpair == NULL) {
        /* The device is currently resetting */
        return -1;
    }

    switch (bdev_io-&amp;gt;type) {

    /* 针对读写请求，会将bdev_io扩展成nvme_bdev_io请求后，再将请求内容填入io channel
        对应的queue pair中，并通知物理硬件处理 */
    case SPDK_BDEV_IO_TYPE_READ:
        spdk_bdev_io_get_buf(bdev_io, bdev_nvme_get_buf_cb,
                        bdev_io-&amp;gt;u.bdev.num_blocks * bdev_io-&amp;gt;bdev-&amp;gt;blocklen);
        return 0;

    case SPDK_BDEV_IO_TYPE_WRITE:
        return bdev_nvme_writev((struct nvme_bdev *)bdev_io-&amp;gt;bdev-&amp;gt;ctxt,
                                ch,
                                (struct nvme_bdev_io *)bdev_io-&amp;gt;driver_ctx,
                                bdev_io-&amp;gt;u.bdev.iovs,
                                bdev_io-&amp;gt;u.bdev.iovcnt,
                                bdev_io-&amp;gt;u.bdev.num_blocks,
                                bdev_io-&amp;gt;u.bdev.offset_blocks);
    ...
    default:
        return -EINVAL;
    }

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  详细的NVMe请求处理不在本文的讨论范围内，感兴趣的读者可以自行深入分析。&lt;/p&gt;

&lt;h3 id=&quot;io响应返回流程代码解析&quot;&gt;IO响应返回流程代码解析&lt;/h3&gt;

&lt;p&gt;  reactor线程通过bdev_nvme_poll函数获知已完成的NVMe响应，最终会调用bdev层的spdk_bdev_io_complete来处理响应：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/bdev/bdev.c:

void
spdk_bdev_io_complete(struct spdk_bdev_io *bdev_io, enum spdk_bdev_io_status status)
{
    ...
    bdev_io-&amp;gt;status = status;

    ...
    _spdk_bdev_io_complete(bdev_io);
}

static inline void
_spdk_bdev_io_complete(void *ctx)
{
    struct spdk_bdev_io *bdev_io = ctx;

    ...

    /* 如果请求执行成功，则更新一些统计信息 */
    if (bdev_io-&amp;gt;status == SPDK_BDEV_IO_STATUS_SUCCESS) {
        switch (bdev_io-&amp;gt;type) {
        case SPDK_BDEV_IO_TYPE_READ:
            bdev_io-&amp;gt;ch-&amp;gt;stat.bytes_read += bdev_io-&amp;gt;u.bdev.num_blocks * bdev_io-&amp;gt;bdev-&amp;gt;blocklen;
            bdev_io-&amp;gt;ch-&amp;gt;stat.num_read_ops++;
            bdev_io-&amp;gt;ch-&amp;gt;stat.read_latency_ticks += (spdk_get_ticks() - bdev_io-&amp;gt;submit_tsc);
            break;
        case SPDK_BDEV_IO_TYPE_WRITE:
            bdev_io-&amp;gt;ch-&amp;gt;stat.bytes_written += bdev_io-&amp;gt;u.bdev.num_blocks * bdev_io-&amp;gt;bdev-&amp;gt;blocklen;
            bdev_io-&amp;gt;ch-&amp;gt;stat.num_write_ops++;
            bdev_io-&amp;gt;ch-&amp;gt;stat.write_latency_ticks += (spdk_get_ticks() - bdev_io-&amp;gt;submit_tsc);
            break;
        default:
            break;
        }
    }

    /* 调用上层注册回调，这里将回到vhost-blk的blk_request_complete_cb */
    bdev_io-&amp;gt;cb(bdev_io, bdev_io-&amp;gt;status == SPDK_BDEV_IO_STATUS_SUCCESS, bdev_io-&amp;gt;caller_ctx);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spdk/lib/vhost/vhost_blk.c:

static void
blk_request_complete_cb(struct spdk_bdev_io *bdev_io, bool success, void *cb_arg)
{
    struct spdk_vhost_blk_task *task = cb_arg;

    spdk_bdev_free_io(bdev_io); /* 释放bdev_io */
    blk_request_finish(success, task);
}

static void
blk_request_finish(bool success, struct spdk_vhost_blk_task *task)
{
    *task-&amp;gt;status = success ? VIRTIO_BLK_S_OK : VIRTIO_BLK_S_IOERR;

    /* 往虚拟机中放入响应并以虚拟中断方式通知虚拟机IO完成 */
    spdk_vhost_vq_used_ring_enqueue(&amp;amp;task-&amp;gt;bvdev-&amp;gt;vdev, task-&amp;gt;vq, task-&amp;gt;req_idx,
            task-&amp;gt;used_len);

    /* 释放当前task，实际就是将task-&amp;gt;used置为false */
    blk_task_finish(task);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，整个IO流程已经分析完毕，可见SPDK对IO的处理还是非常简洁的，这便是高性能的基石。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-ioanalyze/&quot;&gt;【SPDK】三、IO流程代码解析&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 16 May 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/SPDK-ioanalyze/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/SPDK-ioanalyze/</guid>
        
        <category>SPDK</category>
        
        
      </item>
    
      <item>
        <title>【SPDK】二、IO栈对比与线程模型</title>
        <description>&lt;p&gt;  这里我们以SPDK前端配置成vhost-blk、后端配置成NVMe SSD场景为例，来分析SPDK的IO栈和线程模型。&lt;/p&gt;

&lt;h3 id=&quot;io栈对比与时延分析&quot;&gt;IO栈对比与时延分析&lt;/h3&gt;

&lt;p&gt;  我们先来对比一下qemu使用普通内核NVMe驱动和使用SPDK vhost时IO栈的差别，如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/spdk/iostack.jpg&quot; height=&quot;550&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  无论使用传统内核NVMe驱动，还是使用vhost，虚拟机内部的IO处理流程都是一样的：IO请求下发时需要从用户态应用程序中切换到内核态，并穿过文件系统和virtio-blk驱动后，才能借助IO环(IO Ring)将请求信息传递给虚拟设备进行处理；虚拟设备处理完成后，以中断方式通知虚拟机，虚拟机内进过驱动和文件系统的回调后，最终唤醒应用程序返回用户态继续执行业务逻辑。在intel Xeon E5620@2.4GHz服务器上的测试结果表明，虚拟机内部的请求下发与响应处理总时延约15us。&lt;/p&gt;

&lt;p&gt;  针对传统内核NVMe驱动，qemu进程中io线程负责处理虚拟机下发的IO请求：它通过virtio backend从IO环中取出请求，并将请求通过系统调用传递给内核块层和NVMe驱动层进行处理，最后由NVMe驱动将请求通过Queue Pair(类似IO环)交由物理NVMe控制器进行处理；NVMe控制器处理完成后以物理中断方式通知qemu io线程，由它将响应放入虚拟机IO环中并以虚拟中断通知虚拟机请求完成。在此我们看到，qemu中总共的处理时延约15us，而NVMe硬件(华为ES3000 NVMe SSD)上的处理时延才10us(读请求)。&lt;/p&gt;

&lt;p&gt;  针对SPDK vhost，qemu进程不参与IO请求的处理(仅在初始化时起作用)，所有虚拟机下发的IO请求均由vhost进程处理。vhost进程以轮循的方式不断从IO环中取出请求(意味着虚拟机下发IO请求时，不用通知虚拟设备)，对于取出的每个请求，vhost将其以任务方式交给bdev抽象层进行处理；bdev根据后端设备的类型来选择不同的驱动进行处理，例如对于NVMe设备，将使用用户态的NVMe驱动在用户空间完成对Queue Pair的操作。vhost进程同样会轮循物理NVMe设备的Queue Pair，如果有响应例会立刻进行处理，而无须等待物理中断。vhost在处理NVMe响应过程中，会向虚拟机IO环中添加响应，并以虚拟中断方式通知虚拟机。我们可以看到，vhost中绝大部分操作都是在用户态完成的(中断通知虚拟机时会进入内核态通过KVM模块完成)，各层时延均非常短，app和bdev抽象层约2us，NVMe用户态驱动约2us。&lt;/p&gt;

&lt;p&gt;  因此,端到端时延对比来看，我们可以发现传统NVMe IO栈的总时延约40us，而SPDK用户态NVMe IO栈时延不到30us，&lt;strong&gt;时延上有25%以上的优化&lt;/strong&gt;。另一方面，在吞吐量(IOPS)方面，如果我们给virtio-blk设备配置多队列(确保虚拟机IO压力足够)，并在后端NVMe设备不成为瓶颈的前提下，传统NVMe IO栈在单个qemu io线程处理时，最多能达到20万IOPS，而SPDK vhost在单线程处理时可达100万IOPS，&lt;strong&gt;同等CPU开销下，吞吐量上有5倍以上的性能提升&lt;/strong&gt;。传统NVMe IO栈在处理多队列模型时，相比单队列模型，减少了线程间通知开销，一次通知可以处理多个IO请求，因此多队列相比单队列模型会有较大的IOPS提升；而vhost得益于全用户态及轮循模式，进一步减少了内核切换和通知开销，带来了吞吐量的大幅提升。&lt;/p&gt;

&lt;h3 id=&quot;线程模型分析&quot;&gt;线程模型分析&lt;/h3&gt;

&lt;p&gt;  在了解了SPDK的IO栈之后，我们进一步来分析一下vhost进程的线程模型，如下图所示。图中示例场景为，一台服务器上插了一张NVMe SSD卡，卡上划分了三个namespace；三个namespace分别配给了三台虚拟机的vhost-user-blk-pci设备。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/spdk/thread.jpg&quot; height=&quot;560&quot; width=&quot;550&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  vhost进程启动时可以配置多个轮循线程(reactor)，每个线程绑定一个物理CPU。在示例场景下，我们假设配置了两个轮循线程reactor_0和reactor_1，分别对应物理CPU0和物理CPU1。每配置一个vhost-blk设备时，同样要为该设备绑定物理核，并且只能绑定到一个物理核上，例如这里我们假设vm1的vhost-blk设备绑定到CPU0，vm2和vm3绑定到CPU1。那么reactor_0将轮循vm1中vhost-blk的IO环，reactor_1将依次轮循vm2和vm3的IO环。&lt;/p&gt;

&lt;p&gt;  vhost线程在操作相同NVMe控制器下的namespace时，不同的vhost线程会申请不同的IO Channel(实际对应NVMe Queue Pair，作用类似虚拟机IO环)，并且每个线程都会轮循各自申请的IO Channel中的响应消息。例如图中reactor_0会向NVMe控制器申请QueuePair1，并在轮循过程中注册对该QueuePair的poller函数(负责从中取响应)；reactor_1则会向NVMe控制器申请QueuePair2并轮循该QueuePair。如此一来，就能提升对后端NVMe设备的并发访问度，充分发挥物理设备的吞吐量优势。&lt;/p&gt;

&lt;p&gt;  综上所述，&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;每个vhost线程都会轮循若干个vhost设备的IO环(一个vhost设备无论有多少个环，都只会在一个线程中处理)，并且会向有操作述求的物理存储控制器(例如NVMe控制器、virtio-blk控制器、virtio-scsi控制器等)申请一个独立的IO Channel(IO环可以理解为对前端虚拟机呈现的一个IO Channel)并对其进行轮循。&lt;/li&gt;
    &lt;li&gt;无论是前端虚拟机IO环，还是后端IO Channel，都只会在一个vhost线程中被轮循，因此这就避免了多线程并发操作同一个对象，可以通过无锁的方式操作IO环或IO Channel。&lt;/li&gt;
    &lt;li&gt;针对前端虚拟机来说，一个vhost设备无论有多少个环，都只会在一个vhost线程中处理。这种设计上的约束虽说可以简化实现，但也带来了吞吐量性能扩展上的限制，即一个vhost设备在后端物理存储非瓶颈的前提下，最高的IOPS为100万。因此我们可以考虑将vhost的多个IO环拆分到多个vhost线程中处理，进一步提升吞吐量。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-iostack/&quot;&gt;【SPDK】二、IO栈对比与线程模型&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 15 May 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/SPDK-iostack/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/SPDK-iostack/</guid>
        
        <category>SPDK</category>
        
        
      </item>
    
      <item>
        <title>【SPDK】一、概述</title>
        <description>&lt;p&gt;  随着越来越多公有云服务提供商采用SPDK技术作为其高性能云存储的核心技术之一，intel推出的SPDK技术备受业界关注。本篇博文就和大家一起探索SPDK。&lt;/p&gt;

&lt;h3 id=&quot;什么是spdk为什么需要它&quot;&gt;什么是SPDK？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  SPDK(全称Storage Performance Development Kit)，提供了一整套工具和库，以实现高性能、扩展性强、全用户态的存储应用程序。它是继DPDK之后，intel在存储领域推出的又一项颠覆性技术，旨在大幅缩减存储IO栈的软件开销，从而提升存储性能，可以说它就是为了存储性能而生。&lt;/p&gt;

&lt;p&gt;  为便于大家理解，我们先介绍一下SPDK在虚拟化场景下的使用方法，以给大家一些直观的认识。&lt;/p&gt;

&lt;h4 id=&quot;1-dpdk的编译与安装&quot;&gt;&lt;strong&gt;1. DPDK的编译与安装&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SPDK使用了DPDK中一些通用的功能和机制，因此首先需要下载DPDK的源码并完成编译和安装：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@linux:~/DPDK]# &lt;strong&gt;make config T=x86_64-native-linuxapp-gcc&lt;/strong&gt;&lt;br /&gt;
[root@linux:~/DPDK]# &lt;strong&gt;make&lt;/strong&gt;&lt;br /&gt;
[root@linux:~/DPDK]# &lt;strong&gt;make install&lt;/strong&gt; (默认安装到/usr/local，包括.a库文件和头文件)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;2-spdk的编译&quot;&gt;&lt;strong&gt;2. SPDK的编译&lt;/strong&gt;&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@linux:~/SPDK]# &lt;strong&gt;./configure –with-dpdk=&lt;/strong&gt;/usr/local&lt;br /&gt;
[root@linux:~/SPDK]# &lt;strong&gt;make&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  编译成功后，我们在spdk/app/vhost目录下可以看到一个名为vhost的可执行文件，它就是SPDK在虚拟化场景下为虚拟机模拟程序qemu提供的存储转发服务，借此为虚拟机用户带来高性能的虚拟磁盘。&lt;/p&gt;

&lt;h4 id=&quot;3-大页内存配置&quot;&gt;&lt;strong&gt;3. 大页内存配置&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SPDK vhost进程和qemu进程通过大页共享虚拟机可见内存，因此需要进行一些大页的配置和调整：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;可通过设置/sys/kernel/mm/hugepages/hugepages-xxx/nr_hugepages来调整大页数量(xxx通常为2M或1G)&lt;/li&gt;
    &lt;li&gt;qemu使用挂载到/dev/hugepages目录下的hugetlbfs来使用大页内存，可在挂载参数中指定大页大小，如mount -t hugetlbfs -o pagesize=1G nodev /dev/hugepages&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;4-vhost配置与启动&quot;&gt;&lt;strong&gt;4. vhost配置与启动&lt;/strong&gt;&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@linux:~/SPDK]# &lt;strong&gt;HUGEMEM=&lt;/strong&gt;4096 &lt;strong&gt;scripts/setup.sh&lt;/strong&gt;&lt;br /&gt;
[root@linux:~/SPDK]# &lt;strong&gt;app/vhost/vhost -S&lt;/strong&gt; /var/tmp &lt;strong&gt;-m&lt;/strong&gt; 0x3 &lt;strong&gt;-c&lt;/strong&gt; etc/spdk/rootw.conf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  vhost命令执行过程中，是一个常驻的服务进程；-S参数指定了socket文件的生成的目录，每个虚拟磁盘(vhost-blk)或虚拟存储控制器(vhost-scsi)都会在该目录下产生一个socket文件，以便qemu程序与vhost进程建立连接；-m参数指定了vhost进程中的轮循线程所绑定的物理CPU核，例如0x3代表在0号和1号核上各绑定一个轮循线程；-c参数指定了vhost进程所需的配置文件，例如这里我通过内存设备(SPDK中称之为Malloc设备)提供了一个vhost-blk磁盘：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@linux:~/SPDK]# &lt;strong&gt;cat&lt;/strong&gt; etc/spdk/rootw.conf &lt;br /&gt;
[root@linux:~/SPDK]#  &lt;br /&gt;
&lt;strong&gt;[Malloc]&lt;/strong&gt;  &lt;br /&gt;
NumberOfLuns 1   #创建一个内存设备，默认名称为Malloc0  &lt;br /&gt;
LunSizeInMB 128  #该内存设备大小为128M  &lt;br /&gt;
BlockSize 4096   #该内存设备块大小为4096字节  &lt;br /&gt;
&lt;strong&gt;[VhostBlk0]&lt;/strong&gt;  &lt;br /&gt;
Name vhost.2     #创建一个vhost-blk设备，名称为vhost.2  &lt;br /&gt;
Dev Malloc0      #该设备后端对应的物理设备为Malloc0  &lt;br /&gt;
Cpumask 0x1      #将该设备绑定到0号核的轮循线程上&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;5-虚拟机启动与验证&quot;&gt;&lt;strong&gt;5. 虚拟机启动与验证&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  vhost进程启动后，我们就可以拉起qemu进程来启动一个新虚拟机，qemu进程的命令行参数如下(重点关注与SPDK vhost相关部分)：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[root@linux:~/qemu]# &lt;strong&gt;./x86_64-softmmu/qemu-system-x86_64&lt;/strong&gt; -name rootw-vm -machine pc-i440fx-2.6,accel=kvm \  &lt;br /&gt;
&lt;strong&gt;-m 1G -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem&lt;/strong&gt; \  &lt;br /&gt;
-drive file=/mnt/centos.qcow2,format=qcow2,id=virtio-disk0,cache=none,aio=native -device virtio-blk-pci,drive=virtio-disk0,id=blk0 \  &lt;br /&gt;
&lt;strong&gt;-chardev socket,id=char_rootw,path=/var/tmp/vhost.2 -device vhost-user-blk-pci,id=blk_rootw,chardev=char_rootw&lt;/strong&gt; \  &lt;br /&gt;
-vnc 0.0.0.0:0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  通过上述启动参数，我们可以看出：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;vhost进程和qemu进程通过大页方式共享虚拟机可见的所有内存(原因我们将在深入分析时讨论)&lt;/li&gt;
    &lt;li&gt;qemu在配置vhost-user-blk-pci设备时，只需要指定vhost生成的socket文件即可(-S参数指定的路径后拼接上设备名称)&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  虚拟机启动成功后，我们通过vnc工具登陆虚拟机，执行lsblk命令可以查看到vda和vdb两个virtio-blk块设备，表明vhost后端已成功生效。这里要说明一下，qemu中配置的virtio-blk-pci设备、vhost-user-blk-pci设备或vhost-blk-pci设备，在虚拟机内部均呈现为virtio-blk-pci设备，因此在虚拟机中采用相同的virtio-blk-pci和virtio-blk驱动进行使能，如此一来不同的后端实现技术在虚拟机内部均采用一套驱动，可以减少驱动的开发和维护工作量。&lt;/p&gt;

&lt;h3 id=&quot;如何实现spdk&quot;&gt;如何实现SPDK?&lt;/h3&gt;

&lt;p&gt;  SPDK能实现高性能，得益于以下三个关键技术：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;全用户态&lt;/strong&gt;，它把所有必要的驱动全部移到了用户态，避免了系统调用的开销并真正实现内存零拷贝&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;轮循模式&lt;/strong&gt;，针对高速物理存储设备，采用轮循的方式而非中断通知方式判断请求完成，大大降低时延并减少性能波动&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;无锁机制&lt;/strong&gt;，在IO路径上避免采用任何锁机制进行同步，降低时延并提升吞吐量&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  下面我们将深入到SPDK的实现细节，去看看这些关键点分别是如何提升性能的。&lt;/p&gt;

&lt;h4 id=&quot;1-整体架构&quot;&gt;&lt;strong&gt;1. 整体架构&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  首先，我们来了解一下SPDK内部的整体组件架构：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/spdk/arch.png&quot; height=&quot;300&quot; width=&quot;550&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  SPDK整体分为三层：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;存储协议层(Storage Protocols)，指SPDK支持存储应用类型。iSCSI Target对外提供iSCSI服务，用户可以将运行SPDK服务的主机当前标准的iSCSI存储设备来使用；vhost-scsi或vhost-blk对qemu提供后端存储服务，qemu可以基于SPDK提供的后端存储为虚拟机挂载virtio-scsi或virtio-blk磁盘；NVMF对外提供基于NVMe协议的存储服务端。注意，图中vhost-blk在spdk-18.04版本中已实现，后面我们主要基于此版本进行代码分析。&lt;/li&gt;
    &lt;li&gt;存储服务层(Storage Services)，该层实现了对块和文件的抽象。目前来说，SPDK主要在块层实现了QoS特性，这一层整体上还是非常薄的。&lt;/li&gt;
    &lt;li&gt;驱动层(drivers)，这一层实现了存储服务层定义的抽象接口，以对接不同的存储类型，如NVMe，RBD，virtio，aio等等。图中把驱动细分成两层，和块设备强相关的放到了存储服务层，而把和硬件强相关部分放到了驱动层。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;2-深入数据面&quot;&gt;&lt;strong&gt;2. 深入数据面&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  接下来我们将以SPDK前端配置成vhost-blk、后端配置成NVMe SSD场景为例，来分析整个数据面流程。我们将分两部分完成数据面的分析：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-iostack/&quot;&gt;IO栈对比与线程模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-ioanalyze/&quot;&gt;IO流程代码解析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-深入管理面&quot;&gt;&lt;strong&gt;3. 深入管理面&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  管理面流程比数据面要复杂得多，也无趣得多。因此我们在分析完数据面流程之后，再回头看看数据面中涉及的各个对象分别是如何被创建和初始化的，这样更利于我们理解这样做的目的，也不会一下子就被这些复杂的流程吓住而无法坚持往下分析。&lt;/p&gt;

&lt;p&gt;  整个管理面功能包含vhost启动初始化和通过rpc动态管理两个部分，这里我们主要讨化启动初始化，根据启动时的先后顺序，分为&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-reactors-init/&quot;&gt;reactor线程初始化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;bdev子系统初始化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;vhost子系统初始化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;qemu连接请求处理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/05/SPDK-all/&quot;&gt;【SPDK】一、概述&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 10 May 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/SPDK-all/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/SPDK-all/</guid>
        
        <category>SPDK</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】九、Monitor原理分析</title>
        <description>&lt;p&gt;  Monitors是ceph集群的管理节点，负责维护整个集群的全局信息(如OSDMap)；Client和OSD加入和退出集群时，都需要和Monitors打交道，而且都需要从Monitors中获取最新的全局信息(如OSDMap)进行相关操作(如CRUSH数据映射)。 CatKang的&lt;a href=&quot;https://www.jianshu.com/p/60b34ba5cdf2&quot;&gt;博文&lt;/a&gt;对Monitor进行了比较多全的分析，这里我只补充一些自己的理解。Monitor的代码分析大家可以对照原理分析自行开展，略显枯燥，paxos算法相关原理可参考&lt;a href=&quot;https://www.cnblogs.com/linbingdong/p/6253479.html&quot;&gt;此篇博文&lt;/a&gt;，不过注意一点，ceph没有完全按照paxos来实现，作了一定的修改。&lt;/p&gt;

&lt;h3 id=&quot;架构&quot;&gt;架构&lt;/h3&gt;

&lt;p&gt;  Monitor整体架构如下所示，注意，这里没有体现网络层，其原理和OSD中分析的类似，请参考messenger模块分析：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_monitor_arch.jpg&quot; height=&quot;280&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从总体处理流程上来看，Monitor首先也是通过messenger模块接收网络消息；接着对于不同的全局信息提供不同的PaxosService；但是对于这些服务都会提交给Paxos模块处理，该模块实现了核心的Paxos算法；对于更新请求，最终Paxos会将更新内容提交到底层的数据库中进行存储。&lt;/p&gt;

&lt;h3 id=&quot;初始化流程&quot;&gt;初始化流程&lt;/h3&gt;

&lt;p&gt;  Monitor初始化整体流程如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_monitor_init.jpg&quot; height=&quot;450&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  初始化过程中，网络messenger模块执行的动作和OSD是一样的，只不过这里只需要一个public messenger对象(Monitor不接入cluster网络平面)。消息的处理是由Monitor对象(类似OSD进程中的OSD对象)进行的，入口函数在Monitor::dispatch_op。&lt;/p&gt;

&lt;h4 id=&quot;1-bootstrap发现&quot;&gt;&lt;strong&gt;1. bootstrap发现&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  网络模块初始化完成后，Monitor首先进行的是bootstrap动作，通过网络协商的方式加入到Monitor集群中(quorum)：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_monitor_probe.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;2-election选主&quot;&gt;&lt;strong&gt;2. election选主&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  bootstrap发现其它Monitor节点后，将进行一轮选主动作，从所有Monitor中选出一个Leader，而其它Monitor就成为Peon(劳工)。选主的目的是只有Leader可以向所有Monitor发起信息变更请求，解决Paxos算法中的&lt;strong&gt;活性&lt;/strong&gt;问题。所有Monitor都可以响应查询请求。选主流程如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_monitor_election.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;3-recovery恢复&quot;&gt;&lt;strong&gt;3. recovery恢复&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  选主完成后，Leader将发起恢复动作，在所有Monitor之间进行数据信息的同步：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_monitor_recovery.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h3 id=&quot;数据服务流程&quot;&gt;数据服务流程&lt;/h3&gt;

&lt;p&gt;  所有的初始化动作完成后，Monitor进入ACTIVE状态，即可响应其它节点的读写请求。前面已经说过，对于读请求，所有Monitor均可直接提供数据信息且不涉及内部状态变化；对于写请求，只有Leader能发起变更申请(Peon只能将写请求转发给Leader发请)，在写请求被正式接受之前，Monitor是不能提供该信息的读取服务的。写请求被接受的流程如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_monitor_update.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/04/RBD-Monitor&quot;&gt;【Rados Block Device】九、Monitor原理分析&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Apr 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/RBD-Monitor/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/RBD-Monitor/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】八、OSD原理分析－FileStore模块</title>
        <description>&lt;p&gt;  从前面的博文分析中，我们知道OSD模块在请求处理的最后阶段会向ObjectStore发起操作请求，ObjectStore负责对象的实际存储功能，有filestore、bluestore、memstore、kstore等多种存储方式，这里我们以基本的filestore为例展开分析。&lt;/p&gt;

&lt;h3 id=&quot;filestore模块中对象流程概览&quot;&gt;FileStore模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  我们照例先整体来看一下FileStore模块的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_fs.jpg&quot; height=&quot;500&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  OSD模块在tp_osd_tp线程上下文的最后阶段，通过queue_transactions调用FileStore模块功能将操作请求以日志的方式提交到日志队列中，至此tp_osd_tp线程中的工作就完成了。后续由一个独立的日志写入线程journal_write从日志队列中取出操作日志并调用文件系统写入接口将日志操作写入实际的日志文件中(&lt;strong&gt;注，这里我们以journal ahead模式为例进行说明&lt;/strong&gt;)；日志写入完成后，通过queue_completions_thru接口将日志完成回调任务放入fn_jrn_objstorep线程的完成队列中。fn_jrn_objstore线程从完成队列中取出回调任务并立即调用回调，回调任务中会执行两个并发的子任务：一方面是通过op_queue将操作递交给OpWQ进行实际的数据落盘动作；另一方面是把OSD模块传入的回调任务放入fn_odsk_fstore线程池中进行处理。OpWQ队列对应tp_fstore_op线程，它会从队列中取出操作请求，并执行实际的数据落盘动作，落盘完成后再把落盘回调任务放入到fn_appl_fstore队列中进行回调处理。fn_odsk_fstore在处理OSD模块的回调任务时，会把OSD复本操作减一(因为写入日志后就认为本地写入操作完成了)，如果远端OSD复本也完成了，那就会对客户端返回操作结果。此外，还有一个fileStore_sync线程负责日志空间的回收，便于重复使用日志文件。&lt;/p&gt;

&lt;h3 id=&quot;filestore模块中类的概览&quot;&gt;FileStore模块中类的概览&lt;/h3&gt;

&lt;p&gt;  下面我们来看看FileStore模块中涉及的主要类及其关系：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_fs_class.jpg&quot; height=&quot;550&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;JournalingObjectStore是FileStore的父类，继承自抽象父类ObjectStore。JournalingObjectStore包含一个Journal对象和一个Finisher对象，分别代表日志操作对象和日志操作完成后的回调对象。&lt;/li&gt;
    &lt;li&gt;FileJournal继承了Journal类，以文件的方式实现了日志的主要操作功能。内部有一个专门的journal_write线程负责日志的落盘操作。&lt;/li&gt;
    &lt;li&gt;FileStore是核心类，继承自JournalingObjectStore。OpWQ队列用来存放数据落盘请求，op_tp线程池会从该队列中取出请求执行落盘动作。sync_thread线程负责数据同步与日志空间回收。ondisk_finishers用来处理日志落盘后的回调；apply_finishers用来处理数据落盘后的回调。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  filestore的初始化由FileStore::mount完成，其调用栈如下所示，大家可以自行展开阅读：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    \-OSD::init()
        \-FileStore::mount()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-tp_osd_tp线程上下文的日志提交&quot;&gt;&lt;strong&gt;2. tp_osd_tp线程上下文的日志提交&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来分析一下tp_osd_tp线程中的日志提交过程，其栈心处理函数为FileStore::queue_transactions：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FileStore::queue_transactions()
    |-ObjectStore::Transaction::collect_context()
    |-FileStore::build_op()
    |-JournalFile::prepare_entry()
    \-JournalingObjectStore::_op_journal_transaction()
        \-JournalFile::submit_entry()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.cc:

int FileStore::queue_transactions(Sequencer *posr, vector&amp;lt;Transaction&amp;gt;&amp;amp; tls,
        TrackedOpRef osd_op, ThreadPool::TPHandle *handle)
{
    Context *onreadable;
    Context *ondisk;
    Context *onreadable_sync;

    /*将所有事务中的回调分类进行汇总，onreadable代表on_applied，即数据落盘后的回调；
      ondisk代表on_commit代表日志落盘后的回调；onreadable_sync代表on_applied_sync代表
      数据落盘并同步完成后的回调*/
    ObjectStore::Transaction::collect_contexts(tls, &amp;amp;onreadable, &amp;amp;ondisk, &amp;amp;onreadable_sync);
    ...

    if (journal &amp;amp;&amp;amp; journal-&amp;gt;is_writeable() &amp;amp;&amp;amp; !m_filestore_journal_trailing) {
        /*封装一个新的操作op*/
        Op *o = build_op(tls, onreadable, onreadable_sync, osd_op);
        /*准备日志块内容，内部包含操作类型和操作数据*/
        int orig_len = journal-&amp;gt;prepare_entry(o-&amp;gt;tls, &amp;amp;tbl);
        
        /*OSD中对日志主要有两使用方式：parallel和writeahead。parallel代表日志落盘和数据落盘同时发起
          writeahead代表日志先落盘，成功后再发起数据落盘。这里我们主要讨论writeahead模式*/
        if (m_filestore_journal_parallel) {
            ...
        }else if (m_filestore_journal_writeahead) {
            /*提交日志到日志队列，并封装一个回调对象C_JournalAhead*/
            _op_journal_transactions(tbl, orig_len, o-&amp;gt;op,
                new C_JournaledAhead(this, osr, o, ondisk), osd_op);
        }
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/JournalingObjectStore.cc:

void JournalingObjectStore::_op_journal_transactions(
        bufferlist&amp;amp; tbl, uint32_t orig_len, uint64_t op,
        Context *onjournal, TrackedOpRef osd_op)
{
    ...
    journal-&amp;gt;submit_entry(op, tbl, orig_len, onjournal, osd_op);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  C_JournaledAhead回调对象最终会调用FileStore::_journaled_ahead，代码如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.cc:

struct C_JournaledAhead : public Context {
    FileStore *fs;
    FileStore::OpSequencer *osr;
    FileStore::Op *o;
    Context *ondisk;

    C_JournaledAhead(FileStore *f, FileStore::OpSequencer *os, FileStore::Op *o, Context *ondisk):
        fs(f), osr(os), o(o), ondisk(ondisk) { }
    void finish(int r) override {
        fs-&amp;gt;_journaled_ahead(osr, o, ondisk);
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;3-journal_write线程工作过程&quot;&gt;&lt;strong&gt;3. journal_write线程工作过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  tp_osd_tp线程将日志提交到日志队列是通过JournalFile::submit_entry实现的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/JournalFile.cc:

void FileJournal::submit_entry(uint64_t seq, bufferlist&amp;amp; e, uint32_t orig_len,
        Context *oncommit, TrackedOpRef osd_op)
{
    ...
    /*先在compleions列表中记录回调对象*/
    completions.push_back(
        completion_item(seq, oncommit, ceph_clock_now(), osd_op));

    /*唤醒journal_write线程*/
    if (writeq.empty())
        writeq_cond.Signal();

    /*将待提交日志放入日志队列writeq中*/
    writeq.push_back(write_item(seq, e, orig_len, osd_op));
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里我们看看journal_write线程的工作原理，线程入口函数为FileJournal::write_thread_entry：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/JournalFile.cc:

void FileJournal::write_thread_entry()
{
    ...
    while (1) {
        ...

        bufferlist bl;
        /*从日志队列writeq中取出若干日志项*/
        int r = prepare_multi_write(bl, orig_ops, orig_bytes);
        ...

        /*将日志项写入到日志文件中，如果非direct io，会执行fdatasync；执行完成后将回调对象送入fn_jrn_objstore处理*/
        do_write(bl);

        /*记录日志写入完成事件*/
        complete_write(orig_ops, orig_bytes);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;4-fn_jrn_objstore回调&quot;&gt;&lt;strong&gt;4. fn_jrn_objstore回调&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  日志完全落盘执行的回调即是前文指出的FileStore::_journaled_ahead，它会触发两个并行的操作：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;通过queue_op触发tp_fstore_op的数据落盘操作；&lt;/li&gt;
    &lt;li&gt;通地Finisher::queue触发OSD模块的回调&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.cc:

void FileStore::_journaled_ahead(OpSequencer *osr, Op *o, Context *ondisk)
{
    queue_op(osr, o);
    
    if (ondisk) {
        ondisk_finishers[osr-&amp;gt;id % m_ondisk_finisher_num]-&amp;gt;queue(ondisk);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;5a-tp_fstore_op线程的数据落盘及fn_appl_fstore线程池的落盘回调&quot;&gt;&lt;strong&gt;5.(a) tp_fstore_op线程的数据落盘及fn_appl_fstore线程池的落盘回调&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  queue_op操作将落盘请求放入op_wq队列后，将由tp_fstore_op线程处理，该线程将周期性地调用_process和_process_finish函数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/os/filestore/FileStore.h:

struct OpWQ : public ThreadPool::WorkQueue&amp;lt;OpSequencer&amp;gt; {
    FileStore *store;
    ...

    void _process(OpSequencer *osr, ThreadPool::TPHandle &amp;amp;handle) override {
        store-&amp;gt;_do_op(osr, handle);
    }
    
    void _process_finish(OpSequencer *osr) override {
        store-&amp;gt;_finish_op(osr);
    }

} op_wq;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  FileStore::_do_op负责将数据落盘，FileStore::_finish_op负责将回调对象送入fn_appl_fstore线程池进入处理。&lt;/p&gt;

&lt;h4 id=&quot;5b-fn_odsk_fstore线程池对osd模块传入的日志落盘回调的处理&quot;&gt;&lt;strong&gt;5.(b) fn_odsk_fstore线程池对OSD模块传入的日志落盘回调的处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  OSD传入的日志落盘回调对象为C_OSD_OnOpCommit，其实现代码为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/ReplicatedBackend.cc:

class C_OSD_OnOpCommit : public Context {
    ReplicatedBackend *pg;
    ReplicatedBackend::InProgressOp *op;
public:
    C_OSD_OnOpCommit(ReplicatedBackend *pg, ReplicatedBackend::InProgressOp *op) 
        : pg(pg), op(op) {}
    void finish(int) override {
        pg-&amp;gt;op_commit(op);
    }
};

void ReplicatedBackend::op_commit(InProgressOp *op)
{
    /*将当前多复本操作等待对象中减去本地OSD，代表本地复本已完成*/
    op-&amp;gt;waiting_for_commit.erase(get_parent()-&amp;gt;whoami_shard());

    /*如果等待队列为空，表示远端OSD复本操作也完了，那就可以执行复本操作全部完成后的回调*/
    if (op-&amp;gt;waiting_for_commit.empty()) {
        op-&amp;gt;on_commit-&amp;gt;complete(0); /*复本操作全部完成后将给客户端返回结果*/
        op-&amp;gt;on_commit = 0;
    }
    if (op-&amp;gt;done()) {
        assert(!op-&amp;gt;on_commit &amp;amp;&amp;amp; !op-&amp;gt;on_applied);
        in_progress_ops.erase(op-&amp;gt;tid);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/04/RBD-OSD-3&quot;&gt;【Rados Block Device】八、OSD原理分析－FIleStore模块&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 13 Apr 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/RBD-OSD-3/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/RBD-OSD-3/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】七、OSD原理分析－OSD模块</title>
        <description>&lt;p&gt;  OSD进程从网络收到客户端的读写请求后，交由OSD模块执行核心的请求处理逻辑，本篇博文将讨论OSD模块的实现原理。&lt;/p&gt;

&lt;h3 id=&quot;osd模块中对象流程概览&quot;&gt;OSD模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  我们先整体来看一下OSD模块的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_osd.jpg&quot; height=&quot;500&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  回顾前文对SimpleMessenger的分析，我们看到ms_pipe_read线程从网络中收到请求消息后，通过fast_dispatch接口将消息分发给OSD对象进行处理。OSD对象针对接收到的每一个消息，都会将它们放入一个工作队列中(ShardedOpWQ，片式队列)。到这里，ms_pipe_read线程的分发动作就执行完了。对于一个片式队列，会有若干个处理线程，即图中的tp_osd_tp线程，每个处理线程负责处理不同分片中的消息。它们将各自分片中的消息取出后，找到每个消息对应的PG对象(Placement Group)，进而将消息封装成操作(op)转给PG对象处理。PG对象针对读操作将直接从filestore中读出内容并返回响应消息给客户端；而对于写操作，PG对象将请求以事务(Transaction)的方式提交给PGBackend对象(本文主要讨论ReplicatedBackend)，最终事务内的操作会转变成对filestore的操作(我们将在独立的博文中讨论filestore模块的实现原理)。&lt;/p&gt;

&lt;p&gt;  为什么一个请求消息要在两个线程(ms_pipe_read和tp_osd_tp)间传递处理？其实这里体现了ceph一个核心的设计理念：&lt;strong&gt;流水线&lt;/strong&gt;。将请求的处理分成多个步骤，每个步骤放在不同的线程中处理；请求从一个线程流动到下一个线程，类似工产里的流水流；这样可以大大提升处理请求的吞吐量(即每秒完成的请求数量)。那么时延呢？&lt;/p&gt;

&lt;h3 id=&quot;osd模块中类的概览&quot;&gt;OSD模块中类的概览&lt;/h3&gt;

&lt;p&gt;  下面我们来看看OSD模块中涉及的主要类及其关系：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_osd_class.jpg&quot; height=&quot;500&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;OSD是核心类，它内部包含两个Messenger对象指针，分别指向cluster网络(cluster_messenger)和public网络(client_messenger)。store指向后端对象存储池，用来进行实际的对象存取操作。内部包含一个片式队列(ShardedOpWQ)和一个处理线程池(SharedThreadPool)，OSD对象将请求消息放入片式队列中，再由不同的处理线程从队列中取出消息进行下一步处理。OSD中还包含全局的OSDMap和映射到本OSD的所有PG对象。&lt;/li&gt;
    &lt;li&gt;ShardedOpWQ类代表片式队列，number_shards是队列中总的分片数，每个分片都包含一个ShardedData，其内部有一个优先级队列用来接收请求消息(通过_enqueue操作)。每个片式队列都关联一个处理线程池，池中的每个线程都通过_process接口从对应队列中取出消息进行后续处理。&lt;/li&gt;
    &lt;li&gt;PrimaryLogPG类继承PG类，代表具体的Placement Group的一种实现。每个PrimaryLogPG对象包含一个pgbackend对象，该对象负责数据复本的处理。目前有两种数据复本的实现方式，Replicated(复制)和EC(校验码)，分别对应ReplicatedBackend和ECBackend。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  OSD模块的初始化代码位于OSD:init中，代码流程比较锁碎。这里我们给出初始化调用栈，并作一些简要说明：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;在挂载后端ObjectStore后，OSD模块调用load_pgs开始加载后端ObjectStore中保存的pg对象。这里先枚举ObjectStore中所有的pg，例如对于filestore，将查找CURRENT目录下的所有子目录(每个子目录代表一个pg)；然后打开该pg(生成具体的PG对象，如PrimaryLogPG)并读取pg状态信息。&lt;/li&gt;
    &lt;li&gt;启动osd_op_tp线程池，开始对op_shardedwq中的消息进行处理。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    \-OSD::init()
        |-OSD::load_pgs()
        |   |-ObjectStore::list_collections()
        |   |-OSD::_open_lock_pg()
        |   |-ObjectStore::open_collections()
        |   \-PG::read_state()
        \-osd_op_tp.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-请求处理过程&quot;&gt;&lt;strong&gt;2. 请求处理过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来分析一下ms_pipe_read中的请求消息分发流程，其栈心处理函数为OSD::ms_fast_dispatch：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OSD::ms_fast_dispatch()
    |-OpTracker::create_request()
    \-OSD::dispatch_session_waiting()
        \-OSD::enqueue_op()
            \-ShardedOpWQ::queue()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

void OSD::ms_fast_dispatch(Message *m)
{
    /*将消息封装成op*/
    OpRequestRef op = op_tracker.create_request&amp;lt;OpRequest, Message*&amp;gt;(m);
    ...
    
    if (m-&amp;gt;get_connection()-&amp;gt;has_features(CEPH_FEATUREMASK_RESEND_ON_SPLIT) ||
        m-&amp;gt;get_type() != CEPH_MSG_OSD_OP) {
        // queue it directly
        ...
    } else {
        // legacy client, and this is an MOSDOp (the *only* fast dispatch
        // message that didn't have an explicit spg_t); we need to map
        // them to an spg_t while preserving delivery order.

        /*将op放入当前连接的会话上下文中，待获取到OSDMap后进行处理*/
        Session *session = static_cast&amp;lt;Session*&amp;gt;(m-&amp;gt;get_connection()-&amp;gt;get_priv());
        if (session) {
            {
                Mutex::Locker l(session-&amp;gt;session_dispatch_lock);
                op-&amp;gt;get();
                session-&amp;gt;waiting_on_map.push_back(*op);
                OSDMapRef nextmap = service.get_nextmap_reserved();
                dispatch_session_waiting(session, nextmap);
                service.release_map(nextmap);
            }
            session-&amp;gt;put();
        }
    } 
}

void OSD::dispatch_session_waiting(Session *session, OSDMapRef osdmap)
{
    /*遍历session中waiting_on_map中的每个op进行处理*/
    auto i = session-&amp;gt;waiting_on_map.begin();
    while (i != session-&amp;gt;waiting_on_map.end()) {
        OpRequestRef op = &amp;amp;(*i);
        const MOSDFastDispatchOp *m = static_cast&amp;lt;const MOSDFastDispatchOp*&amp;gt;(op-&amp;gt;get_req());
        ...
        session-&amp;gt;waiting_on_map.erase(i++);
        op-&amp;gt;put();

        spg_t pgid;
        if (m-&amp;gt;get_type() == CEPH_MSG_OSD_OP) {
            /*根据消息中记录的pg(对象名称的hash值)计算实际pg(根据pg数取余)*/
            pg_t actual_pgid = osdmap-&amp;gt;raw_pg_to_pg(static_cast&amp;lt;const MOSDOp*&amp;gt;(m)-&amp;gt;get_pg());
            if (!osdmap-&amp;gt;get_primary_shard(actual_pgid, &amp;amp;pgid)) {
                continue;
            }
        } else {
            pgid = m-&amp;gt;get_spg();
        }
        /*依据实际pgid将消息放入片式队列*/
        enqueue_op(pgid, op, m-&amp;gt;get_map_epoch());
    }
    ...
}

void OSD::enqueue_op(spg_t pg, OpRequestRef&amp;amp; op, epoch_t epoch)
{
    ...
    op_shardedwq.queue(make_pair(pg, PGQueueable(op, epoch)));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接下来我们看一下tp_osd_tp线程是如何处理分片中的请求，线程处理的核心函数是ShardedOpWQ::_process，其调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ShardedOpWQ::_process()
    |-OpQueue&amp;lt;&amp;gt;::dequeue()
    |-OSD::_look_up_pg()
    \-PGQueueable::run()
        \-PrimrayLogPG::do_request()
            \-PrimaryLogPG::do_op()
                \-PrimaryLogPG::execute_ctx()
                    |-PrimaryLogPG::prepare_transaction()
                    |   \-PrimaryLogPG::do_osd_ops()
                    \-PrimaryLogPG::issue_repop()
                        \-ReplicatedBackend::submit_transaction()
                            |-ReplicatedBackend::generate_transaction()
                            |-ReplicatedBackend::issue_op()
                            \-PrimaryLogPG::queue_transactions()
                                \-ObjectStore::queue_transactions()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

void OSD::ShardedOpWQ::_process(uint32_t thread_index, heartbeat_handle_d *hb)
{
    /*通过线程号取余的方式找到每个线程片时的分片，可能存在两个线程处理一个分片的情况*/
    uint32_t shard_index = thread_index % num_shards;
    ShardData *sdata = shard_list[shard_index];
    
    /*通过锁机制同步请求的放入与取出，以及多个线程并发取出的场景*/
    sdata-&amp;gt;sdata_op_ordering_lock.Lock();
    ...

    /*取出一个操作对象到item*/
    pair&amp;lt;spg_t, PGQueueable&amp;gt; item = sdata-&amp;gt;pqueue-&amp;gt;dequeue();
    ...

    sdata-&amp;gt;sdata_op_ordering_lock.Unlock();
    ...

    /*查找item对应的pg对象并为该pg加锁，类型为PrimaryLogPG*/
    pg = osd-&amp;gt;_lookup_lock_pg(item.first);
    ...

    sdata-&amp;gt;sdata_op_ordering_lock.Lock();
    ...

    /*对于查找到的pg对象会放入一个临时的slot结构中进行同步加锁*/
    qi = slot.to_process.front();
    slot.to_process.pop_front();
    ...

    sdata-&amp;gt;sdata_op_ordering_lock.Unlock();
    ...

    /*调用实际的处理函数，最终将执行PrimaryLogPG::do_request()*/
    qi-&amp;gt;run(osd, pg, tp_handle);
    ...

    /*解锁pg*/
    pg-&amp;gt;unlock();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  每个PG在处理op时，会为当前op以及op操作的对象生成一个context，用来保存此次操作相关的信息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/PrimaryLogPG.cc:

void PrimaryLogPG::do_request(OpRequestRef&amp;amp; op, ThreadPool::TPHandle &amp;amp;handle)
{
    .../*针对op进行一系列的有效性判断*/

    switch (op-&amp;gt;get_req()-&amp;gt;get_type()) {
    /*针对不同的请求类型进行不同的处理*/
    case CEPH_MSG_OSD_OP:
        do_op(op);
        break;
    ...
    }
}

void PrimaryLogPG::do_op(OpRequestRef&amp;amp; op)
{
    .../*还是一堆锁碎的状态检查*/
    
    MOSDOp *m = static_cast&amp;lt;MOSDOp*&amp;gt;(op-&amp;gt;get_nonconst_req());
    ...
    
    /*在当前PG中查找或生成一个新的对象上下文，用来保存对象修改的相关信息*/
    ObjectContextRef obc;
    int r = find_object_context(
            oid, &amp;amp;obc, can_create,
            m-&amp;gt;has_flag(CEPH_OSD_FLAG_MAP_SNAP_CLONE),
            &amp;amp;missing_oid);
    ...
    
    /*生成一个新op上下文*/
    OpContext *ctx = new OpContext(op, m-&amp;gt;get_reqid(), &amp;amp;m-&amp;gt;ops, obc, this);
    ...

    /*执行该op上下文*/
    execute_ctx(ctx);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PG在执行op上下文时，会使用事务的方式保证多个修改操作的原子性。另外事务具有多个层级，PG中使用PGTransaction；底层ObjectStore中会使用ObjectStore::Transaction进行进一步封装。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/PrimaryLogPG.cc:

void PrimaryLogPG::execute_ctx(OpContext *ctx)
{
    OpRequestRef op = ctx-&amp;gt;op; /*上下文上包含的op*/
    const MOSDOp *m = static_cast&amp;lt;const MOSDOp*&amp;gt;(op-&amp;gt;get_req()); /*op对应的原始请求消息*/
    ObjectContextRef obc = ctx-&amp;gt;obc; /*op操作的对象上下文*/
    const hobject_t&amp;amp; soid = obc-&amp;gt;obs.oi.soid; /*对象id*/

    ctx-&amp;gt;op_t.reset(new PGTransaction); /*设置一个新的PGTransaction事务对象*/
    ...

    /*将上下文中的多个子操作放入到事务中，普通读写只有一个子操作*/
    int result = prepare_transaction(ctx);
    ...

    /*对于读操作，在prepare_transaction中将完成对象的读取，这里将直接返回响应消息给客户端*/
    if ((ctx-&amp;gt;op_t-&amp;gt;empty() || result &amp;lt; 0) &amp;amp;&amp;amp; !ctx-&amp;gt;update_log_only) {
        ...
        complete_read_ctx(result, ctx);
        return;
    }
    
    /*以下均针对写操作*/
    
    /*注册所有复本写操作均完成后的回调函数，该函数将发送响应给客户端*/
    ctx-&amp;gt;register_on_commit(...);
    ...

    /*生成一组复本操作，并将这些操作发射出去：本地复本将写到后端ObjectStore，远端复本将通过网络消息发送并等待响应*/
    ceph_tid_t rep_tid = osd-&amp;gt;get_tid();

    RepGather *repop = new_repop(ctx, obc, rep_tid);

    issue_repop(repop, ctx);
    eval_repop(repop);
    repop-&amp;gt;put();
}

int PrimaryLogPG::prepare_transaction(OpContext *ctx)
{
    ...
    int result = do_osd_ops(ctx, *ctx-&amp;gt;ops);
    ...
｝

int PrimaryLogPG::do_osd_ops(OpContext *ctx, vector&amp;lt;OSDOp&amp;gt;&amp;amp; ops)
{
    ...
    /*循环处理每个子操作*/
    for (vector&amp;lt;OSDOp&amp;gt;::iterator p = ops.begin(); p != ops.end(); ++p, ctx-&amp;gt;current_osd_subop_num++) {
        OSDOp&amp;amp; osd_op = *p;
        ceph_osd_op&amp;amp; op = osd_op.op;

        switch (op.op) {
        ...
        case CEPH_OSD_OP_READ:
            ++ctx-&amp;gt;num_read;
            result = do_read(ctx, osd_op);
            break;
        ...
        case CEPH_OSD_OP_WRITE:
            ++ctx-&amp;gt;num_write;
            t-&amp;gt;write(soid, op.extent.offset, op.extent.length, osd_op.indata, op.flags);
            break;
        ...
        }
    }
}

int PrimaryLogPG::do_read(OpContext *ctx, OSDOp&amp;amp; osd_op)
{
    ...

    /*针对读请求，通过后端同步读接口直接读取对象内容*/
    int r = pgbackend-&amp;gt;objects_read_sync(
            soid, op.extent.offset, op.extent.length, op.flags, &amp;amp;osd_op.outdata);
    ...
}

ceph/src/osd/PGTransaction.h:

/*针对写操作，会将几个关键参数放入事务的buffer_updates中*/
void write(
    const hobject_t &amp;amp;hoid,         ///&amp;lt; [in] object to write
    uint64_t off,                  ///&amp;lt; [in] off at which to write
    uint64_t len,                  ///&amp;lt; [in] len to write from bl
    bufferlist &amp;amp;bl,                ///&amp;lt; [in] bl to write will be claimed to len
    uint32_t fadvise_flags = 0     ///&amp;lt; [in] fadvise hint
) {
    auto &amp;amp;op = get_object_op_for_modify(hoid);
    op.buffer_updates.insert(
        off,
        len,
        ObjectOperation::BufferUpdate::Write{bl, fadvise_flags});
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  针对写操作的多复本操作，将会提交给ReplicatedBackend对象进行处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/PrimaryLogPG.cc:

void PrimaryLogPG::issue_repop(RepGather *repop, OpContext *ctx)
{
    ...

    /*生成复本操作全部完成后的回调对象*/
    Context *on_all_commit = new C_OSD_RepopCommit(this, repop);
    Context *on_all_applied = new C_OSD_RepopApplied(this, repop);
    ...

    /*递交给ReplicatedBackend对象处理*/
    pgbackend-&amp;gt;submit_transaction(
        soid,
        ctx-&amp;gt;delta_stats,
        ctx-&amp;gt;at_version,
        std::move(ctx-&amp;gt;op_t),
        pg_trim_to,
        min_last_complete_ondisk,
        ctx-&amp;gt;log,
        ctx-&amp;gt;updated_hset_history,
        onapplied_sync,
        on_all_applied,
        on_all_commit,
        repop-&amp;gt;rep_tid,
        ctx-&amp;gt;reqid,
        ctx-&amp;gt;op);
}

ceph/src/osd/ReplicatedBackend.cc:

void ReplicatedBackend::submit_transaction(
    const hobject_t &amp;amp;soid,
    const object_stat_sum_t &amp;amp;delta_stats,
    const eversion_t &amp;amp;at_version,
    PGTransactionUPtr &amp;amp;&amp;amp;_t,
    const eversion_t &amp;amp;trim_to,
    const eversion_t &amp;amp;roll_forward_to,
    const vector&amp;lt;pg_log_entry_t&amp;gt; &amp;amp;_log_entries,
    boost::optional&amp;lt;pg_hit_set_history_t&amp;gt; &amp;amp;hset_history,
    Context *on_local_applied_sync,
    Context *on_all_acked,
    Context *on_all_commit,
    ceph_tid_t tid,
    osd_reqid_t reqid,
    OpRequestRef orig_op)
{
    ObjectStore::Transaction op_t;
    ...
    
    /*将PGTransaction对象封装到ObjectStore:: Transaction中*/
    generate_transaction(
        t,
        coll,
        (get_osdmap()-&amp;gt;require_osd_release &amp;lt; CEPH_RELEASE_KRAKEN),
        log_entries,
        &amp;amp;op_t,
        &amp;amp;added,
        &amp;amp;removed);

    /*生成一个新的代表当前操作对象的InProgressOp*/
    InProgressOp &amp;amp;op = in_progress_ops.insert(
        make_pair(
            tid,
            InProgressOp(
                tid, on_all_commit, on_all_acked,
                orig_op, at_version)
        )
    ).first-&amp;gt;second;

    /*记录当前操作需要等待哪些OSD返回结果，包含本地OSD和远端OSD*/
    op.waiting_for_applied.insert(
        parent-&amp;gt;get_actingbackfill_shards().begin(), parent-&amp;gt;get_actingbackfill_shards().end());
    op.waiting_for_commit.insert(
        parent-&amp;gt;get_actingbackfill_shards().begin(), parent-&amp;gt;get_actingbackfill_shards().end());

    /*对于发往远端OSD的复本请求，通过网络发送CEPH_MSG_REPOP消息*/
    issue_op(
        soid,
        at_version,
        tid,
        reqid,
        trim_to,
        at_version,
        added.size() ? *(added.begin()) : hobject_t(),
        removed.size() ? *(removed.begin()) : hobject_t(),
        log_entries,
        hset_history,
        &amp;amp;op,
        op_t);

    /*对于发送本地OSD的复本请求*/
    /*先注册本地完成后的回调*/
    op_t.register_on_applied_sync(on_local_applied_sync);/*写到数据区并同步回刷之后触发*/
    op_t.register_on_applied(
        parent-&amp;gt;bless_context(new C_OSD_OnOpApplied(this, &amp;amp;op)));/*写到数据区之后触发*/
    op_t.register_on_commit(
        parent-&amp;gt;bless_context(new C_OSD_OnOpCommit(this, &amp;amp;op)));/*提交到日志区之后触发*/
    
    /*再将事务发送给后端存储池进行处理*/
    vector&amp;lt;ObjectStore::Transaction&amp;gt; tls;
    tls.push_back(std::move(op_t));

    parent-&amp;gt;queue_transactions(tls, op.op); /*最终会调用不同ObjectStore的queue_transactions函数*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  OSD模块整体分析完毕，后续我们将继续分析FileStore的实现原理。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/04/RBD-OSD-2&quot;&gt;【Rados Block Device】七、OSD原理分析－OSD模块&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Apr 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/RBD-OSD-2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/RBD-OSD-2/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】六、OSD原理分析－SimpleMessenger模块</title>
        <description>&lt;p&gt;  OSD进程通过网络对Client提供服务，因此网络层是OSD中的基础层。本篇博文将讨论ceph中传统的SimpleMessenger实现原理。&lt;/p&gt;

&lt;h3 id=&quot;simplemessenger模块中对象流程概览&quot;&gt;SimpleMessenger模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  如果将OSD进程的网络服务模式配置成SimpleMessenger，那么它采用的是POSIX标准网络接口来实现网络功能。也就是说，此时我们的OSD服务从代码实现流程上来说就是通过socket()-&amp;gt;bind()-&amp;gt;listen()-&amp;gt;accept()进行连接建立，随后再通过每个连接进行网络消息的收发。虽然SimpleMessenger在实现过程中融入一些设计模式的抽象，但是抓住以上POSIX网络编程核心流程后将便于大家理解其实现机理。&lt;/p&gt;

&lt;p&gt;  我们先整体来看一下SimpleMessenger的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_simplemessenger.jpg&quot; height=&quot;450&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们知道一个ceph集群中通常会有两个网络平面：public和cluster，那么每个OSD进程中就会对应产生两个messenger对象ms_public和ms_cluster。对于每个messenger对象(这里其实为SimpleMessenger对象)，会产生一个名为ms_accepter的线程，messenger对象通过accepter成员变量指向该线程。ms_accepter线程是在完成socket()-&amp;gt;bind()-&amp;gt;listen()动作之后产生，它的主要作用就是一直监听Client端的网络连接请求。&lt;/p&gt;

&lt;p&gt;  当某一个Client发起连接请求后，ms_accepter将调用accept()接受请求，并为该连接产生一个pipe对象。pipe对象是对TCP socket连接的封装，可以实现故障重连等可靠性增强特性。每个pipe对象会产生两个线程：一个叫ms_pipe_read线程，它一直在监听socket连接中的消息，如果发现有消息它将取出消息并将消息放入in_q中进行分发处理；另一个叫ms_pipe_write线程，它一直在等待out_q中被放入发送消息，如果发现out_q中有消息，它将把消息取出并通过socket连接将其发送出去。&lt;/p&gt;

&lt;p&gt;  在消息接收处理的过程中，in_q队列指向的其实是SimpleMessenger对象中的DispatchQueue，也就是说对于同一个messenger中的多个pipe，它们接收的消息将被放入到同一个队列中等待处理。DispatchQueue分发消息时如果发现该消息可以被快速处理(fast dispatch)时，会将该消息直接分发给SimpleMessenger，由SimpleMessenger将消息分发给其内部的多个Dispatcher对象进行最终的消息处理。这里OSD模块中的OSD对象就是属于SimpleMessenger的一个Dispatcher，OSD对消息的处理属于OSD模块的内容，我们将在后续博文中介绍。DispatchQueue如果发现该消息无法被快速处理，则会将该消息交给DispatchQueue对应的DispatchThread处理。DispatchThread线程取出消息后会传递给messenger通过ms_deliver_dispatch进行普通处理，其实最终也会交给Dispatcher(如OSD对象)处理，只不过是在DispatchThread线程中被处理(fast dispatch是在ms_pipe_read线程中被处理，请大家注意对比)。&lt;/p&gt;

&lt;h3 id=&quot;simplemessenger模块中类的概览&quot;&gt;SimpleMessenger模块中类的概览&lt;/h3&gt;

&lt;p&gt;  在了解了SimpleMessenger的实现流程后，我们再来看看它的类定义，从而理解它是如何实现抽象，以支持未来更灵活的扩展：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_simplemessenger_class.jpg&quot; height=&quot;750&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  图中标红的类(Messenger、Dispatcher、DispatchQueue、Connection)属于Messenger模块抽象类(不属于某一种网络实现模式)，不同的网络实现模式可以继承它们实现各自特有的功能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Messenger类是SimpleMessenger、AsyncMessenger、XioMessenger的父类，它描绘了messenger对象的功能框架。每个messenger对象包含两个Dispatcher链表，dispatchers代表普通处理对象，fast_dispatchers代表快速处理对象，以便实现针对不同消息类型采用不同处理方式的功能。Dispatcher对象是在初始流程中，通过add_dispatcher_header方法被加入到messenger对应的链表。create、bind、start、ready方法实现messenger对象的创建和初始化；wait等待messenger生命周期结束；shutdown关闭messenger对象。send_message实现了通过messenger对象发送一个消息的功能。ms_fast_dispatch实现消息的快速分发；ms_deliver_dispatch实现消息的普通分发；最终都将分发给messenger中的dispatchers进行处理。&lt;/li&gt;
    &lt;li&gt;Dispatcher类是接收消息的处理者。不同模块可以实现各自不同的Dispatcher，以实现对消息的不同处理逻辑。只要在初始化时通过messenger对象的add_dispatcher_*方法被加入到messenger中，便可保证该Dispatcher可以接收到消息并进行处理。&lt;/li&gt;
    &lt;li&gt;DispatchQueue类实现了一个分发队列。用户通过enqueue操作将消息放入队列，该队列可以按优先级对消息进行排序(PrioritizeQueue)。队列会产生一个专门的DispatchThread线程，由该线程负责从PrioritizedQueue中取出消息进行分发处理。此外，DispatchQueue也可通过can_fast_dispatch判断消息是否可以被快速处理，如果可以被快速处理，则直接调用fast_dispatch进行分发处理，否则调用enqueue进队列交由DispatchThread处理。&lt;/li&gt;
    &lt;li&gt;Connection类是对网络连接的抽象。子类通过实现send_message方法提供不同的网络发送方案。&lt;/li&gt;
    &lt;li&gt;SimpleMessenger类继承Messenger类实现基于POSIX网络接口的网络信使功能。它将实现Messenger类中的bind、start、ready方法，以完成网络socket的初始化。独有的Accepter对象将产生一个独立的线程监听网络连接请求。每当接受一个新连接时，都会生成一个新的Pipe对象，并通过add_accept_pipe方法将其加入到pipes列表中。SimpleMessenger包含了一个DispatchQueue来实现消息的普通分发和快速分发。&lt;/li&gt;
    &lt;li&gt;Pipe类代表一个连接会话，每个连接会产生一个reader_thread线程(入口函数为reader()方法)和一个writer_thread线程(入口函数为writer())。reader方法负责从网络层接收消息放入in_q并进行顶层分发处理；writer方法负责将out_q中消息通过网络发送。accept方法在接受连接时调用，connect方法在发起连接请求时调用。&lt;/li&gt;
    &lt;li&gt;Thread类是Common模块中的公共类，用来生成线程。create方法用来创建线程对象；entry方法为新线程的入口函数；join方法在等待子线程结束时调用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;p&gt;  有了对SimpleMessenger模块中对象、流程和类的整体认识之后，我们再结合代码来深入理解其实现细节。&lt;/p&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SimpleMessenger模块的初始化在OSD进程的main函数中完成，整体调用栈如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    |-Messenger::create()
    |   \-SimpleMessenger::SimpleMessenger()
    |-Messenger::bind() -&amp;gt; SimpleMessenger::bind()
    |   \-Accepter::bind()
    |       |-::socket()
    |       |-::bind()
    |       \-::listen()
    |-OSD::OSD()
    |-Messenger::start() -&amp;gt; SimpleMessenger::start()
    |-OSD::init()
    |   \-Messenger::add_dispatcher_head()
    |       \-Messenger::ready() -&amp;gt; SimpleMessenger::ready()
    |           \-Accepter::start()
    \-Messenger::wait() -&amp;gt; SimpleMessenger::wait()
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们先来看看ceph_osd.cc中main函数里和Messenger初始化相关的部分代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/ceph_osd.cc:

int main(int argc, const char **argv)
{
    ...

    /*这里我们假设public_msgr_type和cluster_msgr_type都为Simple*/
    std::string public_msgr_type = g_conf-&amp;gt;ms_public_type.empty() ? g_conf-&amp;gt;get_val&amp;lt;std::string&amp;gt;(&quot;ms_type&quot;) : g_conf-&amp;gt;ms_public_type;
    std::string cluster_msgr_type = g_conf-&amp;gt;ms_cluster_type.empty() ? g_conf-&amp;gt;get_val&amp;lt;std::string&amp;gt;(&quot;ms_type&quot;) : g_conf-&amp;gt;ms_cluster_type;

    /*根据不同类型创建messenger对象，这里将创建两个SimpleMessenger对象*/
    Messenger *ms_public = Messenger::create(g_ceph_context, public_msgr_type,
        entity_name_t::OSD(whoami), &quot;client&quot;,
        getpid(),
        Messenger::HAS_HEAVY_TRAFFIC |
        Messenger::HAS_MANY_CONNECTIONS);
    Messenger *ms_cluster = Messenger::create(g_ceph_context, cluster_msgr_type,
        entity_name_t::OSD(whoami), &quot;cluster&quot;,
        getpid(),
        Messenger::HAS_HEAVY_TRAFFIC |
        Messenger::HAS_MANY_CONNECTIONS);

    ...

    /*为messenger对象绑定IP地址，最终会调用Accepter::bind函数，由它调用系统的socket()、bind()和listen()函数*/
    r = ms_public-&amp;gt;bind(g_conf-&amp;gt;public_addr);
    ...
    r = ms_cluster-&amp;gt;bind(g_conf-&amp;gt;cluster_addr);

    ...

    /*将创建的ms_public和ms_cluster传递给OSD构造函数，建立一个新的OSD对象*/
    osd = new OSD(g_ceph_context,
                store,
                whoami,
                ms_cluster,
                ms_public,
                ms_hb_front_client,
                ms_hb_back_client,
                ms_hb_front_server,
                ms_hb_back_server,
                ms_objecter,
                &amp;amp;mc,
                g_conf-&amp;gt;osd_data,
                g_conf-&amp;gt;osd_journal);

    /*启动messenger对象，针对SimpleMessenger，其内部细节我们暂不用关心*/
    ms_public-&amp;gt;start();
    ...
    ms_cluster-&amp;gt;start();

    /*OSD执行初始化，下文将展开*/
    osd-&amp;gt;init();

    ...

    /*整个初始化动作完成，OSD主线程进入等待状态*/
    ms_public-&amp;gt;wait();
    ms_cluster-&amp;gt;wait();
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/Messenger.cc:

Messenger *Messenger::create(CephContext *cct, const string &amp;amp;type,
        entity_name_t name, string lname,
        uint64_t nonce, uint64_t cflags)
{
    int r = -1;
    if (type == &quot;random&quot;) {
        static std::random_device seed;
        static std::default_random_engine random_engine(seed());
        static Spinlock random_lock;

        std::lock_guard&amp;lt;Spinlock&amp;gt; lock(random_lock);
        std::uniform_int_distribution&amp;lt;&amp;gt; dis(0, 1);
        r = dis(random_engine);
    }
    if (r == 0 || type == &quot;simple&quot;)
        return new SimpleMessenger(cct, name, std::move(lname), nonce);
    else if (r == 1 || type.find(&quot;async&quot;) != std::string::npos)
        return new AsyncMessenger(cct, name, type, std::move(lname), nonce);
#ifdef HAVE_XIO
    else if ((type == &quot;xio&quot;) &amp;amp;&amp;amp;
            cct-&amp;gt;check_experimental_feature_enabled(&quot;ms-type-xio&quot;))
    return new XioMessenger(cct, name, std::move(lname), nonce, cflags);
#endif
    lderr(cct) &amp;lt;&amp;lt; &quot;unrecognized ms_type '&quot; &amp;lt;&amp;lt; type &amp;lt;&amp;lt; &quot;'&quot; &amp;lt;&amp;lt; dendl;
    return nullptr;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们再深入看看OSD初始化过程中与Messenger相关的部分代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

int OSD::init()
{
    ...

    /*将OSD对象自身加到public messenger和cluster messenger的dispatchers中*/
    client_messenger-&amp;gt;add_dispatcher_head(this);
    cluster_messenger-&amp;gt;add_dispatcher_head(this);

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/Messenger.h:

void add_dispatcher_head(Dispatcher *d) { 
    bool first = dispatchers.empty(); /*是否为添加到dispatchers链表中的第一个元素？*/
    dispatchers.push_front(d); /*加入到dispatchers*/
    if (d-&amp;gt;ms_can_fast_dispatch_any()) /*如果可以进行fast dispatch则加入到fast_dispatchers中*/
        fast_dispatchers.push_front(d);
    if (first)
        ready(); /*如果是首个加入到dispatchers中的对象，则调用messenger对象的ready()*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

void SimpleMessenger::ready()
{
    ldout(cct,10) &amp;lt;&amp;lt; &quot;ready &quot; &amp;lt;&amp;lt; get_myaddr() &amp;lt;&amp;lt; dendl;
    dispatch_queue.start(); /*拉起dispatch_queue对应的dispatch_thread*/

    lock.Lock();
    if (did_bind)
        accepter.start(); /*拉起ms_accepter线程*/
    lock.Unlock();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Accept.cc:

int Accepter::start()
{
    ldout(msgr-&amp;gt;cct,1) &amp;lt;&amp;lt; __func__ &amp;lt;&amp;lt; dendl;

    // start thread
    create(&quot;ms_accepter&quot;);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-连接建立过程&quot;&gt;&lt;strong&gt;2. 连接建立过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SimpleMessenger对象初始化完成后，将拉起一个ms_accepter线程处理Client端的连接请求，该线程入口函数为Accepter::entry。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Accepter.cc:

void *Accepter::entry()
{
    int errors = 0;
    int ch;

    struct pollfd pfd[2];
    memset(pfd, 0, sizeof(pfd));

    pfd[0].fd = listen_sd;
    pfd[0].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
    pfd[1].fd = shutdown_rd_fd;
    pfd[1].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;

    while (!done) {
        int r = poll(pfd, 2, -1); /*通过poll系统调用等待Client连接请求*/

        ...

        if (done) break;

        // accept
        sockaddr_storage ss;
        socklen_t slen = sizeof(ss);
        int sd = ::accept(listen_sd, (sockaddr*)&amp;amp;ss, &amp;amp;slen); /*收到请求后，accept该连接*/
        if (sd &amp;gt;= 0) {
            ...
            msgr-&amp;gt;add_accept_pipe(sd); /*向SimpleMessenger对象中添加pipe*/
        } else {
            ...
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

Pipe *SimpleMessenger::add_accept_pipe(int sd)
{
    lock.Lock();
    Pipe *p = new Pipe(this, Pipe::STATE_ACCEPTING, NULL);
    p-&amp;gt;sd = sd;
    p-&amp;gt;pipe_lock.Lock();
    p-&amp;gt;start_reader(); /*拉起pipe对象的ms_pipe_read线程*/
    p-&amp;gt;pipe_lock.Unlock();
    pipes.insert(p);
    accepting_pipes.insert(p);
    lock.Unlock();
    return p;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.cc:

void Pipe::start_reader()
{
    if (reader_needs_join) {
        reader_thread.join();
        reader_needs_join = false;
    }
    reader_running = true;
    reader_thread.create(&quot;ms_pipe_read&quot;, msgr-&amp;gt;cct-&amp;gt;_conf-&amp;gt;ms_rwthread_stack_bytes);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;3-消息接收与分发过程&quot;&gt;&lt;strong&gt;3. 消息接收与分发过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  每个pipe对象的ms_pipe_read线程被拉起后，会进行会话的协商过程(可以回顾下前期Client端RBD的messenger分析博文)；完成后将处于等待接收消息的状态：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.cc:

void Pipe::reader()
{
    pipe_lock.Lock();

    if (state == STATE_ACCEPTING) {
        /*执行会话协商过程，协商成功后会拉起ms_pipe_write线程*/
        accept();
    }

    while (state != STATE_CLOSED &amp;amp;&amp;amp;
            state != STATE_CONNECTING) {

        // sleep if (re)connecting
        if (state == STATE_STANDBY) {
            /*如果pipe状态为STANDBY，说明底层连接故障且暂无消息处理，则进入睡眠*/
            cond.Wait(pipe_lock);
            continue;
        }

        pipe_lock.Unlock();

        char tag = -1;
        /*先从网络连接中读取一个字节的tag*/
        if (tcp_read((char*)&amp;amp;tag, 1) &amp;lt; 0) {
            pipe_lock.Lock();
            fault(true);
            continue;
        }
        
        /*根据tag值进行不同的处理动作*/
        if (tag == CEPH_MSGR_TAG_KEEPALIVE) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_KEEPALIVE2) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_ACK) {
            ...
            continue;
        }
        else if (tag == CEPH_MSGR_TAG_MSG) {
            /*针对消息，先从网络中读取消息内容到m中*/
            Message *m = 0;
            int r = read_message(&amp;amp;m, auth_handler.get());

            pipe_lock.Lock();
            if (m-&amp;gt;get_seq() &amp;lt;= in_seq) {
                m-&amp;gt;put();
                continue;
            }
            m-&amp;gt;set_connection(connection_state.get());
            ...

            /*对消息进行预处理*/
            in_q-&amp;gt;fast_preprocess(m);
            if (delay_thread) {
                ...
            } else {
                /*如果消息可以被快速处理，则走快速处理流程；否则就enqueue到in_q中交给dispatch_thread处理*/
                if (in_q-&amp;gt;can_fast_dispatch(m)) {
                    reader_dispatching = true;
                    pipe_lock.Unlock();
                    in_q-&amp;gt;fast_dispatch(m);
                    pipe_lock.Lock();
                    reader_dispatching = false;
                    ...
                } else {
                    in_q-&amp;gt;enqueue(m, m-&amp;gt;get_priority(), conn_id);
                }            
            }
        }
        ...
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  消息的快速处理流程可以回头参考前文的对象、流程图。&lt;/p&gt;

&lt;h4 id=&quot;4-消息发送过程&quot;&gt;&lt;strong&gt;4. 消息发送过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  任何模块想通过messenger发送消息时，都可以调用Messenger::send_mesage来完成。对于SimpleMessenger，其实现体位于SimpleMessenger.h，发送是一个异步过程，发送者只会将消息放入Pipe对象的out_q中，随后由ms_pipe_write线程完成向网络协议栈的发送。&lt;/p&gt;

&lt;p&gt;  发送者的调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sender[any thread]
    \-SimpleMessenger::send_message()
        \-SimpleMessenger::_send_message()
            \-SimpleMessenger::submit_message()
                \-Pipe::_send()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.h:

int send_message(Message *m, const entity_inst_t&amp;amp; dest) override {
    return _send_message(m, dest);
}

int send_message(Message *m, Connection *con) {
    return _send_message(m, con);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

int SimpleMessenger::_send_message(Message *m, Connection *con)
{
    //set envelope
    m-&amp;gt;get_header().src = get_myname();

    if (!m-&amp;gt;get_priority()) m-&amp;gt;set_priority(get_default_send_priority());

    submit_message(m, static_cast&amp;lt;PipeConnection*&amp;gt;(con),
                con-&amp;gt;get_peer_addr(), con-&amp;gt;get_peer_type(), false);
    return 0;
}

void SimpleMessenger::submit_message(Message *m, PipeConnection *con,
                const entity_addr_t&amp;amp; dest_addr, int dest_type,
                bool already_locked)
{
    ...
    if (con) {
        Pipe *pipe = NULL;
        bool ok = static_cast&amp;lt;PipeConnection*&amp;gt;(con)-&amp;gt;try_get_pipe(&amp;amp;pipe);
        ...
        while (pipe &amp;amp;&amp;amp; ok) {
            // we loop in case of a racing reconnect, either from us or them
            pipe-&amp;gt;pipe_lock.Lock(); // can't use a Locker because of the Pipe ref
            if (pipe-&amp;gt;state != Pipe::STATE_CLOSED) {
                pipe-&amp;gt;_send(m);
                pipe-&amp;gt;pipe_lock.Unlock();
                pipe-&amp;gt;put();
                return;
            }
        }
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.h:

void _send(Message *m) {
    assert(pipe_lock.is_locked());
    out_q[m-&amp;gt;get_priority()].push_back(m);
    cond.Signal();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  ms_pipe_write线程入口函数为Pipe::writer，其调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pipe::writer()
    |-Pipe::_get_next_outgoing()
    \-Pipe::write_message()
        \-Pipe::do_sendmsg()

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void Pipe::writer()
{
    pipe_lock.Lock();
    while (state != STATE_CLOSED) {
        ...
        Message *m = _get_next_outgoing();
        ...
        const ceph_msg_header&amp;amp; header = m-&amp;gt;get_header();
        const ceph_msg_footer&amp;amp; footer = m-&amp;gt;get_footer();
        bufferlist blist = m-&amp;gt;get_payload();
        blist.append(m-&amp;gt;get_middle());
        blist.append(m-&amp;gt;get_data());
        pipe_lock.Unlock();

        write_message(header, footer, blist);
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/03/RBD-OSD-1&quot;&gt;【Rados Block Device】六、OSD原理分析－SimpleMessenger模块&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/RBD-OSD-1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/RBD-OSD-1/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】五、高精度定时器</title>
        <description>&lt;p&gt;  通过对低精度定时器的分析，我们知道这类定时器的精度是毫秒级的，也就是说存在毫秒级的误差范围。对于像IO超时错误处理这类定时任务，毫秒级的误差完全不算什么问题。然而，对于工业上的许多实时任务，毫秒级的误差是完全不可接受的。因此，基于更高精度的时间硬件(例如TSC和LAPIC Timer)，内核工程师们开发了一套全新的高精度定时器功能(传统基于时间轮的低精度定时器已经很稳定了，与其对它修修补补，还不如新建一套全新的机制)。&lt;/p&gt;

&lt;h3 id=&quot;1-高精度定时器的初始化&quot;&gt;1. 高精度定时器的初始化&lt;/h3&gt;

&lt;p&gt;  高精度定时器的初始化和低精度定时器的初始化有些类似，需要指定到期后的回调函数。然而在内部数据结构的设计上，不同于低精度定时器的时间轮，高精度定时器采用了红黑树(可以高效地实现排序、增删改等操作，内核中有比较成熟稳定的代码实现)。另外，低精度定时器的计时参照是jiffies，而高精度定时器可以采用timekeeper中的多种计时参照，如REAL TIME、MONOTONIC TIME等等。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/**
 * hrtimer_init - initialize a timer to the given clock
 * @timer:	the timer to be initialized
 * @clock_id:	the clock to be used
 * @mode:	timer mode abs/rel
 */
void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
    debug_init(timer, clock_id, mode);
    __hrtimer_init(timer, clock_id, mode);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/hrtimer.h:

/**
 * struct hrtimer - the basic hrtimer structure
 * @node:   timerqueue node, which also manages node.expires,
 *          the absolute expiry time in the hrtimers internal
 *          representation. The time is related to the clock on
 *          which the timer is based. Is setup by adding
 *          slack to the _softexpires value. For non range timers
 *          identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *          The time which was given as expiry time when the timer
 *          was armed.
 * @function:   timer expiry callback function
 * @base:   pointer to the timer base (per cpu and per clock)
 * @state:  state information (See bit values above)
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct hrtimer {
    struct timerqueue_node      node;
    ktime_t                     _softexpires;
    enum hrtimer_restart        (*function)(struct hrtimer *);
    struct hrtimer_clock_base   *base;
    unsigned long               state;
    ...
};

enum hrtimer_mode {
    HRTIMER_MODE_ABS = 0x0,		/* Time value is absolute */
    HRTIMER_MODE_REL = 0x1,		/* Time value is relative to now */
    HRTIMER_MODE_PINNED = 0x02,	/* Timer is bound to CPU */
    HRTIMER_MODE_ABS_PINNED = 0x02,
    HRTIMER_MODE_REL_PINNED = 0x03,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过上面的代码和注释，我们可以看到，高精度定时器初始化时可以指定计时参照对象(clock_id)和计时模式(采用绝对计时或相对计时)。高精度定时器内部结构中的node即是在红黑树中的挂接对象，base指向每个CPU针对不同计时参照对象的全局数据结构，其内部包含一棵红黑树。__hrtimer_init的具体实现比较简单：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
    struct hrtimer_cpu_base *cpu_base;
    int base;

    memset(timer, 0, sizeof(struct hrtimer));

    cpu_base = &amp;amp;__raw_get_cpu_var(hrtimer_bases); /*获取当前CPU的hrtimer_cpu_base对象*/

    if (clock_id == CLOCK_REALTIME &amp;amp;&amp;amp; mode != HRTIMER_MODE_ABS) /*REALTIME只支持绝对模式*/
        clock_id = CLOCK_MONOTONIC;

    base = hrtimer_clockid_to_base(clock_id); /*索引计时参照*/
    timer-&amp;gt;base = &amp;amp;cpu_base-&amp;gt;clock_base[base];
    timerqueue_init(&amp;amp;timer-&amp;gt;node); /*初始化红黑树节点*/

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-高精度定时器的启动&quot;&gt;2. 高精度定时器的启动&lt;/h3&gt;

&lt;p&gt;  初始化完成并指定回调处理函数后，我们通过hrtimer_start函数可以启动一个定时器：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/**
 * hrtimer_start - (re)start an hrtimer on the current CPU
 * @timer:  the timer to be added
 * @tim:    expiry time
 * @mode:   expiry mode: absolute (HRTIMER_MODE_ABS) or
 *          relative (HRTIMER_MODE_REL)
 *
 * Returns:
 *  0 on success
 *  1 when the timer was active
 */
int hrtimer_start(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode)
{
    return __hrtimer_start_range_ns(timer, tim, 0, mode, 1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  高精度定时器允许有一个纳秒级别的误差，由__hrtimer_start_range_ns的delta_ns参数指明：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
    unsigned long delta_ns, const enum hrtimer_mode mode, int wakeup)
{
    struct hrtimer_clock_base *base, *new_base;
    unsigned long flags;
    int ret, leftmost;

    base = lock_hrtimer_base(timer, &amp;amp;flags); /*锁定该timer对应的hrtimer_clock_base对象*/

    /* Remove an active timer from the queue: */
    ret = remove_hrtimer(timer, base);

    if (mode &amp;amp; HRTIMER_MODE_REL) {
        tim = ktime_add_safe(tim, base-&amp;gt;get_time());
        ...
    }

    hrtimer_set_expires_range_ns(timer, tim, delta_ns); /*设置定时器内部的超时时间*/

    /* Switch the timer base, if necessary: */
    new_base = switch_hrtimer_base(timer, base, mode &amp;amp; HRTIMER_MODE_PINNED);

    leftmost = enqueue_hrtimer(timer, new_base); /*将定时器加入到对应hrtimer_clock_base的红黑树中*/

    if (leftmost &amp;amp;&amp;amp; new_base-&amp;gt;cpu_base == &amp;amp;__get_cpu_var(hrtimer_bases)
        &amp;amp;&amp;amp; hrtimer_enqueue_reprogram(timer, new_base)) { /*如果当前定时器是红黑树中最早到期的定时器，则重新设置clock event device的oneshot计数。注，高精度定时器正常工作时，会将clock event device的工作模式切换到oneshot*/
        ...
    }

    unlock_hrtimer_base(timer, &amp;amp;flags);

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-切换到高精度模式&quot;&gt;3. 切换到高精度模式&lt;/h3&gt;

&lt;p&gt;  内核正常启动后首先工作在低精度模式，然而在时钟中断的处理中，内核会检测是否具备切换到高精度的条件，如果各条件均满足，则切换到高精度模式工作。时钟中断中在处理低精度时钟时，通过hrtimer_run_pending()完成切换动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    hrtimer_run_pending();

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies))
        __run_timers(base);
}

void hrtimer_run_pending(void)
{
    if (hrtimer_hres_active()) /*如果已经切换到高精度模式则返回*/
        return;

    if (tick_check_oneshot_change(!hrtimer_is_hres_enabled())) /*判断是否具备切换到高精度的条件，如时钟源精度是否满足、是否支持oneshot模式*/
        hrtimer_switch_to_hres();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  如果切换条件均满足，则通过hrtimer_switch_to_hres切换到高精度模式：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

static int hrtimer_switch_to_hres(void)
{
    int i, cpu = smp_processor_id();
    struct hrtimer_cpu_base *base = &amp;amp;per_cpu(hrtimer_bases, cpu);
    unsigned long flags;

    if (base-&amp;gt;hres_active)
        return 1;

    local_irq_save(flags);

    if (tick_init_highres()) { /*将tick模式切换到oneshot模式并重新指定中断处理函数*/
        local_irq_restore(flags);
        printk(KERN_WARNING &quot;Could not switch to high resolution &quot;
        &quot;mode on CPU %d\n&quot;, cpu);
        return 0;
    }
    base-&amp;gt;hres_active = 1;
    for (i = 0; i &amp;lt; HRTIMER_MAX_CLOCK_BASES; i++)
    base-&amp;gt;clock_base[i].resolution = KTIME_HIGH_RES;

    tick_setup_sched_timer(); /*设置一个专门的调度定时器，用来处理调度任务*/
    /* &quot;Retrigger&quot; the interrupt to get things going */
    retrigger_next_event(NULL);
    local_irq_restore(flags);
    return 1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-oneshot.c:

/**
 * tick_init_highres - switch to high resolution mode
 *
 * Called with interrupts disabled.
 */
int tick_init_highres(void)
{
    return tick_switch_to_oneshot(hrtimer_interrupt); /*高精度模式下时钟中断处理函数为hrtimer_interrupt*/
}

/**
 * tick_switch_to_oneshot - switch to oneshot mode
 */
int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))
{
    struct tick_device *td = &amp;amp;__get_cpu_var(tick_cpu_device);
    struct clock_event_device *dev = td-&amp;gt;evtdev;

    if (!dev || !(dev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_ONESHOT) ||
        !tick_device_is_functional(dev)) {
        ...
    }

    td-&amp;gt;mode = TICKDEV_MODE_ONESHOT;
    dev-&amp;gt;event_handler = handler;
    clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
    tick_broadcast_switch_to_oneshot();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;4-高精度定时器的到期处理&quot;&gt;4. 高精度定时器的到期处理&lt;/h3&gt;

&lt;p&gt;  如前所述，高精度模式下，时钟中断的处理函数已经从tick_handle_periodic切换成hrtimer_interrupt了：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/*
 * High resolution timer interrupt
 * Called with interrupts disabled
 */
void hrtimer_interrupt(struct clock_event_device *dev)
{
    struct hrtimer_cpu_base *cpu_base = &amp;amp;__get_cpu_var(hrtimer_bases);
    ktime_t expires_next, now, entry_time, delta;
    int i, retries = 0;

    BUG_ON(!cpu_base-&amp;gt;hres_active);
    cpu_base-&amp;gt;nr_events++;
    dev-&amp;gt;next_event.tv64 = KTIME_MAX;

    raw_spin_lock(&amp;amp;cpu_base-&amp;gt;lock);
    entry_time = now = hrtimer_update_base(cpu_base); /*通过时钟源更新当前系统时间*/
retry:
    expires_next.tv64 = KTIME_MAX;
    /*
     * We set expires_next to KTIME_MAX here with cpu_base-&amp;gt;lock
     * held to prevent that a timer is enqueued in our queue via
     * the migration code. This does not affect enqueueing of
     * timers which run their callback and need to be requeued on
     * this CPU.
     */
    cpu_base-&amp;gt;expires_next.tv64 = KTIME_MAX;

    for (i = 0; i &amp;lt; HRTIMER_MAX_CLOCK_BASES; i++) { /*针对不同的计时参照对象依次处理*/
        struct hrtimer_clock_base *base;
        struct timerqueue_node *node;
        ktime_t basenow;

        if (!(cpu_base-&amp;gt;active_bases &amp;amp; (1 &amp;lt;&amp;lt; i)))
            continue;

        base = cpu_base-&amp;gt;clock_base + i;
        basenow = ktime_add(now, base-&amp;gt;offset);

        while ((node = timerqueue_getnext(&amp;amp;base-&amp;gt;active))) { /*根据到期时间依次处理红黑树中的定时器*/
            struct hrtimer *timer;

            timer = container_of(node, struct hrtimer, node);

            /*
             * The immediate goal for using the softexpires is
             * minimizing wakeups, not running timers at the
             * earliest interrupt after their soft expiration.
             * This allows us to avoid using a Priority Search
             * Tree, which can answer a stabbing querry for
             * overlapping intervals and instead use the simple
             * BST we already have.
             * We don't add extra wakeups by delaying timers that
             * are right-of a not yet expired timer, because that
             * timer will have to trigger a wakeup anyway.
             */

            if (basenow.tv64 &amp;lt; hrtimer_get_softexpires_tv64(timer)) {
                /*未到期则退出while循环*/

                ktime_t expires;

                expires = ktime_sub(hrtimer_get_expires(timer), base-&amp;gt;offset);
                if (expires.tv64 &amp;lt; 0)
                    expires.tv64 = KTIME_MAX;
                if (expires.tv64 &amp;lt; expires_next.tv64)
                    expires_next = expires;
                break;
            }

            __run_hrtimer(timer, &amp;amp;basenow); /*调用到期回调函数*/
        } /*end of while*/
    } /*end of for*/

    /*
     * Store the new expiry value so the migration code can verify
     * against it.
     */
    cpu_base-&amp;gt;expires_next = expires_next;
    raw_spin_unlock(&amp;amp;cpu_base-&amp;gt;lock);

    /*下面重新设置clock event device的中断触发时间，如果成功则返回*/
    /* Reprogramming necessary ? */
    if (expires_next.tv64 == KTIME_MAX ||
            !tick_program_event(expires_next, 0)) {
        cpu_base-&amp;gt;hang_detected = 0;
        return;
    }

    /*执行到此，后续的逻辑是处理一种特殊的场景，即定时器到期回调函数执行时间过长导致下一个定时器又到期了*/

    /*
     * The next timer was already expired due to:
     * - tracing
     * - long lasting callbacks
     * - being scheduled away when running in a VM
     *
     * We need to prevent that we loop forever in the hrtimer
     * interrupt routine. We give it 3 attempts to avoid
     * overreacting on some spurious event.
     *
     * Acquire base lock for updating the offsets and retrieving
     * the current time.
     */
    raw_spin_lock(&amp;amp;cpu_base-&amp;gt;lock);
    now = hrtimer_update_base(cpu_base);
    cpu_base-&amp;gt;nr_retries++;
    if (++retries &amp;lt; 3)
        goto retry;
    /*
     * Give the system a chance to do something else than looping
     * here. We stored the entry time, so we know exactly how long
     * we spent here. We schedule the next event this amount of
     * time away.
     */
    cpu_base-&amp;gt;nr_hangs++;
    cpu_base-&amp;gt;hang_detected = 1;
    raw_spin_unlock(&amp;amp;cpu_base-&amp;gt;lock);
    delta = ktime_sub(now, entry_time);
    if (delta.tv64 &amp;gt; cpu_base-&amp;gt;max_hang_time.tv64)
        cpu_base-&amp;gt;max_hang_time = delta;
    /*
     * Limit it to a sensible value as we enforce a longer
     * delay. Give the CPU at least 100ms to catch up.
     */
    if (delta.tv64 &amp;gt; 100 * NSEC_PER_MSEC)
        expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
    else
        expires_next = ktime_add(now, delta);
    tick_program_event(expires_next, 1);
    printk_once(KERN_WARNING &quot;hrtimer: interrupt took %llu ns\n&quot;, ktime_to_ns(delta));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过分析高精度模式下的时钟中断处理函数，我们可以发现它只负责处理定时器的到期处理。那么低精调模式下的进程调度的处理逻辑去哪里了？不需要了吗？其实，在前文代码中我们看到，高精度模式下内核会给每个CPU生成一个调度定时器：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-sched.c:

/**
 * tick_setup_sched_timer - setup the tick emulation timer
 */
void tick_setup_sched_timer(void)
{
    struct tick_sched *ts = &amp;amp;__get_cpu_var(tick_cpu_sched);
    ktime_t now = ktime_get();

    /*
     * Emulate tick processing via per-CPU hrtimers:
     */
    hrtimer_init(&amp;amp;ts-&amp;gt;sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
    ts-&amp;gt;sched_timer.function = tick_sched_timer; /*调度定时器的回调函数*/

    /* Get the next period (per cpu) */
    hrtimer_set_expires(&amp;amp;ts-&amp;gt;sched_timer, tick_init_jiffy_update());

    /* Offset the tick to avert jiffies_lock contention. */
    if (sched_skew_tick) {
        u64 offset = ktime_to_ns(tick_period) &amp;gt;&amp;gt; 1;
        do_div(offset, num_possible_cpus());
        offset *= smp_processor_id();
        hrtimer_add_expires_ns(&amp;amp;ts-&amp;gt;sched_timer, offset);
    }

    for (;;) {
        hrtimer_forward(&amp;amp;ts-&amp;gt;sched_timer, now, tick_period);
        hrtimer_start_expires(&amp;amp;ts-&amp;gt;sched_timer, HRTIMER_MODE_ABS_PINNED);
        /* Check, if the timer was already in the past */
        if (hrtimer_active(&amp;amp;ts-&amp;gt;sched_timer))
            break;
        now = ktime_get();
    }
    ...
}

/*
 * We rearm the timer until we get disabled by the idle code.
 * Called with interrupts disabled.
 */
static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
{
    struct tick_sched *ts =
        container_of(timer, struct tick_sched, sched_timer);
    struct pt_regs *regs = get_irq_regs();
    ktime_t now = ktime_get();

    tick_sched_do_timer(now);

    /*
     * Do not call, when we are not in irq context and have
     * no valid regs pointer
     */
    if (regs)
    tick_sched_handle(ts, regs);

    hrtimer_forward(timer, now, tick_period);

    return HRTIMER_RESTART;
}

static void tick_sched_do_timer(ktime_t now)
{
    int cpu = smp_processor_id();

    ...
    /* Check, if the jiffies need an update */
    if (tick_do_timer_cpu == cpu)
        tick_do_update_jiffies64(now);
}

static void tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)
{
    ...
    update_process_times(user_mode(regs));
    profile_tick(CPU_PROFILING);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  由此可见，调度定时器按tick_period周期性触发(暂不考虑动态时钟nohz特性)，每次到期后和处理逻辑和低精度模式下的逻辑类似。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/高精度定时器/&quot;&gt;【时间子系统】五、高精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】四、低精度定时器</title>
        <description>&lt;p&gt;  通过定时器，我们可以控制计算机在将来指定的某个时刻执行特定的动作。传统的定时器，以时钟滴答(jiffy)作为计时单位，因此它的精度较低(例如HZ=1000时，精度为1毫秒)，我们也称之为低精度定时器。&lt;/p&gt;

&lt;h3 id=&quot;1-初始化定时器&quot;&gt;1. 初始化定时器&lt;/h3&gt;

&lt;p&gt;  我们在概述中介绍过，内核中通过init_timer对定时器进行初始化，定时器中最关键的三个信息是：到期时间、到期处理函数、到期处理函数的参数。init_timer宏及定时器结构struct timer_list(取名struct timer可能更合适)的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timer.h:

#define init_timer(timer)                       \
    __init_timer((timer), 0)

#define __init_timer(_timer, _flags)            \
    init_timer_key((_timer), (_flags), NULL, NULL)

struct timer_list {
    /*
     * All fields that change during normal runtime grouped to the
     * same cacheline
     */
    struct list_head entry; /*用于将当前定时器挂到CPU的tvec_base链表中*/
    unsigned long expires; /*定时器到期时间*/
    struct tvec_base *base; /*定时器所属的tvec_base*/

    void (*function)(unsigned long); /*到期处理函数*/
    unsigned long data; /*到期处理函数的参数*/

    int slack; /*允许的偏差值*/

    ...
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  init_timer_key实现时，会将定时器指向执行初始化动作的CPU的tvec_base结构。内核为每个CPU分配一个struct tvec_base对象，用来记录每个CPU上定时器相关的全局信息(我们将在下一节详细说明)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * init_timer_key - initialize a timer
 * @timer: the timer to be initialized
 * @flags: timer flags
 * @name: name of the timer
 * @key: lockdep class key of the fake lock used for tracking timer
 *       sync lock dependencies
 *
 * init_timer_key() must be done to a timer prior calling *any* of the
 * other timer functions.
 */
void init_timer_key(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    debug_init(timer);
    do_init_timer(timer, flags, name, key);
}

static void do_init_timer(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    struct tvec_base *base = __raw_get_cpu_var(tvec_bases);

    timer-&amp;gt;entry.next = NULL;
    timer-&amp;gt;base = (void *)((unsigned long)base | flags);
    timer-&amp;gt;slack = -1;
    ...
}

struct tvec_base {
    spinlock_t lock; /*同步当前tvec_base的链表操作*/
    struct timer_list *running_timer; /*正在运行(到期触发)的定时器*/
    unsigned long timer_jiffies; /*用于判断定时器是否到期的当前时间，通常和系统的jiffies值相等*/
    unsigned long next_timer; /*下一个到期的定时器的到期时间*/
    unsigned long active_timers; /*激活的定时器的个数*/
    struct tvec_root tv1; /*tv1~tv5是用于保存已添加定时器的链表，也称为时间轮*/
    struct tvec tv2;
    struct tvec tv3;
    struct tvec tv4;
    struct tvec tv5;
} ____cacheline_aligned;

/*
 * per-CPU timer vector definitions:
 */
#define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
#define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
#define TVN_SIZE (1 &amp;lt;&amp;lt; TVN_BITS)
#define TVR_SIZE (1 &amp;lt;&amp;lt; TVR_BITS)
#define TVN_MASK (TVN_SIZE - 1)
#define TVR_MASK (TVR_SIZE - 1)
#define MAX_TVAL ((unsigned long)((1ULL &amp;lt;&amp;lt; (TVR_BITS + 4*TVN_BITS)) - 1))

struct tvec {
    struct list_head vec[TVN_SIZE];
};

struct tvec_root {
    struct list_head vec[TVR_SIZE];
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-添加定时器&quot;&gt;2. 添加定时器&lt;/h3&gt;

&lt;p&gt;  add_timer将定时器添加到执行CPU的tvec_base的时间轮链表中。内核根据定时器到期时间与当前时间jiffies的差值(值越小说明到期时间越早)，将定时器分别挂到五个级别的链表数组，级别越低链表到期时间越早，如下表所示：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;链表数组&lt;/th&gt;
      &lt;th&gt;时间差&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;tv1&lt;/td&gt;
      &lt;td&gt;0-255(2^8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv2&lt;/td&gt;
      &lt;td&gt;256–16383(2^14)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv3&lt;/td&gt;
      &lt;td&gt;16384–1048575(2^20)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv4&lt;/td&gt;
      &lt;td&gt;1048576–67108863(2^26)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv5&lt;/td&gt;
      &lt;td&gt;67108864–4294967295(2^32)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  其中tv1的数组大小为TVR_SIZE， tv2 tv3 tv4 tv5的数组大小为TVN_SIZE，根据CONFIG_BASE_SMALL配置项的不同，它们有不同的大小。默认情况下，没有使能CONFIG_BASE_SMALL，TVR_SIZE的大小是256，TVN_SIZE的大小则是64，当需要节省内存空间时，也可以使能CONFIG_BASE_SMALL，这时TVR_SIZE的大小是64，TVN_SIZE的大小则是16，以下的讨论我都是基于没有使能CONFIG_BASE_SMALL的情况。当有一个新的定时器要加入时，系统根据定时器到期的jiffies值和timer_jiffies字段的差值来决定该定时器被放入tv1至tv5中的哪一个数组中，最终，系统中所有的定时器的组织结构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/timer_2.jpg&quot; height=&quot;350&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从add_timer代码实现上看，最终会调用__internal_add_timer并根据时间差将定时器加入到合适的链表中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void
__internal_add_timer(struct tvec_base *base, struct timer_list *timer)
{
    unsigned long expires = timer-&amp;gt;expires;
    unsigned long idx = expires - base-&amp;gt;timer_jiffies; /*idx即为时间差*/
    struct list_head *vec;

    if (idx &amp;lt; TVR_SIZE) {
        int i = expires &amp;amp; TVR_MASK; /*以超时时间(而非时间差idx)作为索引寻找对应的链表，方便后续的超时处理*/
        vec = base-&amp;gt;tv1.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; TVR_BITS) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv2.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 2 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv3.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 3 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + 2 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv4.vec + i;
    } else if ((signed long) idx &amp;lt; 0) {
        /*
         * Can happen if you add a timer with expires == jiffies,
         * or you set a timer to go off in the past
         */
        vec = base-&amp;gt;tv1.vec + (base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK);
    } else {
        int i;
        /* If the timeout is larger than MAX_TVAL (on 64-bit
         * architectures or with CONFIG_BASE_SMALL=1) then we
         * use the maximum timeout.
         */
        if (idx &amp;gt; MAX_TVAL) {
            idx = MAX_TVAL;
            expires = idx + base-&amp;gt;timer_jiffies;
        }
        i = (expires &amp;gt;&amp;gt; (TVR_BITS + 3 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv5.vec + i;
    }
    /*
     * Timers are FIFO:
     */
    list_add_tail(&amp;amp;timer-&amp;gt;entry, vec);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-触发定时器&quot;&gt;3. 触发定时器&lt;/h3&gt;

&lt;p&gt;  在时钟中断部分，我们提到过每次中断处理时都会调用run_local_timers进行本地定时器的处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/*
 * Called by the local, per-CPU timer interrupt on SMP.
 */
void run_local_timers(void)
{
    ...
    raise_softirq(TIMER_SOFTIRQ); /*最终在中断返回时进入软中断处理函数run_timer_softirq*/
}

/*
 * This function runs timers and the timer-tq in bottom half context.
 */
static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    ...

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) /*实际当前时间晚于base中记录的当前时间，说明需要更新base中时间或者有定时器到期*/
        __run_timers(base);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  定时器的到期处理逻辑中，总是先处理tv1中的定时器，如果tv1中所有的链表为空，再从tv2中移动链表并重新添加到tv1中；如果tv1和tv2中为空，再从tv3中移动链表重新添加到tv1和tv2中；依此类推。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * __run_timers - run all expired timers (if any) on this CPU.
 * @base: the timer vector to be processed.
 *
 * This function cascades all vectors and executes all expired timer
 * vectors.
 */
static inline void __run_timers(struct tvec_base *base)
{
    struct timer_list *timer;

    spin_lock_irq(&amp;amp;base-&amp;gt;lock);
    while (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) {
        struct list_head work_list;
        struct list_head *head = &amp;amp;work_list;
        int index = base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK; /*以base中的当前时间为索引取出已到期的定时器*/

        /*
         * Cascade timers:
         */
        /*如果低级链表为空，则从高级别链表中移动添加到低级别中*/
        if (!index &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv2, INDEX(0))) &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv3, INDEX(1))) &amp;amp;&amp;amp;
            !cascade(base, &amp;amp;base-&amp;gt;tv4, INDEX(2)))
                cascade(base, &amp;amp;base-&amp;gt;tv5, INDEX(3));
        ++base-&amp;gt;timer_jiffies; /*累加base中当前时间*/
        list_replace_init(base-&amp;gt;tv1.vec + index, &amp;amp;work_list);
        /*处理已到期的定时期的回调函数*/
        while (!list_empty(head)) {
            void (*fn)(unsigned long);
            unsigned long data;
            bool irqsafe;

            timer = list_first_entry(head, struct timer_list,entry);
            fn = timer-&amp;gt;function;
            data = timer-&amp;gt;data;
            irqsafe = tbase_get_irqsafe(timer-&amp;gt;base);

            timer_stats_account_timer(timer);

            base-&amp;gt;running_timer = timer;
            detach_expired_timer(timer, base);

            if (irqsafe) {
                spin_unlock(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock(&amp;amp;base-&amp;gt;lock);
            } else {
                spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock_irq(&amp;amp;base-&amp;gt;lock);
            }
        }
    }
    base-&amp;gt;running_timer = NULL;
    spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
}

#define INDEX(N) ((base-&amp;gt;timer_jiffies &amp;gt;&amp;gt; (TVR_BITS + (N) * TVN_BITS)) &amp;amp; TVN_MASK)

static int cascade(struct tvec_base *base, struct tvec *tv, int index)
{
    /* cascade all the timers from tv up one level */
    struct timer_list *timer, *tmp;
    struct list_head tv_list;

    list_replace_init(tv-&amp;gt;vec + index, &amp;amp;tv_list);

    /*
     * We are removing _all_ timers from the list, so we
     * don't have to detach them individually.
     */
    list_for_each_entry_safe(timer, tmp, &amp;amp;tv_list, entry) {
        BUG_ON(tbase_get_base(timer-&amp;gt;base) != base);
        /* No accounting, while moving them */
        __internal_add_timer(base, timer);
    }

    return index;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;【时间子系统】四、低精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
  </channel>
</rss>
