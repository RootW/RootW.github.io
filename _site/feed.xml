<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>吴斌</title>
    <description>欢迎来到我的技术博客~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 29 Mar 2018 17:14:19 +0800</pubDate>
    <lastBuildDate>Thu, 29 Mar 2018 17:14:19 +0800</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>【Rados Block Device】OSD原理分析－SimpleMessenger</title>
        <description>&lt;p&gt;  OSD进程通过网络对Client提供服务，因此网络层是OSD中的基础层。本篇博文将讨论ceph中传统的SimpleMessenger实现原理。&lt;/p&gt;

&lt;h3 id=&quot;simplemessenger模块中对象流程概览&quot;&gt;SimpleMessenger模块中对象、流程概览&lt;/h3&gt;

&lt;p&gt;  如果将OSD进程的网络服务模式配置成SimpleMessenger，那么它采用的是POSIX标准网络接口来实现网络功能。也就是说，此时我们的OSD服务从代码实现流程上来说就是通过socket()-&amp;gt;bind()-&amp;gt;listen()-&amp;gt;accept()进行连接建立，随后再通过每个连接进行网络消息的收发。虽然SimpleMessenger在实现过程中融入一些设计模式的抽象，但是抓住以上POSIX网络编程核心流程后将便于大家理解其实现机理。&lt;/p&gt;

&lt;p&gt;  我们先整体来看一下SimpleMessenger的对象和流程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_simplemessenger.jpg&quot; height=&quot;600&quot; width=&quot;1200&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们知道一个ceph集群中通常会有两个网络平面：public和cluster，那么每个OSD进程中就会对应产生两个messenger对象ms_public和ms_cluster。对于每个messenger对象(这里其实为SimpleMessenger对象)，会产生一个名为ms_accepter的线程，messenger对象通过accepter成员变量指向该线程。ms_accepter线程是在完成socket()-&amp;gt;bind()-&amp;gt;listen()动作之后产生，它的主要作用就是一直监听Client端的网络连接请求。&lt;/p&gt;

&lt;p&gt;  当某一个Client发起连接请求后，ms_accepter将调用accept()接受请求，并为该连接产生一个pipe对象。pipe对象是对TCP socket连接的封装，可以实现故障重连等可靠性增强特性。每个pipe对象会产生两个线程：一个叫ms_pipe_read线程，它一直在监听socket连接中的消息，如果发现有消息它将取出消息并将消息放入in_q中进行分发处理；另一个叫ms_pipe_write线程，它一直在等待out_q中被放入发送消息，如果发现out_q中有消息，它将把消息取出并通过socket连接将其发送出去。&lt;/p&gt;

&lt;p&gt;  在消息接收处理的过程中，in_q队列指向的其实是SimpleMessenger对象中的DispatchQueue，也就是说对于同一个messenger中的多个pipe，它们接收的消息将被放入到同一个队列中等待处理。DispatchQueue分发消息时如果发现该消息可以被快速处理(fast dispatch)时，会将该消息直接分发给SimpleMessenger，由SimpleMessenger将消息分发给其内部的多个Dispatcher对象进行最终的消息处理。这里OSD模块中的OSD对象就是属于SimpleMessenger的一个Dispatcher，OSD对消息的处理属于OSD模块的内容，我们将在后续博文中介绍。DispatchQueue如果发现该消息无法被快速处理，则会将该消息交给DispatchQueue对应的DispatchThread处理。DispatchThread线程取出消息后会传递给messenger通过ms_deliver_dispatch进行普通处理，其实最终也会交给Dispatcher(如OSD对象)处理，只不过是在DispatchThread线程中被处理(fast dispatch是在ms_pipe_read线程中被处理，请大家注意对比)。&lt;/p&gt;

&lt;h3 id=&quot;simplemessenger模块中类的概览&quot;&gt;SimpleMessenger模块中类的概览&lt;/h3&gt;

&lt;p&gt;  在了解了SimpleMessenger的实现流程后，我们再来看看它的类定义，从而理解它是如何实现抽象，以支持未来更灵活的扩展：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_osd_simplemessenger_class.jpg&quot; height=&quot;800&quot; width=&quot;1200&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  图中标红的类(Messenger、Dispatcher、DispatchQueue、Connection)属于Messenger模块抽象类(不属于某一种网络实现模式)，不同的网络实现模式可以继承它们实现各自特有的功能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Messenger类是SimpleMessenger、AsyncMessenger、XioMessenger的父类，它描绘了messenger对象的功能框架。每个messenger对象包含两个Dispatcher链表，dispatchers代表普通处理对象，fast_dispatchers代表快速处理对象，以便实现针对不同消息类型采用不同处理方式的功能。Dispatcher对象是在初始流程中，通过add_dispatcher_header方法被加入到messenger对应的链表。create、bind、start、ready方法实现messenger对象的创建和初始化；wait等待messenger生命周期结束；shutdown关闭messenger对象。send_message实现了通过messenger对象发送一个消息的功能。ms_fast_dispatch实现消息的快速分发；ms_deliver_dispatch实现消息的普通分发；最终都将分发给messenger中的dispatchers进行处理。&lt;/li&gt;
    &lt;li&gt;Dispatcher类是接收消息的处理者。不同模块可以实现各自不同的Dispatcher，以实现对消息的不同处理逻辑。只要在初始化时通过messenger对象的add_dispatcher_*方法被加入到messenger中，便可保证该Dispatcher可以接收到消息并进行处理。&lt;/li&gt;
    &lt;li&gt;DispatchQueue类实现了一个分发队列。用户通过enqueue操作将消息放入队列，该队列可以按优先级对消息进行排序(PrioritizeQueue)。队列会产生一个专门的DispatchThread线程，由该线程负责从PrioritizedQueue中取出消息进行分发处理。此外，DispatchQueue也可通过can_fast_dispatch判断消息是否可以被快速处理，如果可以被快速处理，则直接调用fast_dispatch进行分发处理，否则调用enqueue进队列交由DispatchThread处理。&lt;/li&gt;
    &lt;li&gt;Connection类是对网络连接的抽象。子类通过实现send_message方法提供不同的网络发送方案。&lt;/li&gt;
    &lt;li&gt;SimpleMessenger类继承Messenger类实现基于POSIX网络接口的网络信使功能。它将实现Messenger类中的bind、start、ready方法，以完成网络socket的初始化。独有的Accepter对象将产生一个独立的线程监听网络连接请求。每当接受一个新连接时，都会生成一个新的Pipe对象，并通过add_accept_pipe方法将其加入到pipes列表中。SimpleMessenger包含了一个DispatchQueue来实现消息的普通分发和快速分发。&lt;/li&gt;
    &lt;li&gt;Pipe类代表一个连接会话，每个连接会产生一个reader_thread线程(入口函数为reader()方法)和一个writer_thread线程(入口函数为writer())。reader方法负责从网络层接收消息放入in_q并进行顶层分发处理；writer方法负责将out_q中消息通过网络发送。accept方法在接受连接时调用，connect方法在发起连接请求时调用。&lt;/li&gt;
    &lt;li&gt;Thread类是Common模块中的公共类，用来生成线程。create方法用来创建线程对象；entry方法为新线程的入口函数；join方法在等待子线程结束时调用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;代码详解&quot;&gt;代码详解&lt;/h3&gt;

&lt;p&gt;  有了对SimpleMessenger模块中对象、流程和类的整体认识之后，我们再结合代码来深入理解其实现细节。&lt;/p&gt;

&lt;h4 id=&quot;1-初始化过程&quot;&gt;&lt;strong&gt;1. 初始化过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SimpleMessenger模块的初始化在OSD进程的main函数中完成，整体调用栈如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph_osd.cc:main()
    |-Messenger::create()
    |   \-SimpleMessenger::SimpleMessenger()
    |-Messenger::bind() -&amp;gt; SimpleMessenger::bind()
    |   \-Accepter::bind()
    |       |-::socket()
    |       |-::bind()
    |       \-::listen()
    |-OSD::OSD()
    |-Messenger::start() -&amp;gt; SimpleMessenger::start()
    |-OSD::init()
    |   \-Messenger::add_dispatcher_head()
    |       \-Messenger::ready() -&amp;gt; SimpleMessenger::ready()
    |           \-Accepter::start()
    \-Messenger::wait() -&amp;gt; SimpleMessenger::wait()
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们先来看看ceph_osd.cc中main函数里和Messenger初始化相关的部分代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/ceph_osd.cc:

int main(int argc, const char **argv)
{
    ...

    /*这里我们假设public_msgr_type和cluster_msgr_type都为Simple*/
    std::string public_msgr_type = g_conf-&amp;gt;ms_public_type.empty() ? g_conf-&amp;gt;get_val&amp;lt;std::string&amp;gt;(&quot;ms_type&quot;) : g_conf-&amp;gt;ms_public_type;
    std::string cluster_msgr_type = g_conf-&amp;gt;ms_cluster_type.empty() ? g_conf-&amp;gt;get_val&amp;lt;std::string&amp;gt;(&quot;ms_type&quot;) : g_conf-&amp;gt;ms_cluster_type;

    /*根据不同类型创建messenger对象，这里将创建两个SimpleMessenger对象*/
    Messenger *ms_public = Messenger::create(g_ceph_context, public_msgr_type,
        entity_name_t::OSD(whoami), &quot;client&quot;,
        getpid(),
        Messenger::HAS_HEAVY_TRAFFIC |
        Messenger::HAS_MANY_CONNECTIONS);
    Messenger *ms_cluster = Messenger::create(g_ceph_context, cluster_msgr_type,
        entity_name_t::OSD(whoami), &quot;cluster&quot;,
        getpid(),
        Messenger::HAS_HEAVY_TRAFFIC |
        Messenger::HAS_MANY_CONNECTIONS);

    ...

    /*为messenger对象绑定IP地址，最终会调用Accepter::bind函数，由它调用系统的socket()、bind()和listen()函数*/
    r = ms_public-&amp;gt;bind(g_conf-&amp;gt;public_addr);
    ...
    r = ms_cluster-&amp;gt;bind(g_conf-&amp;gt;cluster_addr);

    ...

    /*将创建的ms_public和ms_cluster传递给OSD构造函数，建立一个新的OSD对象*/
    osd = new OSD(g_ceph_context,
                store,
                whoami,
                ms_cluster,
                ms_public,
                ms_hb_front_client,
                ms_hb_back_client,
                ms_hb_front_server,
                ms_hb_back_server,
                ms_objecter,
                &amp;amp;mc,
                g_conf-&amp;gt;osd_data,
                g_conf-&amp;gt;osd_journal);

    /*启动messenger对象，针对SimpleMessenger，其内部细节我们暂不用关心*/
    ms_public-&amp;gt;start();
    ...
    ms_cluster-&amp;gt;start();

    /*OSD执行初始化，下文将展开*/
    osd-&amp;gt;init();

    ...

    /*整个初始化动作完成，OSD主线程进入等待状态*/
    ms_public-&amp;gt;wait();
    ms_cluster-&amp;gt;wait();
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/Messenger.cc:

Messenger *Messenger::create(CephContext *cct, const string &amp;amp;type,
        entity_name_t name, string lname,
        uint64_t nonce, uint64_t cflags)
{
    int r = -1;
    if (type == &quot;random&quot;) {
        static std::random_device seed;
        static std::default_random_engine random_engine(seed());
        static Spinlock random_lock;

        std::lock_guard&amp;lt;Spinlock&amp;gt; lock(random_lock);
        std::uniform_int_distribution&amp;lt;&amp;gt; dis(0, 1);
        r = dis(random_engine);
    }
    if (r == 0 || type == &quot;simple&quot;)
        return new SimpleMessenger(cct, name, std::move(lname), nonce);
    else if (r == 1 || type.find(&quot;async&quot;) != std::string::npos)
        return new AsyncMessenger(cct, name, type, std::move(lname), nonce);
#ifdef HAVE_XIO
    else if ((type == &quot;xio&quot;) &amp;amp;&amp;amp;
            cct-&amp;gt;check_experimental_feature_enabled(&quot;ms-type-xio&quot;))
    return new XioMessenger(cct, name, std::move(lname), nonce, cflags);
#endif
    lderr(cct) &amp;lt;&amp;lt; &quot;unrecognized ms_type '&quot; &amp;lt;&amp;lt; type &amp;lt;&amp;lt; &quot;'&quot; &amp;lt;&amp;lt; dendl;
    return nullptr;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们再深入看看OSD初始化过程中与Messenger相关的部分代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/osd/OSD.cc:

int OSD::init()
{
    ...

    /*将OSD对象自身加到public messenger和cluster messenger的dispatchers中*/
    client_messenger-&amp;gt;add_dispatcher_head(this);
    cluster_messenger-&amp;gt;add_dispatcher_head(this);

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/Messenger.h:

void add_dispatcher_head(Dispatcher *d) { 
    bool first = dispatchers.empty(); /*是否为添加到dispatchers链表中的第一个元素？*/
    dispatchers.push_front(d); /*加入到dispatchers*/
    if (d-&amp;gt;ms_can_fast_dispatch_any()) /*如果可以进行fast dispatch则加入到fast_dispatchers中*/
        fast_dispatchers.push_front(d);
    if (first)
        ready(); /*如果是首个加入到dispatchers中的对象，则调用messenger对象的ready()*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

void SimpleMessenger::ready()
{
    ldout(cct,10) &amp;lt;&amp;lt; &quot;ready &quot; &amp;lt;&amp;lt; get_myaddr() &amp;lt;&amp;lt; dendl;
    dispatch_queue.start(); /*拉起dispatch_queue对应的dispatch_thread*/

    lock.Lock();
    if (did_bind)
        accepter.start(); /*拉起ms_accepter线程*/
    lock.Unlock();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Accept.cc:

int Accepter::start()
{
    ldout(msgr-&amp;gt;cct,1) &amp;lt;&amp;lt; __func__ &amp;lt;&amp;lt; dendl;

    // start thread
    create(&quot;ms_accepter&quot;);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-连接建立过程&quot;&gt;&lt;strong&gt;2. 连接建立过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SimpleMessenger对象初始化完成后，将拉起一个ms_accepter线程处理Client端的连接请求，该线程入口函数为Accepter::entry。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Accepter.cc:

void *Accepter::entry()
{
    int errors = 0;
    int ch;

    struct pollfd pfd[2];
    memset(pfd, 0, sizeof(pfd));

    pfd[0].fd = listen_sd;
    pfd[0].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
    pfd[1].fd = shutdown_rd_fd;
    pfd[1].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;

    while (!done) {
        int r = poll(pfd, 2, -1); /*通过poll系统调用等待Client连接请求*/

        ...

        if (done) break;

        // accept
        sockaddr_storage ss;
        socklen_t slen = sizeof(ss);
        int sd = ::accept(listen_sd, (sockaddr*)&amp;amp;ss, &amp;amp;slen); /*收到请求后，accept该连接*/
        if (sd &amp;gt;= 0) {
            ...
            msgr-&amp;gt;add_accept_pipe(sd); /*向SimpleMessenger对象中添加pipe*/
        } else {
            ...
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

Pipe *SimpleMessenger::add_accept_pipe(int sd)
{
    lock.Lock();
    Pipe *p = new Pipe(this, Pipe::STATE_ACCEPTING, NULL);
    p-&amp;gt;sd = sd;
    p-&amp;gt;pipe_lock.Lock();
    p-&amp;gt;start_reader(); /*拉起pipe对象的ms_pipe_read线程*/
    p-&amp;gt;pipe_lock.Unlock();
    pipes.insert(p);
    accepting_pipes.insert(p);
    lock.Unlock();
    return p;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.cc:

void Pipe::start_reader()
{
    if (reader_needs_join) {
        reader_thread.join();
        reader_needs_join = false;
    }
    reader_running = true;
    reader_thread.create(&quot;ms_pipe_read&quot;, msgr-&amp;gt;cct-&amp;gt;_conf-&amp;gt;ms_rwthread_stack_bytes);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;3-消息接收与分发过程&quot;&gt;&lt;strong&gt;3. 消息接收与分发过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  每个pipe对象的ms_pipe_read线程被拉起后，会进行会话的协商过程(可以回顾下前期Client端RBD的messenger分析博文)；完成后将处于等待接收消息的状态：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.cc:

void Pipe::reader()
{
    pipe_lock.Lock();

    if (state == STATE_ACCEPTING) {
        /*执行会话协商过程，协商成功后会拉起ms_pipe_write线程*/
        accept();
    }

    while (state != STATE_CLOSED &amp;amp;&amp;amp;
            state != STATE_CONNECTING) {

        // sleep if (re)connecting
        if (state == STATE_STANDBY) {
            /*如果pipe状态为STANDBY，说明底层连接故障且暂无消息处理，则进入睡眠*/
            cond.Wait(pipe_lock);
            continue;
        }

        pipe_lock.Unlock();

        char tag = -1;
        /*先从网络连接中读取一个字节的tag*/
        if (tcp_read((char*)&amp;amp;tag, 1) &amp;lt; 0) {
            pipe_lock.Lock();
            fault(true);
            continue;
        }
        
        /*根据tag值进行不同的处理动作*/
        if (tag == CEPH_MSGR_TAG_KEEPALIVE) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_KEEPALIVE2) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) {
            ...
            continue;
        }
        if (tag == CEPH_MSGR_TAG_ACK) {
            ...
            continue;
        }
        else if (tag == CEPH_MSGR_TAG_MSG) {
            /*针对消息，先从网络中读取消息内容到m中*/
            Message *m = 0;
            int r = read_message(&amp;amp;m, auth_handler.get());

            pipe_lock.Lock();
            if (m-&amp;gt;get_seq() &amp;lt;= in_seq) {
                m-&amp;gt;put();
                continue;
            }
            m-&amp;gt;set_connection(connection_state.get());
            ...

            /*对消息进行预处理*/
            in_q-&amp;gt;fast_preprocess(m);
            if (delay_thread) {
                ...
            } else {
                /*如果消息可以被快速处理，则走快速处理流程；否则就enqueue到in_q中交给dispatch_thread处理*/
                if (in_q-&amp;gt;can_fast_dispatch(m)) {
                    reader_dispatching = true;
                    pipe_lock.Unlock();
                    in_q-&amp;gt;fast_dispatch(m);
                    pipe_lock.Lock();
                    reader_dispatching = false;
                    ...
                } else {
                    in_q-&amp;gt;enqueue(m, m-&amp;gt;get_priority(), conn_id);
                }            
            }
        }
        ...
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  消息的快速处理流程可以回头参考前文的对象、流程图。&lt;/p&gt;

&lt;h4 id=&quot;4-消息发送过程&quot;&gt;&lt;strong&gt;4. 消息发送过程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  任何模块想通过messenger发送消息时，都可以调用Messenger::send_mesage来完成。对于SimpleMessenger，其实现体位于SimpleMessenger.h，发送是一个异步过程，发送者只会将消息放入Pipe对象的out_q中，随后由ms_pipe_write线程完成向网络协议栈的发送。&lt;/p&gt;

&lt;p&gt;  发送者的调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sender[any thread]
    \-SimpleMessenger::send_message()
        \-SimpleMessenger::_send_message()
            \-SimpleMessenger::submit_message()
                \-Pipe::_send()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.h:

int send_message(Message *m, const entity_inst_t&amp;amp; dest) override {
    return _send_message(m, dest);
}

int send_message(Message *m, Connection *con) {
    return _send_message(m, con);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/SimpleMessenger.cc:

int SimpleMessenger::_send_message(Message *m, Connection *con)
{
    //set envelope
    m-&amp;gt;get_header().src = get_myname();

    if (!m-&amp;gt;get_priority()) m-&amp;gt;set_priority(get_default_send_priority());

    submit_message(m, static_cast&amp;lt;PipeConnection*&amp;gt;(con),
                con-&amp;gt;get_peer_addr(), con-&amp;gt;get_peer_type(), false);
    return 0;
}

void SimpleMessenger::submit_message(Message *m, PipeConnection *con,
                const entity_addr_t&amp;amp; dest_addr, int dest_type,
                bool already_locked)
{
    ...
    if (con) {
        Pipe *pipe = NULL;
        bool ok = static_cast&amp;lt;PipeConnection*&amp;gt;(con)-&amp;gt;try_get_pipe(&amp;amp;pipe);
        ...
        while (pipe &amp;amp;&amp;amp; ok) {
            // we loop in case of a racing reconnect, either from us or them
            pipe-&amp;gt;pipe_lock.Lock(); // can't use a Locker because of the Pipe ref
            if (pipe-&amp;gt;state != Pipe::STATE_CLOSED) {
                pipe-&amp;gt;_send(m);
                pipe-&amp;gt;pipe_lock.Unlock();
                pipe-&amp;gt;put();
                return;
            }
        }
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph/src/msg/simple/Pipe.h:

void _send(Message *m) {
    assert(pipe_lock.is_locked());
    out_q[m-&amp;gt;get_priority()].push_back(m);
    cond.Signal();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  ms_pipe_write线程入口函数为Pipe::writer，其调用栈如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pipe::writer()
    |-Pipe::_get_next_outgoing()
    \-Pipe::write_message()
        \-Pipe::do_sendmsg()

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void Pipe::writer()
{
    pipe_lock.Lock();
    while (state != STATE_CLOSED) {
        ...
        Message *m = _get_next_outgoing();
        ...
        const ceph_msg_header&amp;amp; header = m-&amp;gt;get_header();
        const ceph_msg_footer&amp;amp; footer = m-&amp;gt;get_footer();
        bufferlist blist = m-&amp;gt;get_payload();
        blist.append(m-&amp;gt;get_middle());
        blist.append(m-&amp;gt;get_data());
        pipe_lock.Unlock();

        write_message(header, footer, blist);
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/03/RBD-OSD-1&quot;&gt;【Rados Block Device】OSD原理分析－SimpleMessenger&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/RBD-OSD-1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/RBD-OSD-1/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】五、高精度定时器</title>
        <description>&lt;p&gt;  通过对低精度定时器的分析，我们知道这类定时器的精度是毫秒级的，也就是说存在毫秒级的误差范围。对于像IO超时错误处理这类定时任务，毫秒级的误差完全不算什么问题。然而，对于工业上的许多实时任务，毫秒级的误差是完全不可接受的。因此，基于更高精度的时间硬件(例如TSC和LAPIC Timer)，内核工程师们开发了一套全新的高精度定时器功能(传统基于时间轮的低精度定时器已经很稳定了，与其对它修修补补，还不如新建一套全新的机制)。&lt;/p&gt;

&lt;h3 id=&quot;1-高精度定时器的初始化&quot;&gt;1. 高精度定时器的初始化&lt;/h3&gt;

&lt;p&gt;  高精度定时器的初始化和低精度定时器的初始化有些类似，需要指定到期后的回调函数。然而在内部数据结构的设计上，不同于低精度定时器的时间轮，高精度定时器采用了红黑树(可以高效地实现排序、增删改等操作，内核中有比较成熟稳定的代码实现)。另外，低精度定时器的计时参照是jiffies，而高精度定时器可以采用timekeeper中的多种计时参照，如REAL TIME、MONOTONIC TIME等等。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/**
 * hrtimer_init - initialize a timer to the given clock
 * @timer:	the timer to be initialized
 * @clock_id:	the clock to be used
 * @mode:	timer mode abs/rel
 */
void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
    debug_init(timer, clock_id, mode);
    __hrtimer_init(timer, clock_id, mode);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/hrtimer.h:

/**
 * struct hrtimer - the basic hrtimer structure
 * @node:   timerqueue node, which also manages node.expires,
 *          the absolute expiry time in the hrtimers internal
 *          representation. The time is related to the clock on
 *          which the timer is based. Is setup by adding
 *          slack to the _softexpires value. For non range timers
 *          identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *          The time which was given as expiry time when the timer
 *          was armed.
 * @function:   timer expiry callback function
 * @base:   pointer to the timer base (per cpu and per clock)
 * @state:  state information (See bit values above)
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct hrtimer {
    struct timerqueue_node      node;
    ktime_t                     _softexpires;
    enum hrtimer_restart        (*function)(struct hrtimer *);
    struct hrtimer_clock_base   *base;
    unsigned long               state;
    ...
};

enum hrtimer_mode {
    HRTIMER_MODE_ABS = 0x0,		/* Time value is absolute */
    HRTIMER_MODE_REL = 0x1,		/* Time value is relative to now */
    HRTIMER_MODE_PINNED = 0x02,	/* Timer is bound to CPU */
    HRTIMER_MODE_ABS_PINNED = 0x02,
    HRTIMER_MODE_REL_PINNED = 0x03,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过上面的代码和注释，我们可以看到，高精度定时器初始化时可以指定计时参照对象(clock_id)和计时模式(采用绝对计时或相对计时)。高精度定时器内部结构中的node即是在红黑树中的挂接对象，base指向每个CPU针对不同计时参照对象的全局数据结构，其内部包含一棵红黑树。__hrtimer_init的具体实现比较简单：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
    struct hrtimer_cpu_base *cpu_base;
    int base;

    memset(timer, 0, sizeof(struct hrtimer));

    cpu_base = &amp;amp;__raw_get_cpu_var(hrtimer_bases); /*获取当前CPU的hrtimer_cpu_base对象*/

    if (clock_id == CLOCK_REALTIME &amp;amp;&amp;amp; mode != HRTIMER_MODE_ABS) /*REALTIME只支持绝对模式*/
        clock_id = CLOCK_MONOTONIC;

    base = hrtimer_clockid_to_base(clock_id); /*索引计时参照*/
    timer-&amp;gt;base = &amp;amp;cpu_base-&amp;gt;clock_base[base];
    timerqueue_init(&amp;amp;timer-&amp;gt;node); /*初始化红黑树节点*/

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-高精度定时器的启动&quot;&gt;2. 高精度定时器的启动&lt;/h3&gt;

&lt;p&gt;  初始化完成并指定回调处理函数后，我们通过hrtimer_start函数可以启动一个定时器：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/**
 * hrtimer_start - (re)start an hrtimer on the current CPU
 * @timer:  the timer to be added
 * @tim:    expiry time
 * @mode:   expiry mode: absolute (HRTIMER_MODE_ABS) or
 *          relative (HRTIMER_MODE_REL)
 *
 * Returns:
 *  0 on success
 *  1 when the timer was active
 */
int hrtimer_start(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode)
{
    return __hrtimer_start_range_ns(timer, tim, 0, mode, 1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  高精度定时器允许有一个纳秒级别的误差，由__hrtimer_start_range_ns的delta_ns参数指明：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
    unsigned long delta_ns, const enum hrtimer_mode mode, int wakeup)
{
    struct hrtimer_clock_base *base, *new_base;
    unsigned long flags;
    int ret, leftmost;

    base = lock_hrtimer_base(timer, &amp;amp;flags); /*锁定该timer对应的hrtimer_clock_base对象*/

    /* Remove an active timer from the queue: */
    ret = remove_hrtimer(timer, base);

    if (mode &amp;amp; HRTIMER_MODE_REL) {
        tim = ktime_add_safe(tim, base-&amp;gt;get_time());
        ...
    }

    hrtimer_set_expires_range_ns(timer, tim, delta_ns); /*设置定时器内部的超时时间*/

    /* Switch the timer base, if necessary: */
    new_base = switch_hrtimer_base(timer, base, mode &amp;amp; HRTIMER_MODE_PINNED);

    leftmost = enqueue_hrtimer(timer, new_base); /*将定时器加入到对应hrtimer_clock_base的红黑树中*/

    if (leftmost &amp;amp;&amp;amp; new_base-&amp;gt;cpu_base == &amp;amp;__get_cpu_var(hrtimer_bases)
        &amp;amp;&amp;amp; hrtimer_enqueue_reprogram(timer, new_base)) { /*如果当前定时器是红黑树中最早到期的定时器，则重新设置clock event device的oneshot计数。注，高精度定时器正常工作时，会将clock event device的工作模式切换到oneshot*/
        ...
    }

    unlock_hrtimer_base(timer, &amp;amp;flags);

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-切换到高精度模式&quot;&gt;3. 切换到高精度模式&lt;/h3&gt;

&lt;p&gt;  内核正常启动后首先工作在低精度模式，然而在时钟中断的处理中，内核会检测是否具备切换到高精度的条件，如果各条件均满足，则切换到高精度模式工作。时钟中断中在处理低精度时钟时，通过hrtimer_run_pending()完成切换动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    hrtimer_run_pending();

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies))
        __run_timers(base);
}

void hrtimer_run_pending(void)
{
    if (hrtimer_hres_active()) /*如果已经切换到高精度模式则返回*/
        return;

    if (tick_check_oneshot_change(!hrtimer_is_hres_enabled())) /*判断是否具备切换到高精度的条件，如时钟源精度是否满足、是否支持oneshot模式*/
        hrtimer_switch_to_hres();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  如果切换条件均满足，则通过hrtimer_switch_to_hres切换到高精度模式：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

static int hrtimer_switch_to_hres(void)
{
    int i, cpu = smp_processor_id();
    struct hrtimer_cpu_base *base = &amp;amp;per_cpu(hrtimer_bases, cpu);
    unsigned long flags;

    if (base-&amp;gt;hres_active)
        return 1;

    local_irq_save(flags);

    if (tick_init_highres()) { /*将tick模式切换到oneshot模式并重新指定中断处理函数*/
        local_irq_restore(flags);
        printk(KERN_WARNING &quot;Could not switch to high resolution &quot;
        &quot;mode on CPU %d\n&quot;, cpu);
        return 0;
    }
    base-&amp;gt;hres_active = 1;
    for (i = 0; i &amp;lt; HRTIMER_MAX_CLOCK_BASES; i++)
    base-&amp;gt;clock_base[i].resolution = KTIME_HIGH_RES;

    tick_setup_sched_timer(); /*设置一个专门的调度定时器，用来处理调度任务*/
    /* &quot;Retrigger&quot; the interrupt to get things going */
    retrigger_next_event(NULL);
    local_irq_restore(flags);
    return 1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-oneshot.c:

/**
 * tick_init_highres - switch to high resolution mode
 *
 * Called with interrupts disabled.
 */
int tick_init_highres(void)
{
    return tick_switch_to_oneshot(hrtimer_interrupt); /*高精度模式下时钟中断处理函数为hrtimer_interrupt*/
}

/**
 * tick_switch_to_oneshot - switch to oneshot mode
 */
int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))
{
    struct tick_device *td = &amp;amp;__get_cpu_var(tick_cpu_device);
    struct clock_event_device *dev = td-&amp;gt;evtdev;

    if (!dev || !(dev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_ONESHOT) ||
        !tick_device_is_functional(dev)) {
        ...
    }

    td-&amp;gt;mode = TICKDEV_MODE_ONESHOT;
    dev-&amp;gt;event_handler = handler;
    clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
    tick_broadcast_switch_to_oneshot();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;4-高精度定时器的到期处理&quot;&gt;4. 高精度定时器的到期处理&lt;/h3&gt;

&lt;p&gt;  如前所述，高精度模式下，时钟中断的处理函数已经从tick_handle_periodic切换成hrtimer_interrupt了：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/hrtimer.c:

/*
 * High resolution timer interrupt
 * Called with interrupts disabled
 */
void hrtimer_interrupt(struct clock_event_device *dev)
{
    struct hrtimer_cpu_base *cpu_base = &amp;amp;__get_cpu_var(hrtimer_bases);
    ktime_t expires_next, now, entry_time, delta;
    int i, retries = 0;

    BUG_ON(!cpu_base-&amp;gt;hres_active);
    cpu_base-&amp;gt;nr_events++;
    dev-&amp;gt;next_event.tv64 = KTIME_MAX;

    raw_spin_lock(&amp;amp;cpu_base-&amp;gt;lock);
    entry_time = now = hrtimer_update_base(cpu_base); /*通过时钟源更新当前系统时间*/
retry:
    expires_next.tv64 = KTIME_MAX;
    /*
     * We set expires_next to KTIME_MAX here with cpu_base-&amp;gt;lock
     * held to prevent that a timer is enqueued in our queue via
     * the migration code. This does not affect enqueueing of
     * timers which run their callback and need to be requeued on
     * this CPU.
     */
    cpu_base-&amp;gt;expires_next.tv64 = KTIME_MAX;

    for (i = 0; i &amp;lt; HRTIMER_MAX_CLOCK_BASES; i++) { /*针对不同的计时参照对象依次处理*/
        struct hrtimer_clock_base *base;
        struct timerqueue_node *node;
        ktime_t basenow;

        if (!(cpu_base-&amp;gt;active_bases &amp;amp; (1 &amp;lt;&amp;lt; i)))
            continue;

        base = cpu_base-&amp;gt;clock_base + i;
        basenow = ktime_add(now, base-&amp;gt;offset);

        while ((node = timerqueue_getnext(&amp;amp;base-&amp;gt;active))) { /*根据到期时间依次处理红黑树中的定时器*/
            struct hrtimer *timer;

            timer = container_of(node, struct hrtimer, node);

            /*
             * The immediate goal for using the softexpires is
             * minimizing wakeups, not running timers at the
             * earliest interrupt after their soft expiration.
             * This allows us to avoid using a Priority Search
             * Tree, which can answer a stabbing querry for
             * overlapping intervals and instead use the simple
             * BST we already have.
             * We don't add extra wakeups by delaying timers that
             * are right-of a not yet expired timer, because that
             * timer will have to trigger a wakeup anyway.
             */

            if (basenow.tv64 &amp;lt; hrtimer_get_softexpires_tv64(timer)) {
                /*未到期则退出while循环*/

                ktime_t expires;

                expires = ktime_sub(hrtimer_get_expires(timer), base-&amp;gt;offset);
                if (expires.tv64 &amp;lt; 0)
                    expires.tv64 = KTIME_MAX;
                if (expires.tv64 &amp;lt; expires_next.tv64)
                    expires_next = expires;
                break;
            }

            __run_hrtimer(timer, &amp;amp;basenow); /*调用到期回调函数*/
        } /*end of while*/
    } /*end of for*/

    /*
     * Store the new expiry value so the migration code can verify
     * against it.
     */
    cpu_base-&amp;gt;expires_next = expires_next;
    raw_spin_unlock(&amp;amp;cpu_base-&amp;gt;lock);

    /*下面重新设置clock event device的中断触发时间，如果成功则返回*/
    /* Reprogramming necessary ? */
    if (expires_next.tv64 == KTIME_MAX ||
            !tick_program_event(expires_next, 0)) {
        cpu_base-&amp;gt;hang_detected = 0;
        return;
    }

    /*执行到此，后续的逻辑是处理一种特殊的场景，即定时器到期回调函数执行时间过长导致下一个定时器又到期了*/

    /*
     * The next timer was already expired due to:
     * - tracing
     * - long lasting callbacks
     * - being scheduled away when running in a VM
     *
     * We need to prevent that we loop forever in the hrtimer
     * interrupt routine. We give it 3 attempts to avoid
     * overreacting on some spurious event.
     *
     * Acquire base lock for updating the offsets and retrieving
     * the current time.
     */
    raw_spin_lock(&amp;amp;cpu_base-&amp;gt;lock);
    now = hrtimer_update_base(cpu_base);
    cpu_base-&amp;gt;nr_retries++;
    if (++retries &amp;lt; 3)
        goto retry;
    /*
     * Give the system a chance to do something else than looping
     * here. We stored the entry time, so we know exactly how long
     * we spent here. We schedule the next event this amount of
     * time away.
     */
    cpu_base-&amp;gt;nr_hangs++;
    cpu_base-&amp;gt;hang_detected = 1;
    raw_spin_unlock(&amp;amp;cpu_base-&amp;gt;lock);
    delta = ktime_sub(now, entry_time);
    if (delta.tv64 &amp;gt; cpu_base-&amp;gt;max_hang_time.tv64)
        cpu_base-&amp;gt;max_hang_time = delta;
    /*
     * Limit it to a sensible value as we enforce a longer
     * delay. Give the CPU at least 100ms to catch up.
     */
    if (delta.tv64 &amp;gt; 100 * NSEC_PER_MSEC)
        expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
    else
        expires_next = ktime_add(now, delta);
    tick_program_event(expires_next, 1);
    printk_once(KERN_WARNING &quot;hrtimer: interrupt took %llu ns\n&quot;, ktime_to_ns(delta));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过分析高精度模式下的时钟中断处理函数，我们可以发现它只负责处理定时器的到期处理。那么低精调模式下的进程调度的处理逻辑去哪里了？不需要了吗？其实，在前文代码中我们看到，高精度模式下内核会给每个CPU生成一个调度定时器：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-sched.c:

/**
 * tick_setup_sched_timer - setup the tick emulation timer
 */
void tick_setup_sched_timer(void)
{
    struct tick_sched *ts = &amp;amp;__get_cpu_var(tick_cpu_sched);
    ktime_t now = ktime_get();

    /*
     * Emulate tick processing via per-CPU hrtimers:
     */
    hrtimer_init(&amp;amp;ts-&amp;gt;sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
    ts-&amp;gt;sched_timer.function = tick_sched_timer; /*调度定时器的回调函数*/

    /* Get the next period (per cpu) */
    hrtimer_set_expires(&amp;amp;ts-&amp;gt;sched_timer, tick_init_jiffy_update());

    /* Offset the tick to avert jiffies_lock contention. */
    if (sched_skew_tick) {
        u64 offset = ktime_to_ns(tick_period) &amp;gt;&amp;gt; 1;
        do_div(offset, num_possible_cpus());
        offset *= smp_processor_id();
        hrtimer_add_expires_ns(&amp;amp;ts-&amp;gt;sched_timer, offset);
    }

    for (;;) {
        hrtimer_forward(&amp;amp;ts-&amp;gt;sched_timer, now, tick_period);
        hrtimer_start_expires(&amp;amp;ts-&amp;gt;sched_timer, HRTIMER_MODE_ABS_PINNED);
        /* Check, if the timer was already in the past */
        if (hrtimer_active(&amp;amp;ts-&amp;gt;sched_timer))
            break;
        now = ktime_get();
    }
    ...
}

/*
 * We rearm the timer until we get disabled by the idle code.
 * Called with interrupts disabled.
 */
static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
{
    struct tick_sched *ts =
        container_of(timer, struct tick_sched, sched_timer);
    struct pt_regs *regs = get_irq_regs();
    ktime_t now = ktime_get();

    tick_sched_do_timer(now);

    /*
     * Do not call, when we are not in irq context and have
     * no valid regs pointer
     */
    if (regs)
    tick_sched_handle(ts, regs);

    hrtimer_forward(timer, now, tick_period);

    return HRTIMER_RESTART;
}

static void tick_sched_do_timer(ktime_t now)
{
    int cpu = smp_processor_id();

    ...
    /* Check, if the jiffies need an update */
    if (tick_do_timer_cpu == cpu)
        tick_do_update_jiffies64(now);
}

static void tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)
{
    ...
    update_process_times(user_mode(regs));
    profile_tick(CPU_PROFILING);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  由此可见，调度定时器按tick_period周期性触发(暂不考虑动态时钟nohz特性)，每次到期后和处理逻辑和低精度模式下的逻辑类似。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/高精度定时器/&quot;&gt;【时间子系统】五、高精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】四、低精度定时器</title>
        <description>&lt;p&gt;  通过定时器，我们可以控制计算机在将来指定的某个时刻执行特定的动作。传统的定时器，以时钟滴答(jiffy)作为计时单位，因此它的精度较低(例如HZ=1000时，精度为1毫秒)，我们也称之为低精度定时器。&lt;/p&gt;

&lt;h3 id=&quot;1-初始化定时器&quot;&gt;1. 初始化定时器&lt;/h3&gt;

&lt;p&gt;  我们在概述中介绍过，内核中通过init_timer对定时器进行初始化，定时器中最关键的三个信息是：到期时间、到期处理函数、到期处理函数的参数。init_timer宏及定时器结构struct timer_list(取名struct timer可能更合适)的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timer.h:

#define init_timer(timer)                       \
    __init_timer((timer), 0)

#define __init_timer(_timer, _flags)            \
    init_timer_key((_timer), (_flags), NULL, NULL)

struct timer_list {
    /*
     * All fields that change during normal runtime grouped to the
     * same cacheline
     */
    struct list_head entry; /*用于将当前定时器挂到CPU的tvec_base链表中*/
    unsigned long expires; /*定时器到期时间*/
    struct tvec_base *base; /*定时器所属的tvec_base*/

    void (*function)(unsigned long); /*到期处理函数*/
    unsigned long data; /*到期处理函数的参数*/

    int slack; /*允许的偏差值*/

    ...
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  init_timer_key实现时，会将定时器指向执行初始化动作的CPU的tvec_base结构。内核为每个CPU分配一个struct tvec_base对象，用来记录每个CPU上定时器相关的全局信息(我们将在下一节详细说明)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * init_timer_key - initialize a timer
 * @timer: the timer to be initialized
 * @flags: timer flags
 * @name: name of the timer
 * @key: lockdep class key of the fake lock used for tracking timer
 *       sync lock dependencies
 *
 * init_timer_key() must be done to a timer prior calling *any* of the
 * other timer functions.
 */
void init_timer_key(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    debug_init(timer);
    do_init_timer(timer, flags, name, key);
}

static void do_init_timer(struct timer_list *timer, unsigned int flags,
    const char *name, struct lock_class_key *key)
{
    struct tvec_base *base = __raw_get_cpu_var(tvec_bases);

    timer-&amp;gt;entry.next = NULL;
    timer-&amp;gt;base = (void *)((unsigned long)base | flags);
    timer-&amp;gt;slack = -1;
    ...
}

struct tvec_base {
    spinlock_t lock; /*同步当前tvec_base的链表操作*/
    struct timer_list *running_timer; /*正在运行(到期触发)的定时器*/
    unsigned long timer_jiffies; /*用于判断定时器是否到期的当前时间，通常和系统的jiffies值相等*/
    unsigned long next_timer; /*下一个到期的定时器的到期时间*/
    unsigned long active_timers; /*激活的定时器的个数*/
    struct tvec_root tv1; /*tv1~tv5是用于保存已添加定时器的链表，也称为时间轮*/
    struct tvec tv2;
    struct tvec tv3;
    struct tvec tv4;
    struct tvec tv5;
} ____cacheline_aligned;

/*
 * per-CPU timer vector definitions:
 */
#define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
#define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
#define TVN_SIZE (1 &amp;lt;&amp;lt; TVN_BITS)
#define TVR_SIZE (1 &amp;lt;&amp;lt; TVR_BITS)
#define TVN_MASK (TVN_SIZE - 1)
#define TVR_MASK (TVR_SIZE - 1)
#define MAX_TVAL ((unsigned long)((1ULL &amp;lt;&amp;lt; (TVR_BITS + 4*TVN_BITS)) - 1))

struct tvec {
    struct list_head vec[TVN_SIZE];
};

struct tvec_root {
    struct list_head vec[TVR_SIZE];
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-添加定时器&quot;&gt;2. 添加定时器&lt;/h3&gt;

&lt;p&gt;  add_timer将定时器添加到执行CPU的tvec_base的时间轮链表中。内核根据定时器到期时间与当前时间jiffies的差值(值越小说明到期时间越早)，将定时器分别挂到五个级别的链表数组，级别越低链表到期时间越早，如下表所示：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;链表数组&lt;/th&gt;
      &lt;th&gt;时间差&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;tv1&lt;/td&gt;
      &lt;td&gt;0-255(2^8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv2&lt;/td&gt;
      &lt;td&gt;256–16383(2^14)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv3&lt;/td&gt;
      &lt;td&gt;16384–1048575(2^20)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv4&lt;/td&gt;
      &lt;td&gt;1048576–67108863(2^26)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tv5&lt;/td&gt;
      &lt;td&gt;67108864–4294967295(2^32)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  其中tv1的数组大小为TVR_SIZE， tv2 tv3 tv4 tv5的数组大小为TVN_SIZE，根据CONFIG_BASE_SMALL配置项的不同，它们有不同的大小。默认情况下，没有使能CONFIG_BASE_SMALL，TVR_SIZE的大小是256，TVN_SIZE的大小则是64，当需要节省内存空间时，也可以使能CONFIG_BASE_SMALL，这时TVR_SIZE的大小是64，TVN_SIZE的大小则是16，以下的讨论我都是基于没有使能CONFIG_BASE_SMALL的情况。当有一个新的定时器要加入时，系统根据定时器到期的jiffies值和timer_jiffies字段的差值来决定该定时器被放入tv1至tv5中的哪一个数组中，最终，系统中所有的定时器的组织结构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/timer_2.jpg&quot; height=&quot;350&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从add_timer代码实现上看，最终会调用__internal_add_timer并根据时间差将定时器加入到合适的链表中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

static void
__internal_add_timer(struct tvec_base *base, struct timer_list *timer)
{
    unsigned long expires = timer-&amp;gt;expires;
    unsigned long idx = expires - base-&amp;gt;timer_jiffies; /*idx即为时间差*/
    struct list_head *vec;

    if (idx &amp;lt; TVR_SIZE) {
        int i = expires &amp;amp; TVR_MASK; /*以超时时间(而非时间差idx)作为索引寻找对应的链表，方便后续的超时处理*/
        vec = base-&amp;gt;tv1.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; TVR_BITS) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv2.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 2 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv3.vec + i;
    } else if (idx &amp;lt; 1 &amp;lt;&amp;lt; (TVR_BITS + 3 * TVN_BITS)) {
        int i = (expires &amp;gt;&amp;gt; (TVR_BITS + 2 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv4.vec + i;
    } else if ((signed long) idx &amp;lt; 0) {
        /*
         * Can happen if you add a timer with expires == jiffies,
         * or you set a timer to go off in the past
         */
        vec = base-&amp;gt;tv1.vec + (base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK);
    } else {
        int i;
        /* If the timeout is larger than MAX_TVAL (on 64-bit
         * architectures or with CONFIG_BASE_SMALL=1) then we
         * use the maximum timeout.
         */
        if (idx &amp;gt; MAX_TVAL) {
            idx = MAX_TVAL;
            expires = idx + base-&amp;gt;timer_jiffies;
        }
        i = (expires &amp;gt;&amp;gt; (TVR_BITS + 3 * TVN_BITS)) &amp;amp; TVN_MASK;
        vec = base-&amp;gt;tv5.vec + i;
    }
    /*
     * Timers are FIFO:
     */
    list_add_tail(&amp;amp;timer-&amp;gt;entry, vec);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-触发定时器&quot;&gt;3. 触发定时器&lt;/h3&gt;

&lt;p&gt;  在时钟中断部分，我们提到过每次中断处理时都会调用run_local_timers进行本地定时器的处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/*
 * Called by the local, per-CPU timer interrupt on SMP.
 */
void run_local_timers(void)
{
    ...
    raise_softirq(TIMER_SOFTIRQ); /*最终在中断返回时进入软中断处理函数run_timer_softirq*/
}

/*
 * This function runs timers and the timer-tq in bottom half context.
 */
static void run_timer_softirq(struct softirq_action *h)
{
    struct tvec_base *base = __this_cpu_read(tvec_bases);

    ...

    if (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) /*实际当前时间晚于base中记录的当前时间，说明需要更新base中时间或者有定时器到期*/
        __run_timers(base);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  定时器的到期处理逻辑中，总是先处理tv1中的定时器，如果tv1中所有的链表为空，再从tv2中移动链表并重新添加到tv1中；如果tv1和tv2中为空，再从tv3中移动链表重新添加到tv1和tv2中；依此类推。代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/timer.c:

/**
 * __run_timers - run all expired timers (if any) on this CPU.
 * @base: the timer vector to be processed.
 *
 * This function cascades all vectors and executes all expired timer
 * vectors.
 */
static inline void __run_timers(struct tvec_base *base)
{
    struct timer_list *timer;

    spin_lock_irq(&amp;amp;base-&amp;gt;lock);
    while (time_after_eq(jiffies, base-&amp;gt;timer_jiffies)) {
        struct list_head work_list;
        struct list_head *head = &amp;amp;work_list;
        int index = base-&amp;gt;timer_jiffies &amp;amp; TVR_MASK; /*以base中的当前时间为索引取出已到期的定时器*/

        /*
         * Cascade timers:
         */
        /*如果低级链表为空，则从高级别链表中移动添加到低级别中*/
        if (!index &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv2, INDEX(0))) &amp;amp;&amp;amp;
            (!cascade(base, &amp;amp;base-&amp;gt;tv3, INDEX(1))) &amp;amp;&amp;amp;
            !cascade(base, &amp;amp;base-&amp;gt;tv4, INDEX(2)))
                cascade(base, &amp;amp;base-&amp;gt;tv5, INDEX(3));
        ++base-&amp;gt;timer_jiffies; /*累加base中当前时间*/
        list_replace_init(base-&amp;gt;tv1.vec + index, &amp;amp;work_list);
        /*处理已到期的定时期的回调函数*/
        while (!list_empty(head)) {
            void (*fn)(unsigned long);
            unsigned long data;
            bool irqsafe;

            timer = list_first_entry(head, struct timer_list,entry);
            fn = timer-&amp;gt;function;
            data = timer-&amp;gt;data;
            irqsafe = tbase_get_irqsafe(timer-&amp;gt;base);

            timer_stats_account_timer(timer);

            base-&amp;gt;running_timer = timer;
            detach_expired_timer(timer, base);

            if (irqsafe) {
                spin_unlock(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock(&amp;amp;base-&amp;gt;lock);
            } else {
                spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
                call_timer_fn(timer, fn, data);
                spin_lock_irq(&amp;amp;base-&amp;gt;lock);
            }
        }
    }
    base-&amp;gt;running_timer = NULL;
    spin_unlock_irq(&amp;amp;base-&amp;gt;lock);
}

#define INDEX(N) ((base-&amp;gt;timer_jiffies &amp;gt;&amp;gt; (TVR_BITS + (N) * TVN_BITS)) &amp;amp; TVN_MASK)

static int cascade(struct tvec_base *base, struct tvec *tv, int index)
{
    /* cascade all the timers from tv up one level */
    struct timer_list *timer, *tmp;
    struct list_head tv_list;

    list_replace_init(tv-&amp;gt;vec + index, &amp;amp;tv_list);

    /*
     * We are removing _all_ timers from the list, so we
     * don't have to detach them individually.
     */
    list_for_each_entry_safe(timer, tmp, &amp;amp;tv_list, entry) {
        BUG_ON(tbase_get_base(timer-&amp;gt;base) != base);
        /* No accounting, while moving them */
        __internal_add_timer(base, timer);
    }

    return index;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;【时间子系统】四、低精度定时器&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】三、时钟中断－定时基础</title>
        <description>&lt;p&gt;  时钟中断是各种定时器(timer)能够正常工作的前提，同时它和进程调度(tick事件)也密不可分，因此在分析定时器原理前，我们先来深入了解一下时钟中断的原理。&lt;/p&gt;

&lt;h3 id=&quot;1-中断初始化&quot;&gt;1. 中断初始化&lt;/h3&gt;

&lt;p&gt;  时钟中断涉及时钟事件设备(Clock Event Device)等多个概念，我们先通过分析初始化流程来理解这些概念。&lt;/p&gt;

&lt;h4 id=&quot;11-bsp初始化阶段&quot;&gt;&lt;strong&gt;1.1. BSP初始化阶段&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  时钟中断的初始化发生在启动CPU(BSP)上，由start_kernel函数作为总体入口。在完成IO-APIC中断控制器的相关初始化动作后，由late_time_init作为初始化入口。针对x86架构，该函数的实现体为x86_late_time_init：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/time.c:

void __init time_init(void)
{
    late_time_init = x86_late_time_init;
}

static __init void x86_late_time_init(void)
{
    x86_init.timers.timer_init(); /*指向hpet_time_init*/
    ...
}

void __init hpet_time_init(void)
{
    if (!hpet_enable())
        setup_pit_timer();
    setup_default_timer_irq();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  在我们的示例架构i440fx下，hpet没有使能，因此系统将使用PIT作为启动CPU(BSP)的本地tick设备(tick事件发生源)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/i8253.c:

void __init setup_pit_timer(void)
{
    clockevent_i8253_init(true); /*PIT芯片代号为8253*/
    global_clock_event = &amp;amp;i8253_clockevent;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PIT本质上是一种全局时钟事件设备，也就是说它不和某一个CPU绑定，这和后文将介绍的LAPIC Timer不同。然而在BSP初始化过程中，它将暂时被用作BSP的本地tick设备。后续在完成SMP的初始化后，每个CPU都有各自不同的本地tick设备(即本地LAPIC Timer)。tick设备的作用就是周期性(由内核配置参数HZ控制，例始HZ=1000代表每秒产生1000个tick中断)地产生时钟中断，CPU在处理中断的过程中可以决定是否需要进行进程调度。&lt;/p&gt;

&lt;p&gt;  每个时钟事件设备可以有两种工作模式：单次模式(oneshot)和周期模式(periodic)。工作在单次模式时，每次设置完到期时间后，时钟事件设备只会产生一次中断；而工作在周期模式时，时钟事件设备会以设定频率周期性地产生中断。单次模式相比周期模式具备更强的灵活性，我们可以动态控制时钟中断的间隔，从而实现像动态时钟(nohz)之类的高级特性(我们将在后续博文专题介绍)。从下面的代码中，我们可以看出PIT可以同时支持oneshot和periodic两种模式，并在初始化时指定其亲和CPU为当前执行CPU(即BSP)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/driver/clocksource/i8253.c:

/*
 * On UP the PIT can serve all of the possible timer functions. On SMP systems
 * it can be solely used for the global tick.
 */
struct clock_event_device i8253_clockevent = {
    .name           = &quot;pit&quot;,
    .features       = CLOCK_EVT_FEAT_PERIODIC,
    .set_mode       = init_pit_timer,
    .set_next_event = pit_next_event,
};

void __init clockevent_i8253_init(bool oneshot)
{
    if (oneshot)
        i8253_clockevent.features |= CLOCK_EVT_FEAT_ONESHOT;
    /*
     * Start pit with the boot cpu mask. x86 might make it global
     * when it is used as broadcast device later.
     */
    i8253_clockevent.cpumask = cpumask_of(smp_processor_id());

    clockevents_config_and_register(&amp;amp;i8253_clockevent, PIT_TICK_RATE,
            0xF, 0x7FFF);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  PIT是时钟事件设备的一种具体硬件实现，从软件抽象层来说，各种时钟事件设备都会调度内核的clockevents中的注册函数进行注册：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
/**
 * clockevents_config_and_register - Configure and register a clock event device
 * @dev:	device to register
 * @freq:	The clock frequency
 * @min_delta:	The minimum clock ticks to program in oneshot mode
 * @max_delta:	The maximum clock ticks to program in oneshot mode
 *
 * min/max_delta can be 0 for devices which do not support oneshot mode.
 */
void clockevents_config_and_register(struct clock_event_device *dev,
    u32 freq, unsigned long min_delta, unsigned long max_delta)
{
    dev-&amp;gt;min_delta_ticks = min_delta;
    dev-&amp;gt;max_delta_ticks = max_delta;
    clockevents_config(dev, freq); /*根据内部计数器频率计算相关转换参数*/
    clockevents_register_device(dev);
}

void clockevents_register_device(struct clock_event_device *dev)
{
    unsigned long flags;

    ...

    raw_spin_lock_irqsave(&amp;amp;clockevents_lock, flags);

    list_add(&amp;amp;dev-&amp;gt;list, &amp;amp;clockevent_devices); /*将当前设备加入到全局clockevent_devices链表中*/
    tick_check_new_device(dev); /*检测当前设备是否适合作当前执行CPU的本地tick设备或全局broadcast设备*/
    clockevents_notify_released(); /*对于被释放的设备，重新加入全局列表并作tick_check_new_device检测*/

    raw_spin_unlock_irqrestore(&amp;amp;clockevents_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  如上代码所示，新注册一个事钟设备时都会对其进行检测，以判断其是否适合作为本地tick设备(由struct tick_device定义，它是对struct clock_event_device的封装)。如果新的设备适合作本地tick设备，那将替换原有的tcik设备(如果在存的话)。被替换的老设备将有机会重新加入全局clockevent_devices链表并进行检测，此时的检测主要是判定它是否适合作为广播(broadcast)设备。广播设备的作用是为了当某些本地tick设备随CPU进入节电状态而停止工作时，能够再次发生中断以唤醒进入节电状态的CPU继续进行工作。这种情况下本地tick设备是无能为力的，因为它也随CPU进入睡眠状态了。这里我们只需要理解广播设备的作用，不用太深挖其内部实现细节：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

void tick_check_new_device(struct clock_event_device *newdev)
{
    struct clock_event_device *curdev;
    struct tick_device *td;
    int cpu;
    unsigned long flags;

    raw_spin_lock_irqsave(&amp;amp;tick_device_lock, flags);

    cpu = smp_processor_id(); /*取当前执行CPU*/
    if (!cpumask_test_cpu(cpu, newdev-&amp;gt;cpumask)) /*判断当前CPU是否在新设备的CPU掩码位中*/
        goto out_bc; /*不在，则转而判断新设备是否可作为bc(broadcast)设备*/

    td = &amp;amp;per_cpu(tick_cpu_device, cpu); /*取出当前CPU的本地tick设备*/
    curdev = td-&amp;gt;evtdev; /*本地tick设备所封装的当前时钟事件设备，可能为空*/

    /* cpu local device ? */
    if (!tick_check_percpu(curdev, newdev, cpu)) /*判断新设备是否更适合作本地tick设备*/
        goto out_bc; /*如果不合适则进行bc判定*/

    /* Preference decision */
    if (!tick_check_preferred(curdev, newdev)) /*判断新设备是否符合偏好，如oneshot优先等*/
        goto out_bc;

    if (!try_module_get(newdev-&amp;gt;owner))
        return;

    /*如果执行到这里，说明新设备newdev相比老设备curdev更适合作本地tick设备，将进行替换操作*/

    /*
     * Replace the eventually existing device by the new
     * device. If the current device is the broadcast device, do
     * not give it back to the clockevents layer !
     */
    if (tick_is_broadcast_device(curdev)) { /*如果老设备是一个广播设备将对其进行关闭*/
        clockevents_shutdown(curdev);
        curdev = NULL;
    }
    clockevents_exchange_device(curdev, newdev); /*进行交换*/
    tick_setup_device(td, newdev, cpu, cpumask_of(cpu)); /*重新设定新设备为本地tick设备*/
    if (newdev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_ONESHOT)
        tick_oneshot_notify();

    raw_spin_unlock_irqrestore(&amp;amp;tick_device_lock, flags);
    return;

out_bc:
    /*
     * Can the new device be used as a broadcast device ?
     */
    tick_install_broadcast_device(newdev);
    raw_spin_unlock_irqrestore(&amp;amp;tick_device_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于新设定的本地tick设备(没有老设备进行替换时)，初始时内核总是将其设为周期模式(periodic)。随着系统的运行，当外部条件成熟后，在时钟中断的处理过程中会将它的模式切换到单次模式(oneshot)以支持更高级功能。这部分切换我们将在高精度时钟部分进行分析。这里我们看看tick_setup_device的基本动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

static void tick_setup_device(struct tick_device *td,
        struct clock_event_device *newdev, int cpu,
        const struct cpumask *cpumask)
{
    ktime_t next_event;
    void (*handler)(struct clock_event_device *) = NULL;

    /*
     * First device setup ?
     */
    if (!td-&amp;gt;evtdev) {
        /*
         * If no cpu took the do_timer update, assign it to
         * this cpu:
         */
        if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
            if (!tick_nohz_full_cpu(cpu))
                tick_do_timer_cpu = cpu; /*动态时钟未打开情况下，初始化过程中首个注册本地tick设备的CPU将负责在中断处理时完成jiffies更新*/
            else
                tick_do_timer_cpu = TICK_DO_TIMER_NONE;
            tick_next_period = ktime_get(); /*下次tick事件发生时间，这里初始化为当前时间*/
            tick_period = ktime_set(0, NSEC_PER_SEC / HZ); /*tick的时间间隔*/
        }

        /*
         * Startup in periodic mode first.
         */
        td-&amp;gt;mode = TICKDEV_MODE_PERIODIC; /*首次注册tick设备时，将其设为periodic模式*/
    } else {
        handler = td-&amp;gt;evtdev-&amp;gt;event_handler;
        next_event = td-&amp;gt;evtdev-&amp;gt;next_event;
        td-&amp;gt;evtdev-&amp;gt;event_handler = clockevents_handle_noop;
    }

    td-&amp;gt;evtdev = newdev;

    /*
     * When the device is not per cpu, pin the interrupt to the
     * current cpu:
     */
    if (!cpumask_equal(newdev-&amp;gt;cpumask, cpumask))
        irq_set_affinity(newdev-&amp;gt;irq, cpumask);

    /*
     * When global broadcasting is active, check if the current
     * device is registered as a placeholder for broadcast mode.
     * This allows us to handle this x86 misfeature in a generic
     * way. This function also returns !=0 when we keep the
     * current active broadcast state for this CPU.
     */
    if (tick_device_uses_broadcast(newdev, cpu))
    return;

    if (td-&amp;gt;mode == TICKDEV_MODE_PERIODIC)
        tick_setup_periodic(newdev, 0);
    else
        tick_setup_oneshot(newdev, handler, next_event);
}

void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
    tick_set_periodic_handler(dev, broadcast); /*设置dev-&amp;gt;event_handler为tick_handle_periodic*/

    ...

    if ((dev-&amp;gt;features &amp;amp; CLOCK_EVT_FEAT_PERIODIC) &amp;amp;&amp;amp;
            !tick_broadcast_oneshot_active()) {
        clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC); /*设置时钟事件设备的工作模式为周期模式，内部将调用set_mode函数*/
    } else {
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  回到上层初始化流程，完成将PIT设置为BSP的本地tick设备后，内核在setup_default_timer_irq中完成中断处理函数的设定并使能中断信号。之后BSP在初始化过程中有会周期性地收到PIT产生的0号时钟中断，并进行中断处理。对于PIT时钟中断的处理我们将在下一节展开：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
static struct irqaction irq0  = {
    .handler    = timer_interrupt,
    .flags      = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,
    .name       = &quot;timer&quot;
};

void __init setup_default_timer_irq(void)
{
    setup_irq(0, &amp;amp;irq0); /*设备中断处理对象并使能中断信号，0号中断即时钟中断*/
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;12-smp初始化阶段&quot;&gt;&lt;strong&gt;1.2. SMP初始化阶段&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在x86 SMP系统中，每个CPU的Local APIC中都有一个高精度的时钟事件设备(LAPIC Timer)，因此在BSP初始化的最后阶段及AP的初始化过程中，都会调用setup_APIC_timer进行LAPIC Timer的初始化：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/apic/apic.c:

static void __cpuinit setup_APIC_timer(void)
{
    struct clock_event_device *levt = &amp;amp;__get_cpu_var(lapic_events); /*每个CPU对应的lapic timer*/

    if (this_cpu_has(X86_FEATURE_ARAT)) { /*ARAT: Always Run Apic Timer，intel实现的特性；timer不随CPU睡眠而停止*/
        lapic_clockevent.features &amp;amp;= ~CLOCK_EVT_FEAT_C3STOP;
        /* Make LAPIC timer preferrable over percpu HPET */
        lapic_clockevent.rating = 150;
    }

    memcpy(levt, &amp;amp;lapic_clockevent, sizeof(*levt));
    levt-&amp;gt;cpumask = cpumask_of(smp_processor_id());

    if (this_cpu_has(X86_FEATURE_TSC_DEADLINE_TIMER)) {
        ...
    } else
        clockevents_register_device(levt);
}

/*
 * The local apic timer can be used for any function which is CPU local.
 */
static struct clock_event_device lapic_clockevent = {
    .name       = &quot;lapic&quot;,
    .features   = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT
                    | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_DUMMY,/*DUMMY标志在BSP初始化时将会被清除*/
    .shift      = 32,
    .set_mode   = lapic_timer_setup,
    .set_next_event	= lapic_next_event,
    .broadcast  = lapic_timer_broadcast,
    .rating     = 100,
    .irq        = -1,
};
static DEFINE_PER_CPU(struct clock_event_device, lapic_events);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  setup_APIC_timer函数内部同样是调用clockevents_register_device进行注册，对于BSP它将使用lapic timer替换PIT作为本地tick设备，而PIT将设为广播设备；对于AP，将直接使用lapic timer作为本地tick设备。注意，对于lapic timer的处理函数入口为smp_apic_timer_interrupt，它是在中断系统初始化过程(start_kernel-&amp;gt;init_IRQ-&amp;gt;apic_intr_init)中设定的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/irqinit.c:

static void __init apic_intr_init(void)
{
    ...
    /*apic_timer_interrupt将跳转到smp_apic_timer_interrupt*/
    alloc_intr_gate(LOCAL_TIMER_VECTOR, apic_timer_interrupt);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-周期性中断处理&quot;&gt;2. 周期性中断处理&lt;/h3&gt;

&lt;h4 id=&quot;21-pit中断处理&quot;&gt;&lt;strong&gt;2.1. PIT中断处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  BSP完成对PIT的初始化并使能中断信号后，BSP便可周期性地接收到来自PIT的中断，它对该中断的处理句柄是：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/time.c:

/*
 * Default timer interrupt handler for PIT/HPET
 */
static irqreturn_t timer_interrupt(int irq, void *dev_id)
{
    global_clock_event-&amp;gt;event_handler(global_clock_event);
    return IRQ_HANDLED;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里的global_clock_event即是i8253_clockevent，它最初工作在周期模式下，相应的处理函数为tick_handle_periodic：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

/*
 * Event handler for periodic ticks
 */
void tick_handle_periodic(struct clock_event_device *dev)
{
    int cpu = smp_processor_id();
    ktime_t next;

    /*实际的周期性处理逻辑*/
    tick_periodic(cpu);

    /*对于周期模式的时钟事件设备直接返回，无须设置下次到期时间*/
    if (dev-&amp;gt;mode != CLOCK_EVT_MODE_ONESHOT)
        return;
    
    /*对于单次模式的设备，如果要实现周期性中断，则在每次中断处理中要设置下次到期时间*/
    /*
     * Setup the next period for devices, which do not have
     * periodic mode:
     */
    next = ktime_add(dev-&amp;gt;next_event, tick_period);
    for (;;) {
        if (!clockevents_program_event(dev, next, false))
            return;
        /*
         * Have to be careful here. If we're in oneshot mode,
         * before we call tick_periodic() in a loop, we need
         * to be sure we're using a real hardware clocksource.
         * Otherwise we could get trapped in an infinite
         * loop, as the tick_periodic() increments jiffies,
         * when then will increment time, posibly causing
         * the loop to trigger again and again.
         */
        if (timekeeping_valid_for_hres())
            tick_periodic(cpu);
        next = ktime_add(next, tick_period);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  tick_periodic负责实际的处理逻辑，它主要完成对jiffies和xtime(墙上时间)的周期性更新，并对进程进行运行计时和调度：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/tick-common.c:

/*
 * Periodic tick
 */
static void tick_periodic(int cpu)
{
    if (tick_do_timer_cpu == cpu) {
        /*如果当前CPU负责计时更新，则调用do_timer进行更新*/
        write_seqlock(&amp;amp;jiffies_lock);

        /* Keep track of the next tick event */
        tick_next_period = ktime_add(tick_next_period, tick_period);

        do_timer(1);
        write_sequnlock(&amp;amp;jiffies_lock);
    }

    /*更新进程运行时间并做调度判断*/
    update_process_times(user_mode(get_irq_regs()));
    profile_tick(CPU_PROFILING);
}

/*
 * Must hold jiffies_lock
 */
void do_timer(unsigned long ticks)
{
    jiffies_64 += ticks;
    update_wall_time(); /*周期性地更新墙上时间*/
    calc_global_load(ticks);
}

void update_process_times(int user_tick)
{
    struct task_struct *p = current;
    int cpu = smp_processor_id();

    /* Note: this timer irq context must be accounted for as well. */
    account_process_tick(p, user_tick); /*当前进程运行时间统计*/
    run_local_timers(); /*检查本地定时器，我们将在定时器部分分析*/
    ...
    scheduler_tick(); /*调度检测*/
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;22-lapic-timer中断处理&quot;&gt;&lt;strong&gt;2.2. LAPIC Timer中断处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  SMP初始化完成后，所有CPU的本地tick设备变更为LAPIC Timer，虽然其工作模式仍然是周期性模式，但中断处理函数入口变更为smp_apic_timer_interrupt：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/apic/apic.c:

void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
{
    struct pt_regs *old_regs = set_irq_regs(regs);

    /*
     * NOTE! We'd better ACK the irq immediately,
     * because timer handling can be slow.
     */
    ack_APIC_irq();
    /*
     * update_process_times() expects us to have done irq_enter().
     * Besides, if we don't timer interrupts ignore the global
     * interrupt lock, which is the WrongThing (tm) to do.
     */
    irq_enter();
    exit_idle();
    local_apic_timer_interrupt();
    irq_exit();

    set_irq_regs(old_regs);
}

/*
 * The guts of the apic timer interrupt
 */
static void local_apic_timer_interrupt(void)
{
    int cpu = smp_processor_id();
    struct clock_event_device *evt = &amp;amp;per_cpu(lapic_events, cpu);

    /*
     * Normally we should not be here till LAPIC has been initialized but
     * in some cases like kdump, its possible that there is a pending LAPIC
     * timer interrupt from previous kernel's context and is delivered in
     * new kernel the moment interrupts are enabled.
     *
     * Interrupts are enabled early and LAPIC is setup much later, hence
     * its possible that when we get here evt-&amp;gt;event_handler is NULL.
     * Check for event_handler being NULL and discard the interrupt as
     * spurious.
     */
    if (!evt-&amp;gt;event_handler) {
        pr_warning(&quot;Spurious LAPIC timer interrupt on cpu %d\n&quot;, cpu);
        /* Switch it off */
        lapic_timer_setup(CLOCK_EVT_MODE_SHUTDOWN, evt);
        return;
    }

    /*
     * the NMI deadlock-detector uses this.
     */
    inc_irq_stat(apic_timer_irqs);

    evt-&amp;gt;event_handler(evt);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  对于周期模式的LAPIC Timer，其event_hander仍然为tick_handle_periodic，因此核心处理逻辑和PIT是一样的。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/02/时钟中断/&quot;&gt;【时间子系统】三、时钟中断－定时基础&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/02/%E6%97%B6%E9%92%9F%E4%B8%AD%E6%96%AD/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/02/%E6%97%B6%E9%92%9F%E4%B8%AD%E6%96%AD/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】二、计时原理－timekeeper与clocksource</title>
        <description>&lt;p&gt;  本篇博文我们将深入分析一下内核是如何使用计时硬件对应用提供服务的。&lt;/p&gt;

&lt;h3 id=&quot;1-内核表示时间数据结构&quot;&gt;1. 内核表示时间数据结构&lt;/h3&gt;

&lt;p&gt;  内核中对时间的表示有多种形式，可以使用在不同的应用场景。我们在时间概述中看到的gettimeofday的示例中，采用的数据结构是struct timeval，它的定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/time.h:

struct timeval {
    __kernel_time_t         tv_sec;     /* seconds */
    __kernel_suseconds_t    tv_usec;    /* microseconds */
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  从上面的定义中，我们可以看到struct timeval记录了当前时间的秒数和毫秒数，精度就是毫秒。那么这里的秒数和毫秒数是相对哪个时间点(epoch)而言的呢？按照UNIX系统的习惯，记录时间的秒数和毫秒数是相对1970年1月1日00:00:00 +0000(UTC)而言的。另外，记录秒数的__kernel_time_t和记录毫秒的__kernel_suseconds_t在64位系统中都是long型的。&lt;/p&gt;

&lt;p&gt;  除了struct timeval，内核中还定义了精度更高的struct timespec，它的精度是纳秒：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/time.h:

struct timespec {
    __kernel_time_t tv_sec;     /* seconds */
    long            tv_nsec;    /* nanoseconds */
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  此外，为了兼容各种系统架构，内核也定义了ktime_t类型，在64位机器中对应long，时间表示单位是纳秒：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ktime.h:

union ktime {
    s64	tv64;
#if BITS_PER_LONG != 64 &amp;amp;&amp;amp; !defined(CONFIG_KTIME_SCALAR)
    struct {
#ifdef __BIG_ENDIAN
        s32	sec, nsec;
#else
        s32	nsec, sec;
#endif
    } tv;
#endif
};

typedef union ktime ktime_t;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-内核时间类别&quot;&gt;2. 内核时间类别&lt;/h3&gt;

&lt;p&gt;  时间概述中示例程序使用的gettimeofday将返回实时间(real time，或叫墙上时间wall time)，代表现实生活中使用的时间。除了墙上时间，内核也提供了线性时间(monotonic time，它不可调整，随系统运行线性增加，但不包括休眠时间)、启动时间(boot time，它也不可调整，并包括了休眠时间)等多种时间类型，以使应用在不同场景(获取不同类型时间的用户态方法是clock_gettime)，下表汇总了各类时间的要素点：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;时间类别&lt;/th&gt;
      &lt;th&gt;精度&lt;/th&gt;
      &lt;th&gt;可手动调整&lt;/th&gt;
      &lt;th&gt;受NTP调整影响&lt;/th&gt;
      &lt;th&gt;时间起点&lt;/th&gt;
      &lt;th&gt;受闰秒影响&lt;/th&gt;
      &lt;th&gt;系统暂停时是否可工作&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC_RAW&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME_COARSE&lt;/td&gt;
      &lt;td&gt;tick&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MONOTONIC_COARSE&lt;/td&gt;
      &lt;td&gt;tick&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BOOTTIME&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;machine start&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REALTIME_ALARM&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BOOTTIME_ALARM&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;machine start&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
      &lt;td&gt;YES&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TAI&lt;/td&gt;
      &lt;td&gt;ns&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;Linux epoch&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
      &lt;td&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;  关于闰秒，我们需要先理解什么是原子秒？原子秒提出的背景是人们对于”秒”的精确定义追求。多长时间可以算作1秒？这是一个很难准确回答的问题。但是后来科学家发现铯133原子在能量跃迁时辐射的电磁波振荡频率非常稳定，因此就被用来定义时间的基本单位：秒，即原子秒。通过原子秒延展出来的时间轴就是TAI(International Atomic Time)。原子时间虽然精准，但是对人类来说不太友好，它和传统的地球自转和公转的周期性自然现象存在时间差。在这样的背景下，UTC(Coordinated Universal Time)被提出。它使用原子秒作为计时单位，但又会适当调整以适应人们的日常生活。这个调整的时间差就是闰秒。TAI和UTC在1972进行了校准，两者相差10秒，从此后到2017年，又调整了27次，因此TAI比UTC快了37秒。&lt;/p&gt;

&lt;h3 id=&quot;3-深入do_gettimeofday&quot;&gt;3. 深入do_gettimeofday&lt;/h3&gt;

&lt;p&gt;  用户态gettimeofday接口在内核中是通过do_gettimeofday实现的，从调用层次上看，它可以分为timekeeper和clocksource两层。&lt;/p&gt;

&lt;h4 id=&quot;31-timekeeper&quot;&gt;&lt;strong&gt;3.1 timekeeper&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  timekeeper是内核中负责计时功能的核心对象，它通过使用当前系统中最优的clocksource来提供时间服务：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/**
 * do_gettimeofday - Returns the time of day in a timeval
 * @tv:		pointer to the timeval to be set
 *
 * NOTE: Users should be converted to using getnstimeofday()
 */
void do_gettimeofday(struct timeval *tv)
{
    struct timespec now;

    getnstimeofday(&amp;amp;now); /*获取纳秒精度的当前时间*/
    tv-&amp;gt;tv_sec = now.tv_sec;
    tv-&amp;gt;tv_usec = now.tv_nsec/1000;
}

/**
 * __getnstimeofday - Returns the time of day in a timespec.
 * @ts:		pointer to the timespec to be set
 *
 * Updates the time of day in the timespec.
 * Returns 0 on success, or -ve when suspended (timespec will be undefined).
 */
int __getnstimeofday(struct timespec *ts)
{
    struct timekeeper *tk = &amp;amp;timekeeper; /*系统全局对象timekeeper*/
    unsigned long seq;
    s64 nsecs = 0;

    do {
        seq = read_seqcount_begin(&amp;amp;timekeeper_seq); /*以顺序锁来同步各个任务对timekeeper的读写操作*/

        ts-&amp;gt;tv_sec = tk-&amp;gt;xtime_sec; /*获取最近更新的墙上时间的秒数(墙上时间会周期性地被更新，将在定时原理部分讨论)*/
        nsecs = timekeeping_get_ns(tk); /*获取当前墙上时间相对(tk-&amp;gt;xtime_sec, 0)的纳秒时间间隔*/

    } while (read_seqcount_retry(&amp;amp;timekeeper_seq, seq));

    ts-&amp;gt;tv_nsec = 0;
    timespec_add_ns(ts, nsecs);/*累加前面获取的纳秒时间间隔以得到正确的当前墙上时间；有可能导致秒数进位*/

    ...
    return 0;
}

static inline s64 timekeeping_get_ns(struct timekeeper *tk)
{
    cycle_t cycle_now, cycle_delta;
    struct clocksource *clock;
    s64 nsec;

    /*通过当前最优clocksource获取当前时间计数cycle；不同的clocksource可以提供不同的read实现*/
    /* read clocksource: */
    clock = tk-&amp;gt;clock;
    cycle_now = clock-&amp;gt;read(clock);
    
    /*通过clocksource中的当前计数值与最近一次更新墙上时间时获取的值的差值来计算时间间隔*/

    /* calculate the delta since the last update_wall_time: */    
    cycle_delta = (cycle_now - clock-&amp;gt;cycle_last) &amp;amp; clock-&amp;gt;mask;

    /*tk-&amp;gt;mult和tk-&amp;gt;shift是用来进行将cycle数值转成纳秒的转换参数，参见clocksource中的说明*/
    nsec = cycle_delta * tk-&amp;gt;mult + tk-&amp;gt;xtime_nsec; /*tk-&amp;gt;xtime_nsec是最近更新的墙上时间的秒纳数左移tk-&amp;gt;shift后的值*/
    nsec &amp;gt;&amp;gt;= tk-&amp;gt;shift;

    /* If arch requires, add in get_arch_timeoffset() */
    return nsec + get_arch_timeoffset();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/timekeeper_internal.h:

/* Structure holding internal timekeeping values. */
struct timekeeper {
    /* Current clocksource used for timekeeping. */
    struct clocksource	*clock;
    /* NTP adjusted clock multiplier */
    u32			mult;
    /* The shift value of the current clocksource. */
    u32			shift;
    /* Number of clock cycles in one NTP interval. */
    cycle_t			cycle_interval;
    /* Last cycle value (also stored in clock-&amp;gt;cycle_last) */
    cycle_t			cycle_last;
    /* Number of clock shifted nano seconds in one NTP interval. */
    u64			xtime_interval;
    /* shifted nano seconds left over when rounding cycle_interval */
    s64			xtime_remainder;
    /* Raw nano seconds accumulated per NTP interval. */
    u32			raw_interval;

    /* Current CLOCK_REALTIME time in seconds */
    u64			xtime_sec;
    /* Clock shifted nano seconds */
    u64			xtime_nsec;

    /* Difference between accumulated time and NTP time in ntp
     * shifted nano seconds. */
    s64			ntp_error;
    /* Shift conversion between clock shifted nano seconds and
     * ntp shifted nano seconds. */
    u32			ntp_error_shift;

    /*
     * wall_to_monotonic is what we need to add to xtime (or xtime corrected
     * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
     * at zero at system boot time, so wall_to_monotonic will be negative,
     * however, we will ALWAYS keep the tv_nsec part positive so we can use
     * the usual normalization.
     *
     * wall_to_monotonic is moved after resume from suspend for the
     * monotonic time not to jump. We need to add total_sleep_time to
     * wall_to_monotonic to get the real boot based time offset.
     *
     * - wall_to_monotonic is no longer the boot time, getboottime must be
     * used instead.
     */
    struct timespec		wall_to_monotonic;
    /* Offset clock monotonic -&amp;gt; clock realtime */
    ktime_t			offs_real;
    /* time spent in suspend */
    struct timespec		total_sleep_time;
    /* Offset clock monotonic -&amp;gt; clock boottime */
    ktime_t			offs_boot;
    /* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
    struct timespec		raw_time;
    /* The current UTC to TAI offset in seconds */
    s32			tai_offset;
    /* Offset clock monotonic -&amp;gt; clock tai */
    ktime_t			offs_tai;

};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;32-clocksource&quot;&gt;&lt;strong&gt;3.2 clocksource&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  内核通过clocksource对象来描述物理计时设备，x86架构下最常见的计时设备是tsc，我们来看看tsc对应的clocksource:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/kernel/tsc.c:

static struct clocksource clocksource_tsc = {
    .name                   = &quot;tsc&quot;,
    .rating                 = 300,
    .read                   = read_tsc,
    .resume                 = resume_tsc,
    .mask                   = CLOCKSOURCE_MASK(64),
    .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
                              CLOCK_SOURCE_MUST_VERIFY,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/clocksource.h:

/**
 * struct clocksource - hardware abstraction for a free running counter
 *	Provides mostly state-free accessors to the underlying hardware.
 *	This is the structure used for system time.
 *
 * @name:		ptr to clocksource name
 * @list:		list head for registration
 * @rating:		rating value for selection (higher is better)
 *			To avoid rating inflation the following
 *			list should give you a guide as to how
 *			to assign your clocksource a rating
 *			1-99: Unfit for real use
 *				Only available for bootup and testing purposes.
 *			100-199: Base level usability.
 *				Functional for real use, but not desired.
 *			200-299: Good.
 *				A correct and usable clocksource.
 *			300-399: Desired.
 *				A reasonably fast and accurate clocksource.
 *			400-499: Perfect
 *				The ideal clocksource. A must-use where
 *				available.
 * @read:		returns a cycle value, passes clocksource as argument
 * @enable:		optional function to enable the clocksource
 * @disable:		optional function to disable the clocksource
 * @mask:		bitmask for two's complement
 *			subtraction of non 64 bit counters
 * @mult:		cycle to nanosecond multiplier
 * @shift:		cycle to nanosecond divisor (power of two)
 * @max_idle_ns:	max idle time permitted by the clocksource (nsecs)
 * @maxadj:		maximum adjustment value to mult (~11%)
 * @flags:		flags describing special properties
 * @archdata:		arch-specific data
 * @suspend:		suspend function for the clocksource, if necessary
 * @resume:		resume function for the clocksource, if necessary
 * @cycle_last:		most recent cycle counter value seen by ::read()
 */
struct clocksource {
    /*
     * Hotpath data, fits in a single cache line when the
     * clocksource itself is cacheline aligned.
     */
    cycle_t (*read)(struct clocksource *cs);
    cycle_t cycle_last;
    cycle_t mask;
    u32 mult;
    u32 shift;
    u64 max_idle_ns;
    u32 maxadj;
#ifdef CONFIG_ARCH_CLOCKSOURCE_DATA
    struct arch_clocksource_data archdata;
#endif

    const char *name;
    struct list_head list;
    int rating;
    int (*enable)(struct clocksource *cs);
    void (*disable)(struct clocksource *cs);
        unsigned long flags;
    void (*suspend)(struct clocksource *cs);
    void (*resume)(struct clocksource *cs);

    /* private: */
#ifdef CONFIG_CLOCKSOURCE_WATCHDOG
    /* Watchdog related data, used by the framework */
    struct list_head wd_list;
    cycle_t cs_last;
    cycle_t wd_last;
#endif
} ____cacheline_aligned;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  上面代码的注释部分已经清楚地解析了tsc是一种精度良好的clocksource，我们可以使用read_tsc(本质是通过rdtsc指令)来获取当前tsc计数值。cycle_last表示最近一次从clocksource中获取的cycle计数值；mask表示当前clocksource中计数器的有效位数；mult和shift用来计算从cycle到纳秒的转换；max_idle_ns表示当前clocksource允许的最长时间更新间隔，因为如果CPU长期不更新时间，将会导致再次获取到的cycle计数值过大，使得转换成纳秒时发生溢出错误。从理论计算上说，将cycle转换成纳秒的公式是”cycle * 每秒纳秒数 / 频率”，但是由于内核无法进行浮点运算，只能通过一种变通的方法来计算，即”cycle * mult » shift”，这里的mult和shif就是基于频率、计算精度和最大表示范围计算而得的。&lt;/p&gt;

&lt;h3 id=&quot;4-计时初始化&quot;&gt;4. 计时初始化&lt;/h3&gt;

&lt;p&gt;  最后我们再来看看内核计时功能的初始化过程，了解一下计时功能是如何一步步生效的。timekeeper的初始化是在内核启动过程start_kernel中调用timekeeping_init进行：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/*
 * timekeeping_init - Initializes the clocksource and common timekeeping values
 */
void __init timekeeping_init(void)
{
    struct timekeeper *tk = &amp;amp;timekeeper;
    struct clocksource *clock;
    unsigned long flags;
    struct timespec now, boot, tmp;

    /*x86架构下，persistent clock为系统RTC时钟源，我们先从中获取当前时间，精度为秒*/
    read_persistent_clock(&amp;amp;now);

    if (!timespec_valid_strict(&amp;amp;now)) {
        pr_warn(&quot;WARNING: Persistent clock returned invalid value!\n&quot;
                &quot;         Check your CMOS/BIOS settings.\n&quot;);
        now.tv_sec = 0;
        now.tv_nsec = 0;
    } else if (now.tv_sec || now.tv_nsec)
        persistent_clock_exist = true;

    /*x86架构下，没有boot clock，所以boot time为0*/
    read_boot_clock(&amp;amp;boot);
    if (!timespec_valid_strict(&amp;amp;boot)) {
        pr_warn(&quot;WARNING: Boot clock returned invalid value!\n&quot;
                &quot;         Check your CMOS/BIOS settings.\n&quot;);
        boot.tv_sec = 0;
        boot.tv_nsec = 0;
    }

    raw_spin_lock_irqsave(&amp;amp;timekeeper_lock, flags);
    write_seqcount_begin(&amp;amp;timekeeper_seq);
    ntp_init();

    /*获取系统默认时钟源，x86架构中即为jiffies*/
    clock = clocksource_default_clock();
    if (clock-&amp;gt;enable)
    clock-&amp;gt;enable(clock);
    /*将jiffy设备timekeeper中的时钟源并设定内部相关变量*/
    tk_setup_internals(tk, clock);

    /*将当前时间设定为tk的墙上时间，注其中tk-&amp;gt;xtime_sec为当前秒数，tk-&amp;gt;xtime_nsec为纳秒左移shift位后的值*/
    tk_set_xtime(tk, &amp;amp;now);
    /*raw_time设为0*/
    tk-&amp;gt;raw_time.tv_sec = 0;
    tk-&amp;gt;raw_time.tv_nsec = 0;
    /*boot time设为墙上时间*/
    if (boot.tv_sec == 0 &amp;amp;&amp;amp; boot.tv_nsec == 0)
        boot = tk_xtime(tk);

    /*将monotonic time减去wall time的时间偏移记录下来*/
    set_normalized_timespec(&amp;amp;tmp, -boot.tv_sec, -boot.tv_nsec);
    tk_set_wall_to_mono(tk, tmp);

    /*将sleep time初始化为零*/
    tmp.tv_sec = 0;
    tmp.tv_nsec = 0;
    tk_set_sleep_time(tk, tmp);

    /*备份当前timekeeper*/
    memcpy(&amp;amp;shadow_timekeeper, &amp;amp;timekeeper, sizeof(timekeeper));

    write_seqcount_end(&amp;amp;timekeeper_seq);
    raw_spin_unlock_irqrestore(&amp;amp;timekeeper_lock, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  内核jiffies变量代表时间滴答，是对同期性tick事件的记数，因此可以将它视为一个最为简单的时钟源。它和一般时钟源的不同之处在于它没有实际的计时设备与之对应，完全是记录在计算机内存中；另外它的精度和系统tick数相关。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/jiffies.c:

struct clocksource * __init __weak clocksource_default_clock(void)
{
    return &amp;amp;clocksource_jiffies;
}

static struct clocksource clocksource_jiffies = {
    .name		= &quot;jiffies&quot;,
    .rating		= 1, /* lowest valid rating*/
    .read		= jiffies_read,
    .mask		= 0xffffffff, /*32bits*/
    .mult		= NSEC_PER_JIFFY &amp;lt;&amp;lt; JIFFIES_SHIFT, /* details above */
    .shift		= JIFFIES_SHIFT,
};

static cycle_t jiffies_read(struct clocksource *cs)
{
    return (cycle_t) jiffies;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于tsc时钟源，在内核对模块进行初始化时，会注册tsc时钟，并通知timekeeper将时钟源从jiffies切换到tsc:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int __init init_tsc_clocksource(void)
{
    ...
    if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE)) {
        /*注册一个频率为tsc_khz的时钟源，最终调用__clocksource_register_scale实现*/
        clocksource_register_khz(&amp;amp;clocksource_tsc, tsc_khz);
    return 0;
    }
    ...
}
/*
 * We use device_initcall here, to ensure we run after the hpet
 * is fully initialized, which may occur at fs_initcall time.
 */
device_initcall(init_tsc_clocksource);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/clocksource.c:

/**
 * __clocksource_register_scale - Used to install new clocksources
 * @cs:		clocksource to be registered
 * @scale:	Scale factor multiplied against freq to get clocksource hz
 * @freq:	clocksource frequency (cycles per second) divided by scale
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 *
 * This *SHOULD NOT* be called directly! Please use the
 * clocksource_register_hz() or clocksource_register_khz helper functions.
 */
int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
{

    /*首先根据时钟源的频率计算合适的mult和shift，以及最大更新延时max_idle_ns*/
    /* Initialize mult/shift and max_idle_ns */
    __clocksource_updatefreq_scale(cs, scale, freq);

    /* Add clocksource to the clcoksource list */
    mutex_lock(&amp;amp;clocksource_mutex);
    clocksource_enqueue(cs); /*加入到全局clocksource_list*/
    clocksource_enqueue_watchdog(cs); /*加入到时钟源监控中，如果发现当前时钟源精度下降会重新选择更优的时钟源*/
    clocksource_select(); /*选择最优的时钟源，内部会调用timekeeping_notify通知timerkeeper*/
    mutex_unlock(&amp;amp;clocksource_mutex);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/time/timekeeping.c:

/**
* timekeeping_notify - Install a new clock source
* @clock:		pointer to the clock source
*
* This function is called from clocksource.c after a new, better clock
* source has been registered. The caller holds the clocksource_mutex.
*/
void timekeeping_notify(struct clocksource *clock)
{
    struct timekeeper *tk = &amp;amp;timekeeper;

    if (tk-&amp;gt;clock == clock)
        return;
    /*注意，这里会暂停所有CPU的运行，并选定一个默认的CPU(0号核)执行change_clocksource。
      这是因为时钟源是计时的基础，在进行时钟源切换时系统将无法提供正确的时间服务。只有当切换
      完成后系统才可恢复运行。*/
    stop_machine(change_clocksource, clock, NULL);
    tick_clock_notify();
}

/**
 * change_clocksource - Swaps clocksources if a new one is available
 *
 * Accumulates current time interval and initializes new clocksource
 */
static int change_clocksource(void *data)
{
    struct timekeeper *tk = &amp;amp;timekeeper;
    struct clocksource *new, *old;
    unsigned long flags;

    new = (struct clocksource *) data;

    raw_spin_lock_irqsave(&amp;amp;timekeeper_lock, flags);
    write_seqcount_begin(&amp;amp;timekeeper_seq);

    timekeeping_forward_now(tk);
    if (!new-&amp;gt;enable || new-&amp;gt;enable(new) == 0) {
        old = tk-&amp;gt;clock;
        tk_setup_internals(tk, new);
        if (old-&amp;gt;disable)
            old-&amp;gt;disable(old);
    }
    timekeeping_update(tk, true, true);

    write_seqcount_end(&amp;amp;timekeeper_seq);
    raw_spin_unlock_irqrestore(&amp;amp;timekeeper_lock, flags);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/计时/&quot;&gt;【时间子系统】二、计时原理－timekeeper与clocksource&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%AE%A1%E6%97%B6/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%AE%A1%E6%97%B6/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【时间子系统】一、概述</title>
        <description>&lt;p&gt;  除了计算、存储、网络等核心子系统外，计算机内部还包含时间子系统，它对系统的运行起着重要作用。&lt;/p&gt;

&lt;h3 id=&quot;什么是计算机的时间子系统&quot;&gt;什么是计算机的时间子系统？&lt;/h3&gt;

&lt;p&gt;  计算机内的时间子系统包含多种时间设备，我们可以把这些设备分为两大类：&lt;strong&gt;计时&lt;/strong&gt;设备和&lt;strong&gt;定时通知&lt;/strong&gt;设备。&lt;/p&gt;

&lt;p&gt;  我们可以将计时设备理解为生活中所见的”墙上挂钟”或者”手表”，它们为系统提供了时间(时刻)。常见的计时设备有TSC(Time Stamp Counter)、RTC(Real Time Clock)、ACPI_PM：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;TSC是每个CPU内部的一个计数器，它按CPU主频以固定频率递增，例如一个2400Mhz的CPU，计数器每秒逐一增加2400M个计数值。计数器在CPU启动时初始化为零，假设我们已知CPU启动时刻，那么只要把当前计算器的值除以频率再加上启动时刻，就可以得知当前时间了。&lt;/li&gt;
    &lt;li&gt;RTC是位于CMOS电路中的一个计时设备，它与TSC相比的优点是有独立的电池供电，即使计算机下电，RTC计时器仍可以继续工作；缺点是计数频率较低，因此时间精度较差。通常我们初始时将RTC计数器的值设置为当前时刻相对于1970年1月1日的时间差，因此通过RTC提供的寄存器接口，我们可以直接获取到当前时间。&lt;/li&gt;
    &lt;li&gt;ACPI_PM通常是南桥中的APCI电源管理模块提供的计时设备，其精度较低，通常不推荐使用。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  我们可以将定时通知设备理解为生活中所见的”闹钟”，它们周期性地或者在一定时间间隔后向系统通知到期事件。常见的通知设备有Local APIC Timer、PIT(Programmable Interval Timer)、HPET(High Precision Event Timer)。初始时我们向时间通知设备的计数器中写入一个到期计数值，然后时间通知设备按固定频率递减计数器中的值，当计数器值为零时便通过中断向CPU通知事件发生。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Local APIC Timer是每个CPU的本地中断控制器(APIC)内部的定时设备，精度较高，是系统正常运行时采用的通知设备。&lt;/li&gt;
    &lt;li&gt;PIT是CPU之外的独立定时通知设备，属全局设备，精度较低，通常不使用。&lt;/li&gt;
    &lt;li&gt;HPET也是全局定时通知设备，精度较高，需要系统中含专属硬件。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  计算机用户是无法直接使用这些时间设备的，必须通过运行在CPU上的内核程序，应用程序或者用户才能最终获取计时和时间通知服务。因此我们可以将计算机时间子系统分为硬件和软件两部分：各种时间设备属于硬件部分，内核使能这些硬件的模块(也称内核时间子系统)属于软件部分。内核中将计时设备称为&lt;strong&gt;时钟源(Clock Source)&lt;/strong&gt;，将定时通知设备称为&lt;strong&gt;时钟事件设备(Clock Event Device)&lt;/strong&gt;。时间子系统整体结构如下图所示(感谢droidphone的分享，&lt;a href=&quot;http://blog.csdn.net/droidphone/article/details/8017604&quot;&gt;原文链接&lt;/a&gt;)，后续我们将深入内核分析其实现原理。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/time_1.jpg&quot; height=&quot;280&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;h3 id=&quot;为什么需要时间子系统&quot;&gt;为什么需要时间子系统？&lt;/h3&gt;

&lt;p&gt;  基于各种时间设备和内核时间子系统模块，应用程序可以实现计时和到期通知(&lt;strong&gt;定时器&lt;/strong&gt;)功能：例如我们桌面应用中的日历程序就使用了计时功能来提供实时时间，又例如邮件客户端的定时接收功能就使用了定时器。此外，内核自己的进程调度功能也依赖于内核的定时器，从而进行周期性的调度决策。&lt;/p&gt;

&lt;p&gt;  下面我们给出了两段示例程序来展示时间子系统的基本使用方法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//用户态计时功能示例程序

#include&amp;lt;stdio.h&amp;gt;
#include&amp;lt;sys/time.h&amp;gt;
#include&amp;lt;unistd.h&amp;gt;

int main()
{
    struct  timeval    tv;
    struct  timezone   tz;

    gettimeofday(&amp;amp;tv,&amp;amp;tz); /*获取当前时间*/

    printf(“tv_sec:%d\n”,tv.tv_sec);
    printf(“tv_usec:%d\n”,tv.tv_usec);

    printf(“tz_minuteswest:%d\n”,tz.tz_minuteswest);
    printf(“tz_dsttime:%d\n”,tz.tz_dsttime);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//用户态定时器参考setitime，这里以内核使用定时器示例

/*1. 初始化定时器结构*/
init_timer(&amp;amp;wb_timer);

/*2. 定时器超时函数*/
wb_timer.function = wb_timer_function; 

/*3.或者初始化定时器和超时函数作为一步(data作为fn的参数)*/
setup_timer(timer, fn, data)    

/*4. 添加定时器*/
add_timer(&amp;amp;buttons_timer); 

/*5. 设置定时器超时时间 1\100 s（修改一次超时时间只会触发一次定时器*/
mod_timer(&amp;amp;buttons_timer, jiffies+HZ/100); 

/*6.删除定时器*/
del_timer(&amp;amp;timer);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;内核如何实现时间子系统&quot;&gt;内核如何实现时间子系统？&lt;/h3&gt;

&lt;p&gt;  后续我们将以一系统博文深入分析内核时间子系统的实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/01/计时/&quot;&gt;计时原理－timekeeper与clocksource&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/时钟中断/&quot;&gt;时钟中断－定时基础&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/低精度定时器/&quot;&gt;低精度定时器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/02/高精度定时器/&quot;&gt;高精度定时器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;动态时钟－nohz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/时间子系统概述/&quot;&gt;【时间子系统】一、概述&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 19 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E6%97%B6%E9%97%B4%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E6%97%B6%E9%97%B4%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【计算子系统】进程管理之二：进程替换</title>
        <description>&lt;p&gt;  本篇讨论进程替换(exec)，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是进程替换为什么需要它&quot;&gt;什么是进程替换？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  进程替换是用新的代码和数据替换当前进程已有代码和数据，从而开始执行新的业务逻辑。&lt;/p&gt;

&lt;p&gt;  通过fork创建出来的进程是继承父进程的代码和数据，如果想要进程执行一些新的任务，那就得从磁盘程序中加载新的代码和数据并替换当前进程已有的代码和数据。&lt;/p&gt;

&lt;h3 id=&quot;如何实现进程替换&quot;&gt;如何实现进程替换？&lt;/h3&gt;

&lt;h4 id=&quot;1-exec用户态示例代码&quot;&gt;&lt;strong&gt;1. exec用户态示例代码&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们先来看看在用户态程序中是如何实现替换的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exec_test.c:

#include &amp;lt;stdio.h&amp;gt;  
#include &amp;lt;unistd.h&amp;gt;  

/*示例函数fork一个新进程并在其中执行ps命令*/
int main()  
{
    /*构造命令行参数ps_argv和环境变量ps_envp*/
    char *const ps_argv[] ={&quot;ps&quot;, &quot;-o&quot;, &quot;pid,ppid,pgrp,session,tpgid,comm&quot;, NULL};  
    char *const ps_envp[] ={&quot;PATH=/bin:/usr/bin&quot;, &quot;TERM=console&quot;, NULL};

    if(fork()==0){ 
        /*执行execve系统调用，第一个参数表示可执行文件的位置，第二个表示命令行参数，第三个表示环境变量。
          这里注意exec是一个函数簇，其有6种类似的系统调用：
          (1)6种调用的前4个字符相同，均是exec;
          (2)第5位有v和l两种，v表示向量表示法，l表示逐个列举;
          (3)第6位有e和p两种，e表示带环境变量，p表示可执行程序以文件名方式查找，而不是路径查找;*/
        if(execve(&quot;/bin/ps&quot;, ps_argv, ps_envp) &amp;lt; 0)  {  
            perror(&quot;execve error!&quot;);  
            return -1 ;  
        }  
    }  
    return 0 ;  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-elf可执行文件格式解析&quot;&gt;&lt;strong&gt;2. ELF可执行文件格式解析&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  通过上面的例子，我们看到应用程序通过调用execve系统调用实现执行程序的替换。Linux平台上主流的可执行文件格式是ELF(Executable and Linkable Format，类似Windows平台上的exe文件格式)，如果想深入分析execve调用功能，那我们就得了解ELF文件格式，详细规范说明&lt;a href=&quot;http://www.skyfree.org/linux/references/ELF_Format.pdf&quot;&gt;点此&lt;/a&gt;进入。&lt;/p&gt;

&lt;h5 id=&quot;21-示例程序&quot;&gt;&lt;strong&gt;2.1. 示例程序&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  真正理解ELF格式后，我们能够将C语言源文件与ELF二进制文件进行对应，这样可以提升对底层系统问题的定位能力。因此为方便大家入门，这里以一个简单的C语言程序为例，来逐一对应ELF中的各部分：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/tmp_analyze_elf/main.c:

#include &amp;lt;stdio.h&amp;gt;

void say_hello(char *who)
{
    printf(&quot;hello, %s!\n&quot;, who);
}

char *my_name = &quot;wb&quot;;

int main()
{
    say_hello(my_name);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们将其编译生成名为app的可执行程序并运行：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;linux-XqAfQZ:~/tmp_analyze_elf # &lt;strong&gt;gcc -o&lt;/strong&gt; app main.c&lt;br /&gt;
linux-XqAfQZ:~/tmp_analyze_elf # &lt;strong&gt;./app&lt;/strong&gt;&lt;br /&gt;
hello, wb!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;22-elf整体布局&quot;&gt;&lt;strong&gt;2.2. ELF整体布局&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  ELF格式可以表达三种类型的二进制对象文件(object files)：可重定位文件(relocatable file，就是大家平常见到的.o文件)、可执行文件(executable file, 例上述示例代码生成的app文件)、共享库文件(shared object files，就是.so文件，用来做动态链接)。可重定位文件用在编译和链接阶段；可执行文件用在程序运行阶段；共享库则同时用在编译链接和运行阶段。在不同阶段，我们可以用不同视角来理解ELF文件，整体布局如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_1.jpg&quot; height=&quot;300&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从上图可见，ELF格式文件整体可分为四大部分：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;ELF Header, ELF头部，定义全局性信息；&lt;/li&gt;
    &lt;li&gt;Program Header Table， 描述段(Segment)信息的数组，每个元素对应一个段；通常包含在可执行文件中，可重定文件中可选(通常不包含)；&lt;/li&gt;
    &lt;li&gt;Segment and Section，段(Segment)由若干区(Section)组成；段在运行时被加载到进程地址空间中，包含在可执行文件中；区是段的组成单元，包含在可执行文件和可重定位文件中；&lt;/li&gt;
    &lt;li&gt;Section Header Table，描述区(Section)信息的数组，每个元素对应一个区；通常包含在可重定位文件中，可执行文件中为可选(通常包含)；&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;23-elf-header实例解析&quot;&gt;&lt;strong&gt;2.3. ELF Header实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  ELF规范中对ELF Header中各字段的定义如下所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_2.jpg&quot; height=&quot;300&quot; width=&quot;350&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  接下来我们通过readelf -h命令来看看示例程序app中的ELF Header内容，显示结果如下图：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-header.jpg&quot; height=&quot;380&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;e_ident&lt;/strong&gt;含前16个字节，又可细分成class、data、version等字段，具体含义不用太关心，只需知道前4个字节点包含”ELF”关键字，这样可以判断当前文件是否是ELF格式；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_type&lt;/strong&gt;表示具体ELF类型，可重定位文件/可执行文件/共享库文件，显然这里是一个可执行文件；&lt;strong&gt;e_machine&lt;/strong&gt;表示执行的机器平台，这里是x86_64；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_version&lt;/strong&gt;表示文件版本号，这里的1表示初始版本号；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_entry&lt;/strong&gt;对应”Entry point address”，程序入口函数地址，通过进程虚拟地址空间地址(0x400440)表达；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phoff&lt;/strong&gt;对应“Start of program headers”，表示program header table在文件内的偏移位置，这里是从第64号字节(假设初始为0号字节)开始；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shoff&lt;/strong&gt;对应”Start of section headers”，表示section header table在文件内的偏移位置，这里是从第4472号字节开始，靠近文件尾部；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_flags&lt;/strong&gt;表示与CPU处理器架构相关的信息，这里为零；&lt;strong&gt;e_ehsize&lt;/strong&gt;对应”Size of this header”，表示本ELF header自身的长度，这里为64个字节，回看前面的e_phoff为64，说明ELF header后紧跟着program header table；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phentsize&lt;/strong&gt;对应“Size of program headers”，表示program header table中每个元素的大小，这里为56个字节；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_phnum&lt;/strong&gt;对应”Number of program headers”，表示program header table中元素个数，这里为9个；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shentsize&lt;/strong&gt;对应”Size of section headers”，表示section header table中每个元素的大小，这里为64个字节；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;e_shnum&lt;/strong&gt;对应”Number of section headers”，表示section header table中元素的个数，这里为30个；&lt;/li&gt;
    &lt;li&gt;最后， &lt;strong&gt;e_shstrndx&lt;/strong&gt;对应”Section header string table index”，表示描述各section字符名称的string table在section header table中的下标，详见后文对string table的介绍。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;24-program-header-table实例解析&quot;&gt;&lt;strong&gt;2.4. Program Header Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  Program Header Table是一个数组，每个元素叫Program Header，规范对其结构定义如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_3.jpg&quot; height=&quot;200&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  同样我们用readelf -l命令查看示例程序的program header table：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-program-header1.jpg&quot; height=&quot;500&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  上图截取了readelf命令返回的上半部，我们重点看下前面几项：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;PHDR&lt;/strong&gt;，此类型header元素描述了program header table自身的信息。从这里的内容看出，示例程序的program header table在文件中的偏移(Offset)为0x40，即64号字节处；该段映射到进程空间的虚拟地址(VirtAddr)为0x400040；PhysAddr暂时不用，其保持和VirtAddr一致；该段占用的文件大小FileSiz为00x1f8；运行时占用进程空间内存大小MemSiz也为0x1f8；Flags标记表示该段的读写权限，这里”R E”表示可读可执行，说明本段属于代码段；Align对齐为8，表明本段按8字节对齐。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;INTERP&lt;/strong&gt;，此类型header元素描述了一个特殊内存段，该段内存记录了动态加载解析器的访问路径字符串。示例程序中，该段内存位于文件偏移0x238处，即紧跟program header table；映射的进程虚拟地址空间地址为0x400238；文件长度和内存映射长度均为0x1c，即28个字符，具体内容为”/lib64/ld-linux-x86-64.so.2”；段属性为只读，并按字节对齐；&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;LOAD&lt;/strong&gt;，此类型header元素描述了可加载到进程空间的代码段或数据段：第三项为代码段，文件内偏移为0，映射到进程地址0x400000处，代码段长度为0x764个字节，属性为只读可执行，段地址按2M边界对齐；第四段为数据段，文件内偏移为0xe10，映射到进程地址为0x600e10处(按2M对齐移动)，文件大小为0x230，内存大小为0x238(因为其内部包含了8字节的bss段，即未初始化数据段，该段内容不占文件空间，但在运行时需要为其分配空间并清零)，属性为读写，段地址也按2M边界对齐。&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;DYNAMIC&lt;/strong&gt;，此类型header元素描述了动态加载段，其内部通常包含了一个名为”.dynamic”的动态加载区；这也是一个数组，每个元素描述了与动态加载相关的各方面信息，我们将在动态加载中介绍。该段是从文件偏移0xe28处开始，长度为0x1d0，并映射到进程的0x600e28；可见该段和上一个数据段是有重叠的。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  readelf命令返回内容的下半部分给出了各段(segment)和各区(section)之间的包含关系，如下图所示。INTERP段只包含了”.interp”区；代码段包含”.interp”、”.plt”、”.text”等区；数据段包含”.dynamic”、”.data”、”.bss”等区；DYNAMIC段包含”.dynamic”区。从这里可以看出，有些区被包含在多个段中。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-program-header2.jpg&quot; height=&quot;100&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;25-section-header-table实例解析&quot;&gt;&lt;strong&gt;2.5. Section Header Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  针对各区的描述信息由Section Header Table提供，该数组中每个元素的定义如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_4.jpg&quot; height=&quot;200&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  下面我们再通过readelf -S命令看看示例程序中section header table的内容，如下图所示。示例程序共生成30个区，Name表示每个区的名字，Type表示每个区的功能，Address表示每个区的进程映射地址，Offset表示文件内偏移，Size表示区的大小，EntSize表示区中每个元素的大小(如果该区为一个数组的话，否则该值为0)，Flags表示每个区的属性(参见图中最后的说明)，Link和Info记录不同类型区的相关信息(不同类型含义不同，具体参见规范)，Align表示区的对齐单位。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-section-header1.jpg&quot; height=&quot;600&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-section-header2.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;26-string-table实例解析&quot;&gt;&lt;strong&gt;2.6. String Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  从上述Section Header Table示例中，我们看到有一种类型为STRTAB的区(在Section Header Table中的下标为6,27,29)。此类区叫做String Table，其作用是集中记录字符串信息，其它区在需要使用字符串的时候，只需要记录字符串起始地址在该String Table表中的偏移即可，而无需包含整个字符串内容。&lt;/p&gt;

&lt;p&gt;  我们使用readelf -x读出下标27区的详细内容观察：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-strtable1.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  红框内为该区实际内容，左侧为区内偏移地址，后侧为对应内容的字符表示。我们可以发现，这里其实是一堆字符串，这些字符串对应的就是各个区的名字。因此section header table中每个元素的Name字段其实是这个string table的索引。再回头看看ELF header中的e_shstrndx，它的值正好就是27，指向了当前的string table。&lt;/p&gt;

&lt;p&gt;  同理再来看下29区的内容，如下图所示。这里我们看到了”main”、”say_hello”字符串，这些是我们在示例中源码中定义的符号，由此可以29区是应用自身的String Table，记录了应用使用的字符串。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-strtable2.jpg&quot; height=&quot;700&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;27-symbol-table实例解析&quot;&gt;&lt;strong&gt;2.7. Symbol Table实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  Section Header Table中，还有一类SYMTAB(DYNSYM)区，该区叫符号表。符号表中的每个元素对应一个符号，记录了每个符号对应的实际数值信息，通常用在重定位过程中或问题定位过程中，进程执行阶段并不加载符号表。符号表中每个元素定义如下：name表示符号对应的源码字符串，为对应String Table中的索引；value表示符号对应的数值；size表示符号对应数值的空间占用大小；info表示符号的相关信息，如符号类型(变量符号、函数符号)；shndx表示与该符号相关的区的索引，例如函数符号与对应的代码区相关。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process2_5.jpg&quot; height=&quot;160&quot; width=&quot;300&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们用readelf -s读出示例程序中的符号表，如下图所示。如红框中内容所示，我们示例程序定义的main函数符号对应的数值为0x400554，其类型为FUNC，大小为26字节，对应的代码区在Section Header Table中的索引为13；say_hello函数符号对应数值为0x400530，其类型为FUNC，大小为36字节，对应的代码区也为13。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-symtable1.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-symtable2.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h5 id=&quot;28-代码段实例解析&quot;&gt;&lt;strong&gt;2.8. 代码段实例解析&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  在理解了String Table和Symbol Table的作用后，我们通过objdump反汇编来理解一下.text代码段：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-main-code.jpg&quot; height=&quot;300&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  这里截取了与示例程序相关部分，我们看到0x400530和0x400554两处各定义一个函数，其符号分别为say_hello和main，这部分信息实际是通过符号表解析而来的；在涉及到内存地址的指令中，除了对数据段地址的引用是通过绝对地址进行的之外，对于代码段地址的引用都是以相对地址的方式进行的，这样做的好处是在二进制文件的重定位过程中，我们不用修改指令的访问地址(因为相对地址保持不变)；最后，我们看到对于库函数printf的访问指向了代码段地址0x400410，那么这个地址处放的是printf函数么？要回答这个问题就涉及动态链接，我们将在下文专题分析。&lt;/p&gt;

&lt;h5 id=&quot;29-动态链接dynamic-linking&quot;&gt;&lt;strong&gt;2.9. 动态链接(Dynamic Linking)&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  基于模块化设计思路，我们在应用开发时会将基础的、共用的功能抽取出来，设计成可共享的库。应用程序在编译时只是建立了与共享库的联系，并不将其包含在内；运行时，由系统负责加载所需的共享库。这就是动态链接，如此一来，既可以节省磁盘和内存的空间占用(相同功能在磁盘和内存中均只存在一份)，也可以方便基础模块自身的优化改进。&lt;/p&gt;

&lt;p&gt;  要实现动态链接，需要解决两个大的问题：(1)共享库内部的函数的地址访问需要与加载地址无关，因为不同的程序可能将库加载到不同的地址处；回顾2.8中的代码分析，我们看到这个可以通过相对寻址的方式解决。(2)调用共享库的应用程序如何能够获知共享库的加载地址并准确对其进行调用？&lt;/p&gt;

&lt;p&gt;  ELF规范对问题2的解决方法给出明确思路：系统中需要有一个Program Interpreter配合内核完成进程执行上下文的准备。Program Interpreter可以是一个可执行程序，也可以是一个共享库；在Linux x86_64平台下，这个解析器就是/lib64/ld-ld-linux-x86-64.so.2，就是由INTERP段指明的。解析器和内核需要配合完成以下动作：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;内核加载可执行程序和解析器到进程空间，之后将控制权交给解析器；&lt;/li&gt;
    &lt;li&gt;解析器加载共享库到进程空间；&lt;/li&gt;
    &lt;li&gt;解析器进行重定位；&lt;/li&gt;
    &lt;li&gt;解析器将控制权交给进程正常执行。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  解析器工作时需要从DYNAMIC段中获取信息，并通过GOT(Global Offset Table)记录解析后的库函数地址；应用程序通过PLT(Procedure Linkage Table)中的代码间接访问GOT，并最终完成向目标库函数的跳转。&lt;/p&gt;

&lt;p&gt;  结合我们的示例程序，我们通过readelf -d获取DYNAMIC段中的信息，如下图所示。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  重点看一下红框中标出的两行，NEEDED表示当前可执行程序依赖的共享库，这里只有libc.so.6一项；PLTGOT表示当前程序调用共享库时使用的GOT表的地址为0x601000，回看前文的Section Header Table，可知对应区的索引为23。我们接着可以看看该区的内容：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic-got.jpg&quot; height=&quot;100&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从23区的section header中可知该区为一个数组，每个元素大小为8字节。结合规范中的说明可知，GOT中的前三个元素有着特殊作用：第一个元素转换成地址为0x600e28，即为DYNAMIC段的映射地址；第二元素和第三个元素会在解析器获得控制权后被放置动态解析函数的参数和入口地址，其作用将在后续说明PLT功能是说明。从第四个元素起，每个元素代表一个调用地址，依次为0x400416、0x400426、0x400436，这些地址对应什么内容呢？&lt;/p&gt;

&lt;p&gt;  我们通过objdump来看看.plt段的内容：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/elf-dynamic-plt.jpg&quot; height=&quot;250&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  plt包含几段相似的汇编指令，回顾前文，.text代码段的say_hello在调用printf时访问的函数地址即为0x40010，对应plt中第二段汇编的第一条指令。这是一条jump指令，跳转到0x601018即GOT表的第4个元素。前文分析时指出，GOT中第4个元素为0x400416，正好对应jump指令的下一条指令。感觉上转了一圈却只是跳到了下一条指令处，有点多余，那么我们接着往下分析。后续的push指令把0压入了栈中(代表每个调用函数的索引，这里printf索引值为0),然后跳转到plt表中的第一段汇编指令处。这里把GOT表的第二个元素压力栈中，然后跳转到GOT表中第三个元素指定的函数。这个函数是解析器的动态解析函数，它的作用是针对栈中的调用函数索引，找到调用函数的实际加载地址，并将该地址填入GOT表中对应的元素位置(这里就是将printf的实际加载地址填入0x601018处(即GOT表中的第4个元素))，然后跳转到printf处执行。当应用程序再次调用printf时，跳转到plt中对应的函数后，那里的jump指令将根据GOT表中被解析器更新后的地址直接跳转到printf处开始执行，这样就不用解析器的干预了，从而达到了动态跳转的目的。&lt;/p&gt;

&lt;p&gt;  我们对动态调用做个总结：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;需要进行动态调用的可执行程序在编译时会自动生成DYNAMIC段、GOT表和PLT表；&lt;/li&gt;
    &lt;li&gt;对动态库的每个函数调用都会在GOT(从第4个元素开始)和PLT(从第二段汇编指令开始)中生成一项；&lt;/li&gt;
    &lt;li&gt;解析器在获得控制权后会在GOT第2个元素和第3个元素放置解析函数的参数和入口地址；PLT的第一段汇编指令会将GOT第2个元素压栈并跳转到第3个元素指定的函数位置；&lt;/li&gt;
    &lt;li&gt;进行过一次动态调用后，GOT中对应的元素中就记录了库函数的实际加载地址，后续的调用就可以进行直接跳转。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;3-深入内核sys_execve&quot;&gt;&lt;strong&gt;3. 深入内核sys_execve&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们在2.9节中介绍过，内核和解析器相互配合完了进程执行空间的替换，下面我们深入内核代码来看看sys_execve的实现原理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/fs/exec.c:

SYSCALL_DEFINE3(execve,
        const char __user *, filename,
        const char __user *const __user *, argv,
        const char __user *const __user *, envp)
{
    struct filename *path = getname(filename);
    int error = PTR_ERR(path);
    if (!IS_ERR(path)) {
        error = do_execve(path-&amp;gt;name, argv, envp);
        putname(path);
    }
    return error;
}

int do_execve(const char *filename,
            const char __user *const __user *__argv,
    const char __user *const __user *__envp)
{
    struct user_arg_ptr argv = { .ptr.native = __argv };
    struct user_arg_ptr envp = { .ptr.native = __envp };
    return do_execve_common(filename, argv, envp);
}

/*
 * sys_execve() executes a new program.
 */
static int do_execve_common(const char *filename,
                    struct user_arg_ptr argv,
                    struct user_arg_ptr envp)
{
    struct linux_binprm *bprm;
    struct file *file;
    struct files_struct *displaced;
    bool clear_in_exec;
    int retval;
    const struct cred *cred = current_cred();

    ...

    retval = -ENOMEM;
    bprm = kzalloc(sizeof(*bprm), GFP_KERNEL);
    if (!bprm)
        goto out_files;

    ...

    /*打开可执行文件*/
    file = open_exec(filename);
    ...

    bprm-&amp;gt;file = file;
    bprm-&amp;gt;filename = filename;
    bprm-&amp;gt;interp = filename;

    /*初始化将替换当前进程的mm_struct*/
    retval = bprm_mm_init(bprm);
    ...

    /*计算命令行参数和环境变量个数*/
    bprm-&amp;gt;argc = count(argv, MAX_ARG_STRINGS);
    ...
    bprm-&amp;gt;envc = count(envp, MAX_ARG_STRINGS);
    ...

    /*准备bprm中相关信息并读取可执行文件头部*/
    retval = prepare_binprm(bprm);
    ...

    /*复制信息到用户栈空间*/
    retval = copy_strings_kernel(1, &amp;amp;bprm-&amp;gt;filename, bprm);
    ...
    bprm-&amp;gt;exec = bprm-&amp;gt;p;
    retval = copy_strings(bprm-&amp;gt;envc, envp, bprm);
    ...
    retval = copy_strings(bprm-&amp;gt;argc, argv, bprm);
    ...

    /*在内核中搜索能够加载当前可执行文件的二进制解析驱动，这里是elf驱动，
      最终会调用elf_load_binary*/
    retval = search_binary_handler(bprm);
    ...

    return retval;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/fs/binfmt_elf.c:

static int load_elf_binary(struct linux_binprm *bprm)
{
    struct file *interpreter = NULL; /* to shut gcc up */
    unsigned long load_addr = 0, load_bias = 0;
    int load_addr_set = 0;
    char * elf_interpreter = NULL;
    unsigned long error;
    struct elf_phdr *elf_ppnt, *elf_phdata;
    unsigned long elf_bss, elf_brk;
    int retval, i;
    unsigned int size;
    unsigned long elf_entry;
    unsigned long interp_load_addr = 0;
    unsigned long start_code, end_code, start_data, end_data;
    unsigned long reloc_func_desc __maybe_unused = 0;
    int executable_stack = EXSTACK_DEFAULT;
    unsigned long def_flags = 0;
    struct pt_regs *regs = current_pt_regs();
    struct {
        struct elfhdr elf_ex; /*记录当前可执行程序的elf header*/
        struct elfhdr interp_elf_ex; /*记录程序解析器的elf header*/
    } *loc;

    loc = kmalloc(sizeof(*loc), GFP_KERNEL);
    if (!loc) {
        retval = -ENOMEM;
        goto out_ret;
    }

    /* Get the exec-header */
    loc-&amp;gt;elf_ex = *((struct elfhdr *)bprm-&amp;gt;buf); /*当前可执行程序头部128字节已经读到buff中*/

    retval = -ENOEXEC;
    /* First of all, some simple consistency checks */
    if (memcmp(loc-&amp;gt;elf_ex.e_ident, ELFMAG, SELFMAG) != 0) /*较验头部魔术字*/
        goto out;

    if (loc-&amp;gt;elf_ex.e_type != ET_EXEC &amp;amp;&amp;amp; loc-&amp;gt;elf_ex.e_type != ET_DYN) /*校验可执行文件类型*/
        goto out;
    ...
    if (!bprm-&amp;gt;file-&amp;gt;f_op || !bprm-&amp;gt;file-&amp;gt;f_op-&amp;gt;mmap)
        goto out;

    /* Now read in all of the header information */
    if (loc-&amp;gt;elf_ex.e_phentsize != sizeof(struct elf_phdr)) /*校验program header大小*/
        goto out;
    if (loc-&amp;gt;elf_ex.e_phnum &amp;lt; 1 ||
            loc-&amp;gt;elf_ex.e_phnum &amp;gt; 65536U / sizeof(struct elf_phdr)) /*校验program header 个数*/
        goto out;
    size = loc-&amp;gt;elf_ex.e_phnum * sizeof(struct elf_phdr);
    retval = -ENOMEM;
    elf_phdata = kmalloc(size, GFP_KERNEL);
    if (!elf_phdata)
        goto out;

    /*根据e_phoff记录的文件偏移读取program header table*/
    retval = kernel_read(bprm-&amp;gt;file, loc-&amp;gt;elf_ex.e_phoff,
            (char *)elf_phdata, size);
    if (retval != size) {
        if (retval &amp;gt;= 0)
            retval = -EIO;
        goto out_free_ph;
    }

    elf_ppnt = elf_phdata;
    elf_bss = 0;
    elf_brk = 0;

    start_code = ~0UL;
    end_code = 0;
    start_data = 0;
    end_data = 0;

    /*从program header table中找出INTERP段获取程序解析器的访问路径并加载其头部*/
    for (i = 0; i &amp;lt; loc-&amp;gt;elf_ex.e_phnum; i++) {
        if (elf_ppnt-&amp;gt;p_type == PT_INTERP) {
            ...
            elf_interpreter = kmalloc(elf_ppnt-&amp;gt;p_filesz, GFP_KERNEL);
            ...
            retval = kernel_read(bprm-&amp;gt;file, elf_ppnt-&amp;gt;p_offset, elf_interpreter,
                elf_ppnt-&amp;gt;p_filesz);
            ...
            interpreter = open_exec(elf_interpreter);
            ...
            retval = kernel_read(interpreter, 0, bprm-&amp;gt;buf, BINPRM_BUF_SIZE);

            loc-&amp;gt;interp_elf_ex = *((struct elfhdr *)bprm-&amp;gt;buf);
            break;
        }
        elf_ppnt++;
    }

    ...
    /* Some simple consistency checks for the interpreter */
    if (elf_interpreter) {
        ...
    }

    /* Flush all traces of the currently running executable */
    retval = flush_old_exec(bprm); /*替换当前进程的mm_struct*/
    if (retval)
        goto out_free_dentry;

    ...

    setup_new_exec(bprm);

    ...
    retval = setup_arg_pages(bprm, randomize_stack_top(STACK_TOP), executable_stack);
    if (retval &amp;lt; 0) {
        send_sig(SIGKILL, current, 0);
        goto out_free_dentry;
    }

    current-&amp;gt;mm-&amp;gt;start_stack = bprm-&amp;gt;p;

    /* Now we do a little grungy work by mmapping the ELF image into
       the correct location in memory. */
    for(i = 0, elf_ppnt = elf_phdata; i &amp;lt; loc-&amp;gt;elf_ex.e_phnum; i++, elf_ppnt++) {
        int elf_prot = 0, elf_flags;
        unsigned long k, vaddr;
        unsigned long total_size = 0;

        if (elf_ppnt-&amp;gt;p_type != PT_LOAD) /*只处理LOAD段*/
            continue;

        ...
        /*根据段的读写属性设置elf_prot*/
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_R)
            elf_prot |= PROT_READ;
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_W)
            elf_prot |= PROT_WRITE;
        if (elf_ppnt-&amp;gt;p_flags &amp;amp; PF_X)
            elf_prot |= PROT_EXEC;

        elf_flags = MAP_PRIVATE | MAP_DENYWRITE | MAP_EXECUTABLE;

        vaddr = elf_ppnt-&amp;gt;p_vaddr; /*段映射的进程虚拟地址，例如示例程的代码段映射到0x400000*/
        if (loc-&amp;gt;elf_ex.e_type == ET_EXEC || load_addr_set) {
            elf_flags |= MAP_FIXED;
        } else if (loc-&amp;gt;elf_ex.e_type == ET_DYN) {
            ...
        }

        /*将LOAD段映射到进程的vaddr处*/
        error = elf_map(bprm-&amp;gt;file, load_bias + vaddr, elf_ppnt,
                elf_prot, elf_flags, total_size);
        ...

        if (!load_addr_set) {
            load_addr_set = 1;
            load_addr = (elf_ppnt-&amp;gt;p_vaddr - elf_ppnt-&amp;gt;p_offset); /*段起始映射位置*/
            if (loc-&amp;gt;elf_ex.e_type == ET_DYN) {
                ...
            }
        }
        /*下面这段代码用来计算各段的加载位置，针对示例程序：
           start_code = 0x400000;
           end_code = 0x40764;
           start_data = 0x600e10;
           end_data = 0x601040;
           elf_bss = 0x601040;
           elf_brk = 0x601048; */
        k = elf_ppnt-&amp;gt;p_vaddr;
        if (k &amp;lt; start_code)
            start_code = k;
        if (start_data &amp;lt; k)
            start_data = k;

        ...
        k = elf_ppnt-&amp;gt;p_vaddr + elf_ppnt-&amp;gt;p_filesz;

        if (k &amp;gt; elf_bss)
            elf_bss = k;
        if ((elf_ppnt-&amp;gt;p_flags &amp;amp; PF_X) &amp;amp;&amp;amp; end_code &amp;lt; k)
            end_code = k;
        if (end_data &amp;lt; k)
            end_data = k;
        k = elf_ppnt-&amp;gt;p_vaddr + elf_ppnt-&amp;gt;p_memsz;
        if (k &amp;gt; elf_brk)
            elf_brk = k;
    }

    loc-&amp;gt;elf_ex.e_entry += load_bias;
    elf_bss += load_bias;
    elf_brk += load_bias;
    start_code += load_bias;
    end_code += load_bias;
    start_data += load_bias;
    end_data += load_bias;

    /* Calling set_brk effectively mmaps the pages that we need
     * for the bss and break sections.  We must do this before
     * mapping in the interpreter, to make sure it doesn't wind
     * up getting placed where the bss needs to go.
     */
    retval = set_brk(elf_bss, elf_brk); /*为bss映射匿名页并清零*/
    ...

    if (elf_interpreter) {
        unsigned long interp_map_addr = 0;
        
        /*elf_entry记录了execve调用返回后将执行的函数入口，这里指向程序解析器linux-ld*/
        elf_entry = load_elf_interp(&amp;amp;loc-&amp;gt;interp_elf_ex, interpreter, 
                &amp;amp;interp_map_addr, load_bias);
        if (!IS_ERR((void *)elf_entry)) {
            interp_load_addr = elf_entry;
            elf_entry += loc-&amp;gt;interp_elf_ex.e_entry;
        }
        ...
    }

    kfree(elf_phdata);

    set_binfmt(&amp;amp;elf_format);

    /*在用户态栈中放入elf解析获得的相关信息，供用户态程序使用*/
    retval = create_elf_tables(bprm, &amp;amp;loc-&amp;gt;elf_ex, load_addr, interp_load_addr);
    ...
    /* N.B. passed_fileno might not be initialized? */
    current-&amp;gt;mm-&amp;gt;end_code = end_code;
    current-&amp;gt;mm-&amp;gt;start_code = start_code;
    current-&amp;gt;mm-&amp;gt;start_data = start_data;
    current-&amp;gt;mm-&amp;gt;end_data = end_data;
    current-&amp;gt;mm-&amp;gt;start_stack = bprm-&amp;gt;p;
    ...

    /*修改regs指向的栈寄存器，使得execve调用返回到用户空间时，入口函数为elf_entry，栈指向bprm-&amp;gt;p*/
    start_thread(regs, elf_entry, bprm-&amp;gt;p);
    retval = 0;
out:
    kfree(loc);
out_ret:
    return retval;

/* error cleanup */
out_free_dentry:
    allow_write_access(interpreter);
    if (interpreter)
        fput(interpreter);
out_free_interp:
    kfree(elf_interpreter);
out_free_ph:
    kfree(elf_phdata);
goto out;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，我们已经将进程生命周期中最基本的fork和exec分析完成，内容较多，建议大家多思考多理解，打好基础。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/进程替换/&quot;&gt;【计算子系统】进程管理之二：进程替换&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E6%9B%BF%E6%8D%A2/</guid>
        
        <category>自顶向下分析计算机系统(基于Linux内核)</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】Client内核RBD驱动分析－网络信使messenger</title>
        <description>&lt;h3 id=&quot;4-libcephko中messenger模块分析&quot;&gt;4. libceph.ko中messenger模块分析&lt;/h3&gt;

&lt;p&gt;  messenger模块(信使)是libceph中相对比较独立的部分，旨在为上层各种网络客户端(如mon_client、osd_client)提供稳定可靠、有序的网络服务。messenger构建在网络TCP协议之上，虽然TCP协议本身是面向连接且可靠的网络协议，但是TCP连接有可能断开(broken，非长连接情况下会发生；长连接存在底层网络故障排除后上层应用感知不及时的问题)。为解决TCP短连接不可靠的问题，messenger通过间隔性地重连方案(back off，当有消息要发送时)或者等待方案(stand by，当无消息要发送时)来解决。同时会给每个发送的消息带上唯一的序列号(seq)，用以区分是否为重复消息。&lt;/p&gt;

&lt;p&gt;  messenger模块内部架构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_6.jpg&quot; height=&quot;400&quot; width=&quot;700&quot; /&gt;  
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;从发送流程来看(图中实线)，应用程序(如rbd命令)调用ceph_con_send进行消息发送；该函数会唤醒发送连接对应的工作任务；在工作任务进程中，通过try_write函数调用kernel_sendmsg访问底层网络协议栈，最终触发网卡发包；&lt;/li&gt;
    &lt;li&gt;从接收流程来看(图中虚线)，网卡收包后通过中断和协议栈回调唤醒工作任务；工作任务通过try_read调用kernel_recvmsg从网络协议栈中读取消息内容；最后调用连接初始化时指定的消息回调函数对消息进行处理。&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;41-正常网络收发的处理&quot;&gt;&lt;strong&gt;4.1. 正常网络收发的处理&lt;/strong&gt;&lt;/h4&gt;

&lt;h5 id=&quot;411-socket连接阶段&quot;&gt;&lt;strong&gt;4.1.1 socket连接阶段&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  每当上层通过ceph_con_init创建一个ceph_connection对象(对底层TCP连接的封装，含有自身的状态变化)并调用ceph_con_open打开该连接后(连接状态变为PREOPEN)，就会在内核工作队列中添加一项新的任务，并对该任务进行一次调度，其入口函数为con_work，代码框架如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con); /*socket连接初始化阶段不进行任何工作*/
        ...
        ret = try_write(con); /*建立socket连接并设定socket回调函数*/
        ...
        break;
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  conection worker首次被调度时进入socket连接阶段，完成的主要工作是建立TCP socket连接，并将ceph_connection的状态置为CONNECTING：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static int try_write(struct ceph_connection *con)
{
    ...

    if (con-&amp;gt;state == CON_STATE_PREOPEN) { /*首次被调度时，连接状态为PREOPEN*/
        BUG_ON(con-&amp;gt;sock);
        con-&amp;gt;state = CON_STATE_CONNECTING; /*执行完下面这些动作后状态更新为CONNECTING*/

        con_out_kvec_reset(con); /*重置当前连接con的发送缓冲区，对应con-&amp;gt;out_kvec_* */
        prepare_write_banner(con); /*将客户端的banner内容放入发送缓冲区*/
        prepare_read_banner(con); /*准备接收服务端的banner内容*/

        BUG_ON(con-&amp;gt;in_msg);
        con-&amp;gt;in_tag = CEPH_MSGR_TAG_READY; /*重置消息接收状态为READY，表示可接收任意消息类别，不同类别由CEPH_MSGR_TAG_*进行区分 */
        ret = ceph_tcp_connect(con); /*创建并连接socket，同时指定协议栈的回调函数*/
        if (ret &amp;lt; 0) {
            con-&amp;gt;error_msg = &quot;connect error&quot;;
            goto out;
        }
    }

more_kvec:
    ...
    if (con-&amp;gt;out_kvec_left) { 
        /*调用底层kernel_sendmsg将当前连接con发送缓冲中的内容进行发送*/
        ret = write_partial_kvec(con);
        if (ret &amp;lt;= 0)
            goto out;
    }
    ...
}

static int ceph_tcp_connect(struct ceph_connection *con)
{
    struct sockaddr_storage *paddr = &amp;amp;con-&amp;gt;peer_addr.in_addr;
    struct socket *sock;
    int ret;

    BUG_ON(con-&amp;gt;sock);
    ret = sock_create_kern(con-&amp;gt;peer_addr.in_addr.ss_family, SOCK_STREAM,
        IPPROTO_TCP, &amp;amp;sock);
    ...
    sock-&amp;gt;sk-&amp;gt;sk_allocation = GFP_NOFS;

    set_sock_callbacks(sock, con);

    con_sock_state_connecting(con);
    ret = sock-&amp;gt;ops-&amp;gt;connect(sock, (struct sockaddr *)paddr, sizeof(*paddr),
        O_NONBLOCK);
    ...
    con-&amp;gt;sock = sock;
    return 0;
}

static void set_sock_callbacks(struct socket *sock,
    struct ceph_connection *con)
{
    struct sock *sk = sock-&amp;gt;sk;
    sk-&amp;gt;sk_user_data = con;
    sk-&amp;gt;sk_data_ready = ceph_sock_data_ready; /*协议栈收到包后会回调该函数*/
    sk-&amp;gt;sk_write_space = ceph_sock_write_space;
    sk-&amp;gt;sk_state_change = ceph_sock_state_change; /*协议栈改变socket状态时会回调该函数*/
}

static void ceph_sock_data_ready(struct sock *sk, int count_unused)
{
    struct ceph_connection *con = sk-&amp;gt;sk_user_data;

    ...
    queue_con(con); /*再次唤醒工作任务*/
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;412-协商阶段&quot;&gt;&lt;strong&gt;4.1.2 协商阶段&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  socket连接阶段最后，向服务端发送了客户端的banner，并等待服务端回复banner。当收到服务端回复banner后，connection worker再次被唤醒，此时主要完成客户端与服务端的信息协商：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con); /*读取服务端返回的banner并进入协商阶段*/
        ...
        ret = try_write(con); /*向服务端发送协商信息*/
        ...
        break;
    }
    ...
}

static int try_read(struct ceph_connection *con)
{
    ...
    if (con-&amp;gt;state == CON_STATE_CONNECTING) {
        ret = read_partial_banner(con); /*读取服务端的banner信息*/
        if (ret &amp;lt;= 0)
            goto out;
        ret = process_banner(con); /*对banner进行校验*/
        if (ret &amp;lt; 0)
            goto out;

        con-&amp;gt;state = CON_STATE_NEGOTIATING; /*校验通过后进入协商阶段*/

        /*
         * Received banner is good, exchange connection info.
         * Do not reset out_kvec, as sending our banner raced
         * with receiving peer banner after connect completed.
         */
        ret = prepare_write_connect(con); /*准备全局连接号等协商信息*/
        if (ret &amp;lt; 0)
            goto out;
        prepare_read_connect(con); /*准备接收服务端返回的协商信息*/

        /* Send connection info before awaiting response */
        goto out;
    }
    ...
｝
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;413-正常打开阶段&quot;&gt;&lt;strong&gt;4.1.3 正常打开阶段&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  收到服务端的协商消息后，connection worker再次被唤醒，进行服务端协商消息的处理并进入正常打开阶段可收发消息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con); /*读取服务端协商信息并进入正常打开阶段，可正常接收消息*/
        ...
        ret = try_write(con); /*可正常发送消息*/
        ...
        break;
    }
    ...
}

static int try_read(struct ceph_connection *con)
{
    ...
    if (con-&amp;gt;state == CON_STATE_NEGOTIATING) {
        ret = read_partial_connect(con); /*读取服务端协商信息到in_reply中*/
        if (ret &amp;lt;= 0)
            goto out;
        ret = process_connect(con); /*处理协商消息并进入正常打开阶段*/
        if (ret &amp;lt; 0)
            goto out;
        goto more;
    }
    ...
}

static int process_connect(struct ceph_connection *con)
{
    ...
    switch (con-&amp;gt;in_reply.tag) { /*in_reply中记录服务端返回的协商信息*/
        ...
    case CEPH_MSGR_TAG_SEQ:
    case CEPH_MSGR_TAG_READY:
        ...

        /*记录服务端返回的协商消息并将连接状态置为OPEN*/
    
        con-&amp;gt;state = CON_STATE_OPEN;
        con-&amp;gt;auth_retry = 0;    /* we authenticated; clear flag */
        con-&amp;gt;peer_global_seq = le32_to_cpu(con-&amp;gt;in_reply.global_seq);
        con-&amp;gt;connect_seq++;
        con-&amp;gt;peer_features = server_feat;
        ...
        con-&amp;gt;delay = 0;      /* reset backoff memory */

        if (con-&amp;gt;in_reply.tag == CEPH_MSGR_TAG_SEQ) {
            prepare_write_seq(con);
            prepare_read_seq(con);
        } else {
            prepare_read_tag(con);
        }
        break;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;414-消息收发过程&quot;&gt;&lt;strong&gt;4.1.4 消息收发过程&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  客户端与服务端进行正常消息收发时，总是先在网络连接上发送一个字节的tag，再发送实际的消息。原因是两者可以通过tag来明确消息的具体类别和解析格式。网络流上的数据大体如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_message.jpg&quot; height=&quot;100&quot; width=&quot;700&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  我们先来看看消息的接收：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static int try_read(struct ceph_connection *con)
{
    ...

    if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_READY) { /*READY代表准备好接收具体的tag内容*/
        /*
         * what's next?
         */
        ret = ceph_tcp_recvmsg(con-&amp;gt;sock, &amp;amp;con-&amp;gt;in_tag, 1); /*从网络中接收一个字节的tag*/
        if (ret &amp;lt;= 0)
            goto out;

        /*根据tag内容准备接收后续的消息*/
        switch (con-&amp;gt;in_tag) {
        case CEPH_MSGR_TAG_MSG:
            prepare_read_message(con);
            break;
        case CEPH_MSGR_TAG_ACK:
            prepare_read_ack(con);
            break;
        case CEPH_MSGR_TAG_CLOSE:
            con_close_socket(con);
            con-&amp;gt;state = CON_STATE_CLOSED;
            goto out;
        default:
            goto bad_tag;
        }
    }

    /*如果tag为MSG，则表示后续为一个实际的message消息，开始接收消息内容并调用回调函数*/
    if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_MSG) {
        ret = read_partial_message(con); /*注意，这里通过回调alloc_msg找到ceph_osd_request中分配好的r_reply*/
        ...
        if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_READY)
            goto more;
        process_message(con);
        if (con-&amp;gt;state == CON_STATE_OPEN)
            prepare_read_tag(con);
        goto more;
    }

    /*如果tag为ACK或SEQ，则表示后续为一个服务端返回的确认号，可以释放已确认的发送消息*/
    if (con-&amp;gt;in_tag == CEPH_MSGR_TAG_ACK ||
        con-&amp;gt;in_tag == CEPH_MSGR_TAG_SEQ) {
        /*
         * the final handshake seq exchange is semantically
         * equivalent to an ACK
         */
        ret = read_partial_ack(con);
        ...
        process_ack(con);
        goto more;
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接下来再来看看消息的发送：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static int try_write(struct ceph_connection *con)
{
more:
    ...
more_kvec:    
    if (con-&amp;gt;out_kvec_left) { /*将发送缓冲区中的内容通过网络进行发送*/
        ret = write_partial_kvec(con);
        if (ret &amp;lt;= 0)
            goto out;
    }

    /* msg pages? */
    if (con-&amp;gt;out_msg) { /*如果已选定当前发送消息out_msg且还未完成发送，则发送消息包含的数据*/
        if (con-&amp;gt;out_msg_done) {
            ceph_msg_put(con-&amp;gt;out_msg);
            con-&amp;gt;out_msg = NULL;   /* we're done with this one */
            goto do_next;
        }

        ret = write_partial_message_data(con);
        if (ret == 1)
            goto more_kvec;  /* we need to send the footer, too! */
        if (ret == 0)
            goto out;
        if (ret &amp;lt; 0) {
            dout(&quot;try_write write_partial_message_data err %d\n&quot;, ret);
            goto out;
        }
    }

do_next:
    if (con-&amp;gt;state == CON_STATE_OPEN) {
        /* is anything else pending? */
        if (!list_empty(&amp;amp;con-&amp;gt;out_queue)) {
            /*如果发送队列out_queue中有待发送的消息，则取出一个消息放到out_msg中并发送其头部内容(消息中包含的数据在前面代码中发送)*/
            prepare_write_message(con);
            goto more;
        }
        if (con-&amp;gt;in_seq &amp;gt; con-&amp;gt;in_seq_acked) {
            /*如果当前收到消息的序列号大于已经确认的序列号，则准备发送新的确认号*/
            prepare_write_ack(con);
            goto more;
        }
        ...
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  最后我们来看看ceph_msg的结构定义，发送者负责将请求内容填入ceph_msg以供网络层发送；网络层接收消息后也先填入ceph_msg再交由上层应用处理响应：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/messenger.h:

struct ceph_msg {
    struct ceph_msg_header hdr;	/* header */
    struct ceph_msg_footer footer;	/* footer */
    struct kvec front;              /* unaligned blobs of message */
    struct ceph_buffer *middle;

    size_t				data_length;
    struct list_head		data;
    struct ceph_msg_data_cursor	cursor;

    struct ceph_connection *con;
    struct list_head list_head;	/* links for connection lists */

    struct kref kref;
    bool front_is_vmalloc;
    bool more_to_follow;
    bool needs_out_seq;
    int front_alloc_len;
    unsigned long ack_stamp;        /* tx: when we were acked */

    struct ceph_msgpool *pool;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/msgr.h:

struct ceph_msg_header {
    __le64 seq;       /* message seq# for this session */
    __le64 tid;       /* transaction id */
    __le16 type;      /* message type */
    __le16 priority;  /* priority.  higher value == higher priority */
    __le16 version;   /* version of message encoding */

    __le32 front_len; /* bytes in main payload */
    __le32 middle_len;/* bytes in middle payload */
    __le32 data_len;  /* bytes of data payload */
    __le16 data_off;  /* sender: include full offset;
                        receiver: mask against ~PAGE_MASK */

    struct ceph_entity_name src;
    __le32 reserved;
    __le32 crc;       /* header crc32c */
} __attribute__ ((packed));
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/ceph_fs.h:

/*
 * message types
 */

/* misc */
#define CEPH_MSG_SHUTDOWN               1
#define CEPH_MSG_PING                   2

/* client &amp;lt;-&amp;gt; monitor */
#define CEPH_MSG_MON_MAP                4
#define CEPH_MSG_MON_GET_MAP            5
#define CEPH_MSG_STATFS                 13
#define CEPH_MSG_STATFS_REPLY           14
#define CEPH_MSG_MON_SUBSCRIBE          15
#define CEPH_MSG_MON_SUBSCRIBE_ACK      16
#define CEPH_MSG_AUTH			17
#define CEPH_MSG_AUTH_REPLY		18
#define CEPH_MSG_MON_GET_VERSION        19
#define CEPH_MSG_MON_GET_VERSION_REPLY  20

/* client &amp;lt;-&amp;gt; mds */
#define CEPH_MSG_MDS_MAP                21

#define CEPH_MSG_CLIENT_SESSION         22
#define CEPH_MSG_CLIENT_RECONNECT       23

#define CEPH_MSG_CLIENT_REQUEST         24
#define CEPH_MSG_CLIENT_REQUEST_FORWARD 25
#define CEPH_MSG_CLIENT_REPLY           26
#define CEPH_MSG_CLIENT_CAPS            0x310
#define CEPH_MSG_CLIENT_LEASE           0x311
#define CEPH_MSG_CLIENT_SNAP            0x312
#define CEPH_MSG_CLIENT_CAPRELEASE      0x313

/* pool ops */
#define CEPH_MSG_POOLOP_REPLY           48
#define CEPH_MSG_POOLOP                 49


/* osd */
#define CEPH_MSG_OSD_MAP                41
#define CEPH_MSG_OSD_OP                 42
#define CEPH_MSG_OSD_OPREPLY            43
#define CEPH_MSG_WATCH_NOTIFY           44
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;42-tcp连接故障后的处理&quot;&gt;&lt;strong&gt;4.2. TCP连接故障后的处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  messenger模块在两种情况下可以获知TCP连接故障：一种是底层网络协议栈通知socket状态改变；另外一种是在通过socket进行网络收发包时，返回错误信息。&lt;/p&gt;

&lt;h5 id=&quot;421-网络协议栈通过回调上报故障&quot;&gt;&lt;strong&gt;4.2.1 网络协议栈通过回调上报故障&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  在第一种情况下，网络协议栈在发现TCP连接故障后，通过调用回调ceph_sock_state_change函数通知messenger底层socket处于关闭状态。ceph_sock_state_change函数在更新socket状态后会重新调度connection work：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void ceph_sock_state_change(struct sock *sk)
{
    struct ceph_connection *con = sk-&amp;gt;sk_user_data;

    switch (sk-&amp;gt;sk_state) {
    case TCP_CLOSE: /*发现底层socket已关闭*/
        dout(&quot;%s TCP_CLOSE\n&quot;, __func__);
    case TCP_CLOSE_WAIT:
        dout(&quot;%s TCP_CLOSE_WAIT\n&quot;, __func__);
        con_sock_state_closing(con);
        con_flag_set(con, CON_FLAG_SOCK_CLOSED); /*将连接状态置为关闭*/
        queue_con(con); /*重新唤醒工作任务*/
        break;
    case TCP_ESTABLISHED:
        ...
        break;
    default:	/* Everything else is uninteresting */
        break;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  工作任务被调度后，发现连接状态被关闭，进入故障处理流程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        if ((fault = con_sock_closed(con))) {
            /*工作任务被调度时首先判断连接是否处于关闭状态，如果已关则跳转出循环进行故障处理*/
            dout(&quot;%s: con %p SOCK_CLOSED\n&quot;, __func__, con);
            break;
        }
        ...
        ret = try_read(con); 
        ...
        ret = try_write(con); 
        ...
        break;
    }

    /*下面是故障处理逻辑*/
    if (fault)
        con_fault(con);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;422-通过网络收发函数返回结果获知连接故障&quot;&gt;&lt;strong&gt;4.2.2 通过网络收发函数返回结果获知连接故障&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  每个连接的工作任务在收发过程中，如果底层函数返回错误，也可获知网络连接故障：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

static void con_work(struct work_struct *work)
{
    struct ceph_connection *con = container_of(work, struct ceph_connection,
        work.work);

    ...
    while (true) {
        ...
        ret = try_read(con);
        if (ret &amp;lt; 0) { /*收包时底层返回错误*/
            if (ret == -EAGAIN)
                continue;
            con-&amp;gt;error_msg = &quot;socket error on read&quot;;
            fault = true;
            break;
        }
        
        ret = try_write(con); 
        if (ret &amp;lt; 0) { /*发包时底层返回错误*/
            if (ret == -EAGAIN)
                continue;
            con-&amp;gt;error_msg = &quot;socket error on write&quot;;
            fault = true;
        }

        break;
    }

    /*下面是故障处理逻辑*/
    if (fault)
        con_fault(con);
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;423-socket连接故障处理逻辑&quot;&gt;&lt;strong&gt;4.2.3 socket连接故障处理逻辑&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  连接故障的处理有两种方式：一种是back off，即延时重连；别一种是stand by，即等待有新消息时再重试：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
static void con_fault(struct ceph_connection *con)
{
    ...
    con_close_socket(con); /*关闭底层socket*/

    ...
    if (con-&amp;gt;in_msg) {
        /*如果有新接收到消息，则释放该消息*/
        BUG_ON(con-&amp;gt;in_msg-&amp;gt;con != con);
        con-&amp;gt;in_msg-&amp;gt;con = NULL;
        ceph_msg_put(con-&amp;gt;in_msg);
        con-&amp;gt;in_msg = NULL;
        con-&amp;gt;ops-&amp;gt;put(con);
    }

    /*对于已经发送但还未收到对方确认的消息，我们需要在网络重连后对它们进行重发*/
    /* Requeue anything that hasn't been acked */
    list_splice_init(&amp;amp;con-&amp;gt;out_sent, &amp;amp;con-&amp;gt;out_queue);

    /* If there are no messages queued or keepalive pending, place
     * the connection in a STANDBY state */
    if (list_empty(&amp;amp;con-&amp;gt;out_queue) &amp;amp;&amp;amp;
            !con_flag_test(con, CON_FLAG_KEEPALIVE_PENDING)) {

        /*如果发送队列为空且不需要发送心跳消息时，将当前连接置为stand by状态*/

        dout(&quot;fault %p setting STANDBY clearing WRITE_PENDING\n&quot;, con);
        con_flag_clear(con, CON_FLAG_WRITE_PENDING);
        con-&amp;gt;state = CON_STATE_STANDBY;
    } else {
        
        /*如果还有待发送的消息，那么偿试等待一段时间后重连；等待时间每次翻倍*/

        /* retry after a delay. */
        con-&amp;gt;state = CON_STATE_PREOPEN;
        if (con-&amp;gt;delay == 0)
            con-&amp;gt;delay = BASE_DELAY_INTERVAL;
        else if (con-&amp;gt;delay &amp;lt; MAX_DELAY_INTERVAL)
            con-&amp;gt;delay *= 2;
        con_flag_set(con, CON_FLAG_BACKOFF);
        queue_con(con);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  当连接处理stand by状态时，如果有新的消息通过ceph_con_send发送，其内部会清除stand by状态并重置为PREOPEN以偿试进行重连。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/RBD-client-4/&quot;&gt;【Rados Block Device】Client内核RBD驱动分析－网络信使messenger&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/RBD-client-4/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/RBD-client-4/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】Client内核RBD驱动分析－设备IO流程</title>
        <description>&lt;p&gt;  内核RBD驱动整体软件栈如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_4.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h3 id=&quot;3-rbd块设备io流程分析&quot;&gt;3. RBD块设备IO流程分析&lt;/h3&gt;

&lt;p&gt;  上节我们在分析映射流程时，已经涉及和OSD的交互，但并未深入讨论，因此这里我们将通过IO的处理流程来深入分析其内部原理。&lt;/p&gt;

&lt;p&gt;  IO流程可分为请求下发和响应返回两个阶段，整体过程如下图所示：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;应用程序下发的IO请求在rbd层被表达为一个rbd_img_request；每个rbd_img_request会被划分成一个个以4M为粒度的rbd_obj_request(其实我们在上一节分析时已经涉及rbd_obj_request)，这些rbd_obj_request对应OSD中存储的对象；通过CRUSH算法计算出存储rbd_obj_request的主OSD后，将rbd_obj_request封装成ceph_osd_request，接着将ceph_osd_request放入一个ceph_msg中并通过lmessenger模块进行发送；&lt;/li&gt;
    &lt;li&gt;OSD请求处理完成后回复响应，messenger模块通过ceph_msg接收响应消息，并找到对应的ceph_osd_request，接着就进行回调的处理；&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_io.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;31-io下发rbd_request_fn&quot;&gt;&lt;strong&gt;3.1. IO下发：rbd_request_fn&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  IO处理的核心函数是rbd_request_fn，其调用栈大体如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_request_fn[*]
    |-rbd_img_request_create
    |-rbd_img_request_fill
    |   |-rbd_obj_request_create
    |   |-rbd_osd_req_create
    |   |-osd_req_op_extent_osd_data_bio
    |   \-rbd_osd_req_format_write(or read)
    \-rbd_img_request_submit
        |-__register_request
        |-__map_request
        \-__send_queued
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*当应用向RBD块设备下发读写请求时，最终会调用该函数；
  q代表请求所在的请求队列*/
static void rbd_request_fn(struct request_queue *q)
__releases(q-&amp;gt;queue_lock) __acquires(q-&amp;gt;queue_lock)
{
    struct rbd_device *rbd_dev = q-&amp;gt;queuedata; /*队列私有数据，初始化时指定，代表rbd_dev对象*/
    struct request *rq;
    int result;

    /*通过一个大循环，不断调用blk_fetch_request从请求队列中取出待处理的请求rq*/
    while ((rq = blk_fetch_request(q))) {
        bool write_request = rq_data_dir(rq) == WRITE; /*判断请求读写类型*/
        struct rbd_img_request *img_request;
        u64 offset;
        u64 length;

        ...
        offset = (u64) blk_rq_pos(rq) &amp;lt;&amp;lt; SECTOR_SHIFT; /*计算请求起始位置，从扇区转换成字节*/
        length = (u64) blk_rq_bytes(rq); /*获取请求大小*/
        ...

        result = -ENOMEM;
        /*针对每个rq，这里会生成一个rbd_img_request与之对应；其内部会记录请求起始位置、长度和读写类型等信息*/
        img_request = rbd_img_request_create(rbd_dev, offset, length, write_request);
        if (!img_request)
            goto end_request;
        
        /*绑定img_request和rq之间的关系*/
        img_request-&amp;gt;rq = rq;

        /*向img_request中填入bio信息，下面将展开分析*/
        result = rbd_img_request_fill(img_request, OBJ_REQUEST_BIO, rq-&amp;gt;bio);
        /*将img_request递交给底层libceph过行网络发送，下面将展开分析*/
        if (!result)
            result = rbd_img_request_submit(img_request);
        ...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;311-rbd_img_request_fill&quot;&gt;&lt;strong&gt;3.1.1. rbd_img_request_fill&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_request_fn
    |-rbd_img_request_create
    |-rbd_img_request_fill[*]
    |   |-rbd_obj_request_create
    |   |-rbd_osd_req_create
    |   |-osd_req_op_extent_osd_data_bio
    |   \-rbd_osd_req_format_write(or read)
    \-rbd_img_request_submit
        |-__register_request
        |-__map_request
        \-__send_queued
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  由于每个RBD设备会以4M为粒度划分成一个个对象，所以对每个RBD设备的请求(image request)会转换成若干对象请求(object request)。rbd_img_request_fill的主要作用就是完成image request到object request的转换：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_img_request_fill(struct rbd_img_request *img_request,
                        enum obj_request_type type,void *data_desc)
{
    struct rbd_device *rbd_dev = img_request-&amp;gt;rbd_dev;
    struct rbd_obj_request *obj_request = NULL;
    struct rbd_obj_request *next_obj_request;
    bool write_request = img_request_write_test(img_request); /*是否为写请求*/
    struct bio *bio_list = 0;
    unsigned int bio_offset = 0;
    struct page **pages = 0;
    u64 img_offset;
    u64 resid;
    u16 opcode;

    
    opcode = write_request ? CEPH_OSD_OP_WRITE : CEPH_OSD_OP_READ;
    img_offset = img_request-&amp;gt;offset; /*image request对应的请求偏移*/
    resid = img_request-&amp;gt;length; /*image request对应的请求长度*/
    rbd_assert(resid &amp;gt; 0);

    if (type == OBJ_REQUEST_BIO) {/*这里主要考虑bio类型的请求*/
        bio_list = data_desc;
        rbd_assert(img_offset == bio_list-&amp;gt;bi_sector &amp;lt;&amp;lt; SECTOR_SHIFT);
    } else {
        ...
    }

    while (resid) {
        struct ceph_osd_request *osd_req;
        const char *object_name;
        u64 offset;
        u64 length;

        /*根据image request偏移位置计算对象名称*/
        object_name = rbd_segment_name(rbd_dev, img_offset);
        ...
        /*4M对象内的偏移*/
        offset = rbd_segment_offset(rbd_dev, img_offset);
        /*4M对象内的长度*/
        length = rbd_segment_length(rbd_dev, img_offset, resid);
        /*新建一个object request*/
        obj_request = rbd_obj_request_create(object_name, offset, length, type);
        /* object request has its own copy of the object name */
        rbd_segment_name_free(object_name);
        if (!obj_request)
            goto out_unwind;
        /*
         * set obj_request-&amp;gt;img_request before creating the
         * osd_request so that it gets the right snapc
         */
        rbd_img_obj_request_add(img_request, obj_request);

        if (type == OBJ_REQUEST_BIO) {
            unsigned int clone_size;

            rbd_assert(length &amp;lt;= (u64)UINT_MAX);
            clone_size = (unsigned int)length;
            /*克隆object request对应的bio段*/
            obj_request-&amp;gt;bio_list =
                bio_chain_clone_range(&amp;amp;bio_list,
                                    &amp;amp;bio_offset,
                                    clone_size,
                                    GFP_ATOMIC);
            ...
        } else {
            ...
        }

        /*针对object request创建底层的osd request*/
        osd_req = rbd_osd_req_create(rbd_dev, write_request, obj_request);
        ...
        obj_request-&amp;gt;osd_req = osd_req;
        obj_request-&amp;gt;callback = rbd_img_obj_callback; /*请求处理完成时的回调*/
        rbd_img_request_get(img_request);

        /*将object request的bio信息填入到osd request的extent中*/
        osd_req_op_extent_init(osd_req, 0, opcode, offset, length, 0, 0);
        if (type == OBJ_REQUEST_BIO)
            osd_req_op_extent_osd_data_bio(osd_req, 0, obj_request-&amp;gt;bio_list, length);
        else
            ...

        if (write_request) /*这里将ceph_osd_request中的信息填入到ceph_msg(r_request)中*/
            rbd_osd_req_format_write(obj_request);
        else
            rbd_osd_req_format_read(obj_request);

        obj_request-&amp;gt;img_offset = img_offset;

        /*更新偏移和长度，并继续循环*/
        img_offset += length; 
        resid -= length;
    }

    return 0;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里我们深入看看ceph_osd_request的结构定义，它体现了OSD对客户端呈现的操作接口：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/osd_client.h:

struct ceph_osd_request {
    u64             r_tid;              /* unique for this client */
    ...
    struct ceph_osd *r_osd; /*请求对应的OSD*/
    ...
    struct ceph_msg  *r_request, *r_reply; /*请求对应的发送消息和响应接收消息*/
    ...
    /* request osd ops array  */ /*每个请求由若干操作组成，如CEPH_OSD_OP_READ*/
    unsigned int		r_num_ops;
    struct ceph_osd_req_op	r_ops[CEPH_OSD_MAX_OP];

    ...
};

 /*OSD提供了许多不同的操作，均以CEPH_OSD_OP_打头。例如对于读请求，包含一个CEPH_OSD_OP_READ操作，
   操作对象extent中保存对象访问的偏移、长度和数据存储内存的位置信息；对于写请求，
   包含一个CEPH_OSD_OP_WRITE，操作对象也在extent中*/
struct ceph_osd_req_op {
    u16 op;           /* CEPH_OSD_OP_* */
    u32 payload_len;
    union {
        struct ceph_osd_data raw_data_in;
        struct {
            u64 offset, length;
            u64 truncate_size;
            u32 truncate_seq;
            struct ceph_osd_data osd_data;
        } extent; /*CEPH_OSD_OP_READ/WRITE之类的操作使用*/
        struct {
            const char *class_name;
            const char *method_name;
            struct ceph_osd_data request_info;
            struct ceph_osd_data request_data;
            struct ceph_osd_data response_data;
            __u8 class_len;
            __u8 method_len;
            __u8 argc;
        } cls; /*CEPH_OSD_OP_CALL操作使用*/
        struct {
            u64 cookie;
            u64 ver;
            u32 prot_ver;
            u32 timeout;
            __u8 flag;
        } watch;
    };
};

enum ceph_osd_data_type {
    CEPH_OSD_DATA_TYPE_NONE = 0,
    CEPH_OSD_DATA_TYPE_PAGES,
    CEPH_OSD_DATA_TYPE_PAGELIST,
    CEPH_OSD_DATA_TYPE_BIO,
};

/*数据保存内存位置可以通过page数组、pagelist和bio三种方式表达*/
struct ceph_osd_data {
    enum ceph_osd_data_type	type;
    union {
        struct {
            struct page	**pages;
            u64		length;
            u32		alignment;
            bool		pages_from_pool;
            bool		own_pages;
        };
        struct ceph_pagelist	*pagelist;
        struct {
            struct bio	*bio;		/* list of bios */
            size_t		bio_length;	/* total in list */
        };
    };
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;312-rbd_img_request_submit&quot;&gt;&lt;strong&gt;3.1.2. rbd_img_request_submit&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_request_fn
    |-rbd_img_request_create
    |-rbd_img_request_fill
    |   |-rbd_obj_request_create
    |   |-rbd_osd_req_create
    |   |-osd_req_op_extent_osd_data_bio
    |   \-rbd_osd_req_format_write(or read)
    \-rbd_img_request_submit[*]
        |-__register_request
        |-__map_request
        \-__send_queued
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  完成object request对象的成生后，rbd_image_request_submit调用rbd_obj_request_submit，最终来到libceph中的ceph_osdc_start_request，它的主要作用就是根据访问对象、OSDMap和CRUSH算法计算出对象所在PG对应的主OSD节点，并将该object request对应的osd request的r_request消息通过网络发送给该OSD处理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/osd_client.c:

int ceph_osdc_start_request(struct ceph_osd_client *osdc,
            struct ceph_osd_request *req, bool nofail)
{
    int rc = 0;

    down_read(&amp;amp;osdc-&amp;gt;map_sem);
    mutex_lock(&amp;amp;osdc-&amp;gt;request_mutex);
    /*将该osd请求添加到osdc中*/
    __register_request(osdc, req);
    req-&amp;gt;r_sent = 0;
    req-&amp;gt;r_got_reply = 0;
    /*通过CRUSH算法及OSDMap映射当前osd请求到OSD节点，有关CRUSH的原理可以参考相关论文*/
    rc = __map_request(osdc, req, 0);
    if (rc &amp;lt; 0) {
        ...
    }
    if (req-&amp;gt;r_osd == NULL) {
        dout(&quot;send_request %p no up osds in pg\n&quot;, req);
        ceph_monc_request_next_osdmap(&amp;amp;osdc-&amp;gt;client-&amp;gt;monc);
    } else {
        /*将该请求发送给OSD节点*/
        __send_queued(osdc);
    }
    rc = 0;
out_unlock:
    mutex_unlock(&amp;amp;osdc-&amp;gt;request_mutex);
    up_read(&amp;amp;osdc-&amp;gt;map_sem);
    return rc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;32-io返回&quot;&gt;&lt;strong&gt;3.2. IO返回&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  IO请求通过网络发送给OSD节点后，OSD节点在处理完成后会回复响应，响应的处理逻辑在客户端创建OSD对象时指定：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/osd_client.c:

static struct ceph_osd *create_osd(struct ceph_osd_client *osdc, int onum)
{
    struct ceph_osd *osd;

    osd = kzalloc(sizeof(*osd), GFP_NOFS);
    if (!osd)
        return NULL;

    atomic_set(&amp;amp;osd-&amp;gt;o_ref, 1);
    osd-&amp;gt;o_osdc = osdc;
    osd-&amp;gt;o_osd = onum;
    RB_CLEAR_NODE(&amp;amp;osd-&amp;gt;o_node);
    INIT_LIST_HEAD(&amp;amp;osd-&amp;gt;o_requests);
    INIT_LIST_HEAD(&amp;amp;osd-&amp;gt;o_linger_requests);
    INIT_LIST_HEAD(&amp;amp;osd-&amp;gt;o_osd_lru);
    osd-&amp;gt;o_incarnation = 1;

    ceph_con_init(&amp;amp;osd-&amp;gt;o_con, osd, &amp;amp;osd_con_ops, &amp;amp;osdc-&amp;gt;client-&amp;gt;msgr);

    INIT_LIST_HEAD(&amp;amp;osd-&amp;gt;o_keepalive_item);
    return osd;
}

static const struct ceph_connection_operations osd_con_ops = {
    .get = get_osd_con,
    .put = put_osd_con,
    .dispatch = dispatch, /*消息处理逻辑*/
    .get_authorizer = get_authorizer,
    .verify_authorizer_reply = verify_authorizer_reply,
    .invalidate_authorizer = invalidate_authorizer,
    .alloc_msg = alloc_msg,
    .fault = osd_reset,
};

static void dispatch(struct ceph_connection *con, struct ceph_msg *msg)
{
    struct ceph_osd *osd = con-&amp;gt;private;
    struct ceph_osd_client *osdc;
    int type = le16_to_cpu(msg-&amp;gt;hdr.type);

    if (!osd)
        goto out;
    osdc = osd-&amp;gt;o_osdc;

    switch (type) {
    case CEPH_MSG_OSD_MAP:
        ceph_osdc_handle_map(osdc, msg);
        break;
    case CEPH_MSG_OSD_OPREPLY:
        handle_reply(osdc, msg, con);
        break;
    case CEPH_MSG_WATCH_NOTIFY:
        handle_watch_notify(osdc, msg);
        break;

    default:
        pr_err(&quot;received unknown message type %d %s\n&quot;, type,
        ceph_msg_type_name(type));
    }
out:
    ceph_msg_put(msg);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  在handle_reply中会从osd request到object request，再到image request，层层调用回调并处理已经结束的IO请求。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/RBD-client-3/&quot;&gt;【Rados Block Device】Client内核RBD驱动分析－设备IO流程&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/RBD-client-3/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/RBD-client-3/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
      <item>
        <title>【Rados Block Device】Client内核RBD驱动分析－设备映射流程</title>
        <description>&lt;p&gt;  内核RBD驱动整体软件栈如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_4.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  rbd工具通过map子命令映射产生一个可使用的内核块设备，其本质原理是由rbd工具往sysfs(“/sys/bus/rbd/add”)内核文件接口中写入待创建块设备的信息(例如前文实例写入的内容为”9.22.115.154:6789 name=amdin,key=client, wbpool wb -“，其中9.22.115.154为主monitor的IP)，内核在处理这个写入请求时会调用由RBD驱动事先注册好的处理函数来生成一个内核块设备对象。&lt;/p&gt;

&lt;h3 id=&quot;2-rbd块设备映射流程分析&quot;&gt;2. RBD块设备映射流程分析&lt;/h3&gt;

&lt;p&gt;  我们先来看看rbd模块加载初始化时完成了哪些初始化动作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int __init rbd_init(void)
{
    int rc;

    /*首先检查rbd驱动依赖的底层libceph驱动的兼容性*/
    if (!libceph_compatible(NULL)) {
        rbd_warn(NULL, &quot;libceph incompatibility (quitting)&quot;);

        return -EINVAL;
    }

    /*初始化rbd驱动中使用的内存分配器*/
    rc = rbd_slab_init();
    if (rc)
        return rc;

    /*sysfs文件接口初始化，为用户态rbd工具暴露访问接口*/
    rc = rbd_sysfs_init();
    if (rc)
        rbd_slab_exit();
    else
    pr_info(&quot;loaded &quot; RBD_DRV_NAME_LONG &quot;\n&quot;);

    return rc;
}

module_init(rbd_init);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*
 * create control files in sysfs
 * /sys/bus/rbd/...
 */
static int rbd_sysfs_init(void)
{
    int ret;

    ret = device_register(&amp;amp;rbd_root_dev);
    if (ret &amp;lt; 0)
        return ret;

    /*根据rbd_bus_type定义的信息在/sys/bus/下生成子目录*/
    ret = bus_register(&amp;amp;rbd_bus_type);
    if (ret &amp;lt; 0)
        device_unregister(&amp;amp;rbd_root_dev);

    return ret;
}

static struct bus_attribute rbd_bus_attrs[] = {
    __ATTR(add, S_IWUSR, NULL, rbd_add), /*对rbd目录下的add文件进行写操作时将调用rbd_add*/
    __ATTR(remove, S_IWUSR, NULL, rbd_remove), /*对rbd目录下的remove文件进行写操作时将调用rbd_remove*/
    __ATTR_NULL
};

static struct bus_type rbd_bus_type = {
    .name		= &quot;rbd&quot;, /* /sys/bus/成生的子目录名 */
    .bus_attrs	= rbd_bus_attrs,
};

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  下面我们顺着本文开头给出的软件栈从上至下(rbd-&amp;gt;libceph)深入分析rbd_add函数，看看内核RBD驱动是如何进行块设备的生成的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*用户态工具往/sys/bus/rbd/add中写入RBD设备信息，内核在处理该写入请求时就会
  调用这里的rbd_add，其中bus就是/sys/bus/rbd对象，buf中存放用户态工具写入的
  字符串，如&quot;9.22.115.154:6789 name=amdin,key=client, wbpool wb -&quot;，
  count为写入的字符串长度*/

static ssize_t rbd_add(struct bus_type *bus,
                const char *buf, size_t count)
{
    struct rbd_device *rbd_dev = NULL;
    struct ceph_options *ceph_opts = NULL;
    struct rbd_options *rbd_opts = NULL;
    struct rbd_spec *spec = NULL;
    struct rbd_client *rbdc;
    struct ceph_osd_client *osdc;
    bool read_only;
    int rc = -ENOMEM;

    ...

    /*首先对用户态写入的字符串进行解析，结果返回到ceph_opts，rbd_opt和spec中：
      (1)ceph_opts中保存ceph集群相关信息，针对前文示例，ceph_opts.mon_addr中将保存
         &quot;9.22.115.154:6789&quot;；ceph_opts.name为&quot;admin&quot;，代表访问ceph集群的角色名称；
         ceph_opts.key对应系统配置中的&quot;ceph.client.admin.keyring&quot;，为登陆密钥。
      (2)rbd_opt中包含当前rbd设备是否只读，针对前文示例，rbd_opt.read_only为false。
      (3)spec中包含当前rbd设备的详细描述信息，针对示例，spec.pool_name为&quot;wb_pool&quot;，
         spec.image_name为&quot;wb&quot;，spec.snap_name为&quot;-&quot;，表示无快照*/

    /* parse add command */
    rc = rbd_add_parse_args(buf, &amp;amp;ceph_opts, &amp;amp;rbd_opts, &amp;amp;spec);
    if (rc &amp;lt; 0)
        goto err_out_module;
    read_only = rbd_opts-&amp;gt;read_only;
    kfree(rbd_opts);
    rbd_opts = NULL;	/* done with this */

    /*接着根据ceph集群配置ceph_opts在当前客户端查找是否已生成rbd_client(对应一个ceph_client，
      内部包含mon_client和osd_client)；如果未找到则生成一个新的rbd_client，并建立会话(完成认证、获
      取monmap和osdmap等一系统初始化动作)*/
    rbdc = rbd_get_client(ceph_opts);
    if (IS_ERR(rbdc)) {
        rc = PTR_ERR(rbdc);
        goto err_out_args;
    }

    /*从ceph集群返回的osdmap中获取当前pool对应的id*/
    /* pick the pool */
    osdc = &amp;amp;rbdc-&amp;gt;client-&amp;gt;osdc;
    rc = ceph_pg_poolid_by_name(osdc-&amp;gt;osdmap, spec-&amp;gt;pool_name);
    if (rc &amp;lt; 0)
        goto err_out_client;
    spec-&amp;gt;pool_id = (u64)rc;

    ...

    /*创建内核中的rbd_dev对象*/
    rbd_dev = rbd_dev_create(rbdc, spec);
    if (!rbd_dev)
        goto err_out_client;
    rbdc = NULL;		/* rbd_dev now owns this */
    spec = NULL;		/* rbd_dev now owns this */

    /*从ceph集群中查询当前rbd设备(image)的元数据信息，即从rbd_id.[rbd设备名]中获知image id，并从
      rbd_header.[image id]的omap中获得设备大小，分片大小等信息*/
    rc = rbd_dev_image_probe(rbd_dev, true);
    if (rc &amp;lt; 0)
        goto err_out_rbd_dev;

    ...

    /*根据ceph集群返回的rbd信息，完成对rbd_dev的设置，并通过内核块层接口生成新的块设备对象*/
    rc = rbd_dev_device_setup(rbd_dev);
    if (rc) {
        rbd_dev_image_release(rbd_dev);
        goto err_out_module;
    }

    return count;

    ...
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;21-rbd_get_client&quot;&gt;&lt;strong&gt;2.1. rbd_get_client&lt;/strong&gt;&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_get_client[*]
\-rbd_client_create
    |-ceph_create_client
    |   |-ceph_messenger_init
    |   |-ceph_monc_init
    |   \-ceph_osdc_init
    \_ceph_open_session
        |-ceph_monc_open_session
        |   \-__open_session
        |       |-ceph_con_open
        |       |-ceph_auth_build_hello
        |       \-__send_prepared_auth_request
        \-wait_event_interruptible_timeout
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  针对同一个rados集群，每个客户端节点都会在内核中生成一个rbd_client对象，用来和该rados集群进行会话；在一个客户端节点上，如果针对同一个rados集群创建了多个RBD设备，那么这些RBD设备会共用同一个rbd_client。下面我们看看rbd_get_client的内部实现：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*
 * Get a ceph client with specific addr and configuration, if one does
 * not exist create it.  Either way, ceph_opts is consumed by this
 * function.
 */
static struct rbd_client *rbd_get_client(struct ceph_options *ceph_opts)
{
    struct rbd_client *rbdc;

    rbdc = rbd_client_find(ceph_opts); /*在全局链表rbd_client_list中根据ceph_opts的集群信息查找rbd_client*/
    if (rbdc)	/* using an existing client */
        ceph_destroy_options(ceph_opts); /*如果找到相同的rbd_client(对应同一个rados集群)，则复用该对象，同时销毁ceph_opts*/
    else
        rbdc = rbd_client_create(ceph_opts); /*如果没有找到相同的rbd_client，则新建一个并添加到rbd_client_list中*/

    return rbdc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接着看rbd_client_create：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*rbd_client代表rbd客户端实例，多个rbd设备可共用一个客户端实例；其内部主要包含一个ceph_client对象*/
/*
 * an instance of the client.  multiple devices may share an rbd client.
 */
struct rbd_client {
    struct ceph_client	*client;
    struct kref		kref;
    struct list_head	node;
};

/*
 * Initialize an rbd client instance.  Success or not, this function
 * consumes ceph_opts.
 */
static struct rbd_client *rbd_client_create(struct ceph_options *ceph_opts)
{
    struct rbd_client *rbdc;
    int ret = -ENOMEM;

    rbdc = kmalloc(sizeof(struct rbd_client), GFP_KERNEL);
    ...

    /*调用libceph.ko中的函数创建ceph_client对象*/
    rbdc-&amp;gt;client = ceph_create_client(ceph_opts, rbdc, 0, 0);
    ...

    /*调用libceph.ko中的函数打开ceph_client对象的会话，基于该会话可以和rados集群(monitor or OSD)进行通信*/
    ret = ceph_open_session(rbdc-&amp;gt;client);
    ...

    spin_lock(&amp;amp;rbd_client_list_lock);
    list_add_tail(&amp;amp;rbdc-&amp;gt;node, &amp;amp;rbd_client_list);
    spin_unlock(&amp;amp;rbd_client_list_lock);

    ...

    return rbdc;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  下面我们再深入一层，看看libceph中ceph_client的相关实现，但不会深入到messenger模块中(复杂性较高)，我们在2.1.3节会总结一下对messenger模块的使用方法(API)。对于messenger的分析我们将放到单独的博文中介绍。&lt;/p&gt;

&lt;h5 id=&quot;211-rbd_get_client---ceph_create_client&quot;&gt;&lt;strong&gt;2.1.1. rbd_get_client -&amp;gt; ceph_create_client&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_get_client
\-rbd_client_create
    |-ceph_create_client[*]
    |   |-ceph_messenger_init
    |   |-ceph_monc_init
    |   \-ceph_osdc_init
    \_ceph_open_session
        |-ceph_monc_open_session
        |   \-__open_session
        |       |-ceph_con_open
        |       |-ceph_auth_build_hello
        |       \-__send_prepared_auth_request
        \-wait_event_interruptible_timeout
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/libceph.h:

/*
 * per client state
 *
 * possibly shared by multiple mount points, if they are
 * mounting the same ceph filesystem/cluster.
 */
struct ceph_client {
    ...

    /*每个ceph_client包含一个messenger实例、一个mon_client实例和一个osd_client实例：
      (1)messenger实例描述了与网络通信相关的信息，属于messenger模块，是ceph客户端基于socket封装的网络服务层；
      (2)mon_client实例是专门用来与monitor通信的客户端；
      (3)osd_client实例是专门用来与所有osd通信的客户端。*/
    struct ceph_messenger msgr;   /* messenger instance */
    struct ceph_mon_client monc;
    struct ceph_osd_client osdc;

    ...
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/ceph_common.c:

/*ceph_create_client创建ceph_client对象并对其内部的messenger、mon_client、osd_client进行初始化*/
/*
 * create a fresh client instance
 */
struct ceph_client *ceph_create_client(struct ceph_options *opt, void *private,
        unsigned int supported_features, unsigned int required_features)
{
    struct ceph_client *client;
    struct ceph_entity_addr *myaddr = NULL;
    int err = -ENOMEM;

    client = kzalloc(sizeof(*client), GFP_KERNEL);
    if (client == NULL)
        return ERR_PTR(-ENOMEM);

    client-&amp;gt;private = private;
    client-&amp;gt;options = opt;

    ...

    /* msgr */
    if (ceph_test_opt(client, MYIP))
        myaddr = &amp;amp;client-&amp;gt;options-&amp;gt;my_addr;
    ceph_messenger_init(&amp;amp;client-&amp;gt;msgr, myaddr, client-&amp;gt;supported_features,
        client-&amp;gt;required_features, ceph_test_opt(client, NOCRC));

    /* subsystems */
    err = ceph_monc_init(&amp;amp;client-&amp;gt;monc, client);
    if (err &amp;lt; 0)
        goto fail;
    err = ceph_osdc_init(&amp;amp;client-&amp;gt;osdc, client);
    if (err &amp;lt; 0)
        goto fail_monc;

    return client;
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  接着我们可以浏览一下messenger、mon_client、osd_client的初始化，重点关注注释部分：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/messenger.h:

struct ceph_messenger {
    struct ceph_entity_inst inst;    /* my name+address */
    struct ceph_entity_addr my_enc_addr;

    atomic_t stopping;
    bool nocrc;

    /*
     * the global_seq counts connections i (attempt to) initiate
     * in order to disambiguate certain connect race conditions.
     */
    u32 global_seq;
    spinlock_t global_seq_lock;

    u32 supported_features;
    u32 required_features;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/messenger.c:

/*
 * initialize a new messenger instance
 */
void ceph_messenger_init(struct ceph_messenger *msgr,
            struct ceph_entity_addr *myaddr,
            u32 supported_features,
            u32 required_features,
            bool nocrc)
{
    msgr-&amp;gt;supported_features = supported_features;
    msgr-&amp;gt;required_features = required_features;

    spin_lock_init(&amp;amp;msgr-&amp;gt;global_seq_lock);

    if (myaddr)
        msgr-&amp;gt;inst.addr = *myaddr;

    /* select a random nonce */
    msgr-&amp;gt;inst.addr.type = 0;
    get_random_bytes(&amp;amp;msgr-&amp;gt;inst.addr.nonce, sizeof(msgr-&amp;gt;inst.addr.nonce));
    encode_my_addr(msgr);
    msgr-&amp;gt;nocrc = nocrc;

    atomic_set(&amp;amp;msgr-&amp;gt;stopping, 0);

    dout(&quot;%s %p\n&quot;, __func__, msgr);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/mon_client.h:

struct ceph_mon_client {
    struct ceph_client *client;
    struct ceph_monmap *monmap;

    struct mutex mutex;
    struct delayed_work delayed_work;

    struct ceph_auth_client *auth;
    struct ceph_msg *m_auth, *m_auth_reply, *m_subscribe, *m_subscribe_ack;
    int pending_auth;

    bool hunting;
    int cur_mon;                       /* last monitor i contacted */
    unsigned long sub_sent, sub_renew_after;
    struct ceph_connection con;

    /* pending generic requests */
    struct rb_root generic_request_tree;
    int num_generic_requests;
    u64 last_tid;

    /* mds/osd map */
    int want_mdsmap;
    int want_next_osdmap; /* 1 = want, 2 = want+asked */
    u32 have_osdmap, have_mdsmap;
};

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/mon_client.c:

int ceph_monc_init(struct ceph_mon_client *monc, struct ceph_client *cl)
{
    int err = 0;

    dout(&quot;init\n&quot;);
    memset(monc, 0, sizeof(*monc));
    monc-&amp;gt;client = cl;
    monc-&amp;gt;monmap = NULL;
    mutex_init(&amp;amp;monc-&amp;gt;mutex);

    err = build_initial_monmap(monc);
    if (err)
        goto out;

    /* connection */
    /* authentication */
    monc-&amp;gt;auth = ceph_auth_init(cl-&amp;gt;options-&amp;gt;name, cl-&amp;gt;options-&amp;gt;key);
    ...
    monc-&amp;gt;auth-&amp;gt;want_keys =
        CEPH_ENTITY_TYPE_AUTH | CEPH_ENTITY_TYPE_MON |
        CEPH_ENTITY_TYPE_OSD | CEPH_ENTITY_TYPE_MDS;

    /* msgs */
    err = -ENOMEM;
    monc-&amp;gt;m_subscribe_ack = ceph_msg_new(CEPH_MSG_MON_SUBSCRIBE_ACK,
        sizeof(struct ceph_mon_subscribe_ack), GFP_NOFS, true);
    ...
    monc-&amp;gt;m_subscribe = ceph_msg_new(CEPH_MSG_MON_SUBSCRIBE, 96, GFP_NOFS, true);
    ...
    monc-&amp;gt;m_auth_reply = ceph_msg_new(CEPH_MSG_AUTH_REPLY, 4096, GFP_NOFS, true);
    ...
    monc-&amp;gt;m_auth = ceph_msg_new(CEPH_MSG_AUTH, 4096, GFP_NOFS, true);
    monc-&amp;gt;pending_auth = 0;
    ...

    /*mon_client通过底层messenger模块中的connection对象进行网络通信，这里对mon_client使用的connection进行
      初始化，并指定其网络层回调函数集为mon_con_ops，用来处理消息回调、网络连接故障等*/
    ceph_con_init(&amp;amp;monc-&amp;gt;con, monc, &amp;amp;mon_con_ops, &amp;amp;monc-&amp;gt;client-&amp;gt;msgr);

    monc-&amp;gt;cur_mon = -1;
    monc-&amp;gt;hunting = true;
    monc-&amp;gt;sub_renew_after = jiffies;
    monc-&amp;gt;sub_sent = 0;

    INIT_DELAYED_WORK(&amp;amp;monc-&amp;gt;delayed_work, delayed_work);
    monc-&amp;gt;generic_request_tree = RB_ROOT;
    monc-&amp;gt;num_generic_requests = 0;
    monc-&amp;gt;last_tid = 0;

    monc-&amp;gt;have_mdsmap = 0;
    monc-&amp;gt;have_osdmap = 0;
    monc-&amp;gt;want_next_osdmap = 1;
    return 0;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/ceph/osd_client.h:

struct ceph_osd_client {
    struct ceph_client     *client;

    struct ceph_osdmap     *osdmap;       /* current map */
    struct rw_semaphore    map_sem;
    struct completion      map_waiters;
    u64                    last_requested_map;

    struct mutex           request_mutex;
    struct rb_root         osds;          /* osds */
    struct list_head       osd_lru;       /* idle osds */
    u64                    timeout_tid;   /* tid of timeout triggering rq */
    u64                    last_tid;      /* tid of last request */
    struct rb_root         requests;      /* pending requests */
    struct list_head       req_lru;	      /* in-flight lru */
    struct list_head       req_unsent;    /* unsent/need-resend queue */
    struct list_head       req_notarget;  /* map to no osd */
    struct list_head       req_linger;    /* lingering requests */
    int                    num_requests;
    struct delayed_work    timeout_work;
    struct delayed_work    osds_timeout_work;

    mempool_t              *req_mempool;

    struct ceph_msgpool	msgpool_op;
    struct ceph_msgpool	msgpool_op_reply;

    spinlock_t		event_lock;
    struct rb_root		event_tree;
    u64			event_count;

    struct workqueue_struct	*notify_wq;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/osd_client.c:

int ceph_osdc_init(struct ceph_osd_client *osdc, struct ceph_client *client)
{
    int err;

    dout(&quot;init\n&quot;);
    osdc-&amp;gt;client = client;
    osdc-&amp;gt;osdmap = NULL;
    init_rwsem(&amp;amp;osdc-&amp;gt;map_sem);
    init_completion(&amp;amp;osdc-&amp;gt;map_waiters);
    osdc-&amp;gt;last_requested_map = 0;
    mutex_init(&amp;amp;osdc-&amp;gt;request_mutex);
    osdc-&amp;gt;last_tid = 0;
    osdc-&amp;gt;osds = RB_ROOT;
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;osd_lru);
    osdc-&amp;gt;requests = RB_ROOT;
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_lru);
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_unsent);
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_notarget);
    INIT_LIST_HEAD(&amp;amp;osdc-&amp;gt;req_linger);
    osdc-&amp;gt;num_requests = 0;
    INIT_DELAYED_WORK(&amp;amp;osdc-&amp;gt;timeout_work, handle_timeout);
    INIT_DELAYED_WORK(&amp;amp;osdc-&amp;gt;osds_timeout_work, handle_osds_timeout);
    spin_lock_init(&amp;amp;osdc-&amp;gt;event_lock);
    osdc-&amp;gt;event_tree = RB_ROOT;
    osdc-&amp;gt;event_count = 0;

    schedule_delayed_work(&amp;amp;osdc-&amp;gt;osds_timeout_work,
        round_jiffies_relative(osdc-&amp;gt;client-&amp;gt;options-&amp;gt;osd_idle_ttl * HZ));

    err = -ENOMEM;
    osdc-&amp;gt;req_mempool = mempool_create_kmalloc_pool(10, sizeof(struct ceph_osd_request));
    if (!osdc-&amp;gt;req_mempool)
        goto out;

    err = ceph_msgpool_init(&amp;amp;osdc-&amp;gt;msgpool_op, CEPH_MSG_OSD_OP,
            OSD_OP_FRONT_LEN, 10, true, &quot;osd_op&quot;);
    if (err &amp;lt; 0)
        goto out_mempool;
    err = ceph_msgpool_init(&amp;amp;osdc-&amp;gt;msgpool_op_reply, CEPH_MSG_OSD_OPREPLY,
        OSD_OPREPLY_FRONT_LEN, 10, true, &quot;osd_op_reply&quot;);
    if (err &amp;lt; 0)
        goto out_msgpool;

    err = -ENOMEM;
    osdc-&amp;gt;notify_wq = create_singlethread_workqueue(&quot;ceph-watch-notify&quot;);
    if (!osdc-&amp;gt;notify_wq)
        goto out_msgpool;
    return 0;

out_msgpool:
    ceph_msgpool_destroy(&amp;amp;osdc-&amp;gt;msgpool_op);
out_mempool:
    mempool_destroy(osdc-&amp;gt;req_mempool);
out:
    return err;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;212-rbd_get_client---ceph_open_session&quot;&gt;&lt;strong&gt;2.1.2. rbd_get_client -&amp;gt; ceph_open_session&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_get_client
\-rbd_client_create
    |-ceph_create_client
    |   |-ceph_messenger_init
    |   |-ceph_monc_init
    |   \-ceph_osdc_init
    \_ceph_open_session[*]
        |-ceph_monc_open_session
        |   \-__open_session
        |       |-ceph_con_open
        |       |-ceph_auth_build_hello
        |       \-__send_prepared_auth_request
        \-wait_event_interruptible_timeout
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  在完成ceph_client的初始化动作后，下一步是打开会话：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/ceph_common.c:

int ceph_open_session(struct ceph_client *client)
{
    ...

    ret = __ceph_open_session(client, started);

    ...
    return ret;
}

/*
 * mount: join the ceph cluster, and open root directory.
 */
int __ceph_open_session(struct ceph_client *client, unsigned long started)
{
    int err;
    /*timeout表示打开会话的超时时间，默认为60秒*/
    unsigned long timeout = client-&amp;gt;options-&amp;gt;mount_timeout * HZ;

    /*ceph_client内部是通过mon_client来打开会话，如果mon_client成功打开会话，
      它会成功获得rados集群的mon_map(描述所有monitors信息)和osd_map(描述所有的osd信息)*/
    /* open session, and wait for mon and osd maps */
    err = ceph_monc_open_session(&amp;amp;client-&amp;gt;monc);
    if (err &amp;lt; 0)
        return err;

    while (!have_mon_and_osd_map(client)) { /*如果没有获得mon_map和osd_map，则等待直到超时*/
        err = -EIO;
        if (timeout &amp;amp;&amp;amp; time_after_eq(jiffies, started + timeout)) /*超时退出*/
            return err;

        /* wait */
        dout(&quot;mount waiting for mon_map\n&quot;);
        /*下面，当前进程(rbd工具)将进入睡眠状态，直到内核接收到mon_map和osd_map时被重新唤醒，
          或者出现认证错时也将被唤醒*/
        err = wait_event_interruptible_timeout(client-&amp;gt;auth_wq,
            have_mon_and_osd_map(client) || (client-&amp;gt;auth_err &amp;lt; 0),
            timeout);
        if (err == -EINTR || err == -ERESTARTSYS)
            return err;
        if (client-&amp;gt;auth_err &amp;lt; 0)
            return client-&amp;gt;auth_err;
    }

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/mon_client.c:

int ceph_monc_open_session(struct ceph_mon_client *monc)
{
    mutex_lock(&amp;amp;monc-&amp;gt;mutex);
    __open_session(monc);
    __schedule_delayed(monc);
    mutex_unlock(&amp;amp;monc-&amp;gt;mutex);
    return 0;
}

/*
 * Open a session with a (new) monitor.
 */
static int __open_session(struct ceph_mon_client *monc)
{
    char r;
    int ret;

    if (monc-&amp;gt;cur_mon &amp;lt; 0) {/*初始时cur_mon为－1，表示没有和任何monitor建立连接*/

        /*通过随机数r，从初始化时生成的mon_map中随机选一个进行会话连接的monitor*/
        get_random_bytes(&amp;amp;r, 1);
        monc-&amp;gt;cur_mon = r % monc-&amp;gt;monmap-&amp;gt;num_mon;
        monc-&amp;gt;sub_sent = 0;
        monc-&amp;gt;sub_renew_after = jiffies;  /* i.e., expired */
        monc-&amp;gt;want_next_osdmap = !!monc-&amp;gt;want_next_osdmap;

        /*与选定的monitor建立网络连接，这里使用的messenger模块提供的接口*/
        ceph_con_open(&amp;amp;monc-&amp;gt;con,
            CEPH_ENTITY_TYPE_MON, monc-&amp;gt;cur_mon,
            &amp;amp;monc-&amp;gt;monmap-&amp;gt;mon_inst[monc-&amp;gt;cur_mon].addr);

        /*初始化发送给monitor的首个hello消息*/
        /* initiatiate authentication handshake */
        ret = ceph_auth_build_hello(monc-&amp;gt;auth,
                monc-&amp;gt;m_auth-&amp;gt;front.iov_base,
                monc-&amp;gt;m_auth-&amp;gt;front_alloc_len);

        /*这里通过messenger模块的ceph_con_send接口将消息发送给monitor*/
        __send_prepared_auth_request(monc, ret);
    } else {
        dout(&quot;open_session mon%d already open\n&quot;, monc-&amp;gt;cur_mon);
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;213-messenger模块使用方法小结&quot;&gt;&lt;strong&gt;2.1.3. messenger模块使用方法小结&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  通过前文对mon_client的分析，我们来总结一下mon_client是如何使用底层的messenger模块的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;(1) 通过ceph_messenger_init在ceph_client中初始化一个messenger对象，定义全局网络通信信息；该对象被mon_client和osd_client共享；&lt;/li&gt;
    &lt;li&gt;(2) ceph_con_init(&amp;amp;monc-&amp;gt;con, monc, &amp;amp;mon_con_ops, &amp;amp;monc-&amp;gt;client-&amp;gt;msgr)，网络连接对象初始化，并指明该连接收到消息时的回调处理函数(消息将在内核工作队列上下文被处理)；&lt;/li&gt;
    &lt;li&gt;(3) ceph_con_open打开与选定monitor的网络连接；并拉起一个与当前连接关联的工作任务(connection worker)，该任务负责该连接上网络消息的收发；&lt;/li&gt;
    &lt;li&gt;(4) 发送消息时，先使用ceph_msg_new进行消息的内存分配，再填入消息内容，然后使用ceph_con_send进行消息发送；ceph_con_send会唤醒工作任务执行消息发送；&lt;/li&gt;
    &lt;li&gt;(5) 接收消息时，内核网络协议栈在收到网卡消息后，会通过ceph注册的回调唤醒对应的connection worker进行收包，并调用初始化时绑定的消息处理回调函数；&lt;/li&gt;
    &lt;li&gt;(6) mon_client关闭会话时使用ceph_con_close关闭连接；&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;214-打开会话时客户端与monitor的交互消息处理回调mon_con_ops分析&quot;&gt;&lt;strong&gt;2.1.4. 打开会话时客户端与monitor的交互(消息处理回调mon_con_ops分析)&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  目前monitor对我们来说还是一个黑盒，因此无法分析其内部是如何处理客户端发送的hello消息的。那么我们如何才能获知客户端是如何与monitor进行交互的？这里我们可以打开内核动态日志开关(代码中有很多dout语句)，并通过分析日志来获知整个交互过程。通过打开libceph的messenger模块日志，我们可以得到如下图所示的消息交互过程：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/ceph/rbd_5.jpg&quot; height=&quot;550&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  从上图可见，TCP连接建立后的前几次消息交互主要是messenger模块对连接进行初始化，我们在将在深入分析messenger模块时分析。ceph connection初始化完成后，客户端首先便是向monitor发送Auth(hello)认证消息，关于认证的实现细节我们这里暂不深入分析，通过日志可知，整个认证会有三次Auth消息的交互：第一次Auth_reply返回前，monitor会向客户端返回monmap；第三次Auth_reply返回后，表示认证成功。认证成功之后客户端会向monitor发送Subscribe订阅消息，表示关注osdmap的更新。由于是首次认证，monitor会向client返回初始的osdmap，后续只有在osdmap有更新时，monitor才回返回更新后的map。&lt;/p&gt;

&lt;p&gt;  下面我们打开mon_con_ops看看客户端收到各类消息的处理过程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/net/ceph/mon_client.c:

static const struct ceph_connection_operations mon_con_ops = {
    ...
    .dispatch = dispatch, /*收到monitor的消息时会调用该函数*/
    ...
};

/*
 * handle incoming message
 */
static void dispatch(struct ceph_connection *con, struct ceph_msg *msg)
{
    struct ceph_mon_client *monc = con-&amp;gt;private;
    int type = le16_to_cpu(msg-&amp;gt;hdr.type);

    if (!monc)
        return;

    /*这里针对不同消息进行不同处理*/
    switch (type) {
    case CEPH_MSG_AUTH_REPLY:
        handle_auth_reply(monc, msg);
        break;

    case CEPH_MSG_MON_SUBSCRIBE_ACK:
        handle_subscribe_ack(monc, msg);
        break;

    case CEPH_MSG_STATFS_REPLY:
        handle_statfs_reply(monc, msg);
        break;

    case CEPH_MSG_POOLOP_REPLY:
        handle_poolop_reply(monc, msg);
        break;

    case CEPH_MSG_MON_MAP:
        ceph_monc_handle_map(monc, msg);
        break;

    case CEPH_MSG_OSD_MAP:
        ceph_osdc_handle_map(&amp;amp;monc-&amp;gt;client-&amp;gt;osdc, msg);
        break;

    default:
        /* can the chained handler handle it? */
        if (monc-&amp;gt;client-&amp;gt;extra_mon_dispatch &amp;amp;&amp;amp;
                monc-&amp;gt;client-&amp;gt;extra_mon_dispatch(monc-&amp;gt;client, msg) == 0)
            break;

        pr_err(&quot;received unknown message type %d %s\n&quot;, type,
        ceph_msg_type_name(type));
    }
    ceph_msg_put(msg);
}

/*处理monmap消息，接收最新的monitor信息*/
static void ceph_monc_handle_map(struct ceph_mon_client *monc, struct ceph_msg *msg)
{
    struct ceph_client *client = monc-&amp;gt;client;
    struct ceph_monmap *monmap = NULL, *old = monc-&amp;gt;monmap;
    void *p, *end;
    ...

    dout(&quot;handle_monmap\n&quot;);
    p = msg-&amp;gt;front.iov_base;
    end = p + msg-&amp;gt;front.iov_len;

    /*从收到的消息中解析出monmap内容，它包含所有monitor节点的IP信息*/
    monmap = ceph_monmap_decode(p, end);
    ...

    /*更新monmap*/
    client-&amp;gt;monc.monmap = monmap;
    kfree(old);
    ...

out_unlocked:
    /*唤醒所有在auth_wq中等待的任务*/
    wake_up_all(&amp;amp;client-&amp;gt;auth_wq);
}

/*处理osdmap消息，整体思路和monmap类似；这里分了增量和全量两种模式；
  osdmap中记录了所有osd的IP和状态信息*/
/*
* Process updated osd map.
*
* The message contains any number of incremental and full maps, normally
* indicating some sort of topology change in the cluster.  Kick requests
* off to different OSDs as needed.
*/
void ceph_osdc_handle_map(struct ceph_osd_client *osdc, struct ceph_msg *msg)
{
    ...
}

static void handle_auth_reply(struct ceph_mon_client *monc, struct ceph_msg *msg)
{
    int ret;
    int was_auth = 0;

    ...
    /*处理auth_reply消息*/
    ret = ceph_handle_auth_reply(monc-&amp;gt;auth, msg-&amp;gt;front.iov_base,
                msg-&amp;gt;front.iov_len,
                monc-&amp;gt;m_auth-&amp;gt;front.iov_base,
                monc-&amp;gt;m_auth-&amp;gt;front_alloc_len);
    if (ret &amp;lt; 0) {
        /*如果认证出错，则唤醒等待任务并返回错误信息*/
        monc-&amp;gt;client-&amp;gt;auth_err = ret;
        wake_up_all(&amp;amp;monc-&amp;gt;client-&amp;gt;auth_wq);
    } else if (ret &amp;gt; 0) {
        /*继续发送认证请求，从日志分析共会发送三次*/
        __send_prepared_auth_request(monc, ret);
    } else if (!was_auth &amp;amp;&amp;amp; ceph_auth_is_authenticated(monc-&amp;gt;auth)) {
        /*认证成功，发送针对osdmap的订阅消息*/
        dout(&quot;authenticated, starting session\n&quot;);

        monc-&amp;gt;client-&amp;gt;msgr.inst.name.type = CEPH_ENTITY_TYPE_CLIENT;
        monc-&amp;gt;client-&amp;gt;msgr.inst.name.num =
        cpu_to_le64(monc-&amp;gt;auth-&amp;gt;global_id);

        __send_subscribe(monc);
        __resend_generic_request(monc);
    }
    ...

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;22-rbd_dev_image_probe&quot;&gt;&lt;strong&gt;2.2. rbd_dev_image_probe&lt;/strong&gt;&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_dev_image_probe[*]
    |-rbd_dev_image_id
    |   \-rbd_obj_method_sync -&amp;gt; rbd.get_id
    |-rbd_dev_header_name
    \-rbd_dev_v2_header_info
        |-rbd_dev_v2_image_size
        |   \-rbd_obj_method_sync -&amp;gt; rbd.get_size
        \-rbd_dev_v2_header_onetime
            |-rbd_dev_v2_object_prefix
            |   \-rbd_obj_method_sync -&amp;gt; rbd.get_object_prefix
            \-rbd_dev_v2_features
                \-rbd_obj_method_sync -&amp;gt; rbd.get_features
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  通过前文的分析我们可知，当monitor返回monmap和osdmap后，rbd进程将继续往下执行到rbd_dev_image_probe，这里将从存放对象的OSD中获取与当前RBD设备相关的信息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*rbd_dev已分配内存空间，mapping为true*/
static int rbd_dev_image_probe(struct rbd_device *rbd_dev, bool mapping)
{
    int ret;
    int tmp;

    /*从rados OSD池中获取当前RBD设备的image id，即对象&quot;rbd_id.[RBD设备名称]&quot;的内容*/
    ret = rbd_dev_image_id(rbd_dev);
    ...

    /*构造当前RBD设备元数据对象的名称，即&quot;rbd_header.[image id]&quot;*/
    ret = rbd_dev_header_name(rbd_dev);
    ...

    if (mapping) {
        ret = rbd_dev_header_watch_sync(rbd_dev, true);
        ...
    }

    if (rbd_dev-&amp;gt;image_format == 1)
        ret = rbd_dev_v1_header_info(rbd_dev);
    else
        /*从rados OSD池中获取当前RBD设备元数据对象的omap信息*/
        ret = rbd_dev_v2_header_info(rbd_dev);
    ...

    return 0;

    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;221-rbd_dev_image_probe---rbd_dev_image_id&quot;&gt;&lt;strong&gt;2.2.1. rbd_dev_image_probe -&amp;gt; rbd_dev_image_id&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_dev_image_probe
    |-rbd_dev_image_id[*]
    |   \-rbd_obj_method_sync -&amp;gt; rbd.get_id
    |-rbd_dev_header_name
    \-rbd_dev_v2_header_info
        |-rbd_dev_v2_image_size
        |   \-rbd_obj_method_sync -&amp;gt; rbd.get_size
        \-rbd_dev_v2_header_onetime
            |-rbd_dev_v2_object_prefix
            |   \-rbd_obj_method_sync -&amp;gt; rbd.get_object_prefix
            \-rbd_dev_v2_features
                \-rbd_obj_method_sync -&amp;gt; rbd.get_features
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_dev_image_id(struct rbd_device *rbd_dev)
{
    int ret;
    size_t size;
    char *object_name;
    void *response;
    char *image_id;

    ...

    /*对象名称为RBD_ID_PREFIX(&quot;rbd_id.&quot;)+image_name*/
    size = sizeof (RBD_ID_PREFIX) + strlen(rbd_dev-&amp;gt;spec-&amp;gt;image_name);
    object_name = kmalloc(size, GFP_NOIO);
    ...

    /*返回的对象内容是一个经过编码的字符串，前四个字节代表后续内容的字节长度*/
    size = sizeof (__le32) + RBD_IMAGE_ID_LEN_MAX;
    response = kzalloc(size, GFP_NOIO);
    ...

    /*调用rbd同步对象调用(call)接口(以面向对象class.method的方式来访问存储，即对象接口)，这里本质就是获取&quot;rbd_id.RBD名称&quot;对象的内容：
      (1)rbd_device为rbd_dev，即当前RBD设备；
      (2)接口访问对象名称为object_name，即&quot;rbd_id.RBD名称&quot;；
      (3)接口访问类(class)名称为&quot;rbd&quot;；
      (4)接口访问方法(method)名称为&quot;get_id&quot;；
      (5)接口访问输入参数为空；
      (6)接口访问输入参数长度为0；
      (7)接口访问输出结果保存内存位置为response；
      (8)接口访问输出结果保存内存最大长度为64。*/
    ret = rbd_obj_method_sync(rbd_dev, object_name,
                &quot;rbd&quot;, &quot;get_id&quot;, NULL, 0,
                response, RBD_IMAGE_ID_LEN_MAX);

    if (ret == -ENOENT) {
        ...
    } else if (ret &amp;gt; sizeof (__le32)) {
        void *p = response;

        /*从返回结果中解析出对象内容，即image_id*/
        image_id = ceph_extract_encoded_string(&amp;amp;p, p + ret, NULL, GFP_NOIO);
        ret = IS_ERR(image_id) ? PTR_ERR(image_id) : 0;
        if (!ret)
            rbd_dev-&amp;gt;image_format = 2;
    } else {
        ret = -EINVAL;
    }

    if (!ret) {
        rbd_dev-&amp;gt;spec-&amp;gt;image_id = image_id;
        dout(&quot;image_id is %s\n&quot;, image_id);
    }
    ...
    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;222-rbd_dev_image_probe---rbd_dev_header_name&quot;&gt;&lt;strong&gt;2.2.2. rbd_dev_image_probe -&amp;gt; rbd_dev_header_name&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_dev_image_probe
    |-rbd_dev_image_id
    |   \-rbd_obj_method_sync -&amp;gt; rbd.get_id
    |-rbd_dev_header_name[*]
    \-rbd_dev_v2_header_info
        |-rbd_dev_v2_image_size
        |   \-rbd_obj_method_sync -&amp;gt; rbd.get_size
        \-rbd_dev_v2_header_onetime
            |-rbd_dev_v2_object_prefix
            |   \-rbd_obj_method_sync -&amp;gt; rbd.get_object_prefix
            \-rbd_dev_v2_features
                \-rbd_obj_method_sync -&amp;gt; rbd.get_features
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_dev_header_name(struct rbd_device *rbd_dev)
{
    struct rbd_spec *spec = rbd_dev-&amp;gt;spec;
    size_t size;

    ...
    /*header_name表示RBD设备对应头部对象(其omap保存了RBD设备元数据信息)名称
      即&quot;rbd_header.&quot;+image_id*/
    size = sizeof (RBD_HEADER_PREFIX) + strlen(spec-&amp;gt;image_id);

    rbd_dev-&amp;gt;header_name = kmalloc(size, GFP_KERNEL);
    ...
    sprintf(rbd_dev-&amp;gt;header_name, &quot;%s%s&quot;, RBD_HEADER_PREFIX, spec-&amp;gt;image_id);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;223-rbd_dev_image_probe---rbd_dev_v2_header_info&quot;&gt;&lt;strong&gt;2.2.3. rbd_dev_image_probe -&amp;gt; rbd_dev_v2_header_info&lt;/strong&gt;&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd_dev_image_probe
    |-rbd_dev_image_id
    |   \-rbd_obj_method_sync -&amp;gt; rbd.get_id
    |-rbd_dev_header_name
    \-rbd_dev_v2_header_info[*]
        |-rbd_dev_v2_image_size
        |   \-rbd_obj_method_sync -&amp;gt; rbd.get_size
        \-rbd_dev_v2_header_onetime
            |-rbd_dev_v2_object_prefix
            |   \-rbd_obj_method_sync -&amp;gt; rbd.get_object_prefix
            \-rbd_dev_v2_features
                \-rbd_obj_method_sync -&amp;gt; rbd.get_features
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*这里我们忽略与快照特性(snapshot)相关代码*/
static int rbd_dev_v2_header_info(struct rbd_device *rbd_dev)
{
    bool first_time = rbd_dev-&amp;gt;header.object_prefix == NULL;
    int ret;

    down_write(&amp;amp;rbd_dev-&amp;gt;header_rwsem);

    ret = rbd_dev_v2_image_size(rbd_dev);
    ...

    if (first_time) {
        ret = rbd_dev_v2_header_onetime(rbd_dev);
        ...
    }
    
    ...
    up_write(&amp;amp;rbd_dev-&amp;gt;header_rwsem);

    return ret;
}

static int rbd_dev_v2_image_size(struct rbd_device *rbd_dev)
{
    return _rbd_dev_v2_snap_size(rbd_dev, CEPH_NOSNAP,
            &amp;amp;rbd_dev-&amp;gt;header.obj_order,
            &amp;amp;rbd_dev-&amp;gt;header.image_size);
}

static int _rbd_dev_v2_snap_size(struct rbd_device *rbd_dev, u64 snap_id,
        u8 *order, u64 *snap_size)
{
    __le64 snapid = cpu_to_le64(snap_id);
    int ret;
    struct {
        u8 order;
        __le64 size;
    } __attribute__ ((packed)) size_buf = { 0 };

    /*调用rbd同步对象访问接口，这里本质就是获取“rbd_header.image_id&quot;对象中omap对应key的内容：
      (1)rbd_device为rbd_dev，即当前RBD设备；
      (2)接口访问对象名称为“rbd_header.image_id&quot;；
      (3)接口访问类(class)名称为&quot;rbd&quot;；
      (4)接口访问方法(method)名称为&quot;get_size&quot;，对应omap中的key为&quot;size&quot;；
      (5)接口访问输入参数为snapid；
      (6)接口访问输入参数长度为snapid的字节长度；
      (7)接口访问输出结果保存内存位置为size_buf；
      (8)接口访问输出结果保存内存最大长度为size_buf空间大小。*/
    ret = rbd_obj_method_sync(rbd_dev, rbd_dev-&amp;gt;header_name,
            &quot;rbd&quot;, &quot;get_size&quot;,
            &amp;amp;snapid, sizeof (snapid),
            &amp;amp;size_buf, sizeof (size_buf));
    dout(&quot;%s: rbd_obj_method_sync returned %d\n&quot;, __func__, ret);
    ...

    if (order) {
        *order = size_buf.order;
        dout(&quot;  order %u&quot;, (unsigned int)*order);
    }
    *snap_size = le64_to_cpu(size_buf.size);

    ...

    return 0;
}

static int rbd_dev_v2_header_onetime(struct rbd_device *rbd_dev)
{
    int ret;

    /*通过rbd_obj_method_sync获取头部对象omap中的&quot;object_prefix&quot;key的内容*/
    ret = rbd_dev_v2_object_prefix(rbd_dev);
    if (ret)
        goto out_err;

    /*通过rbd_obj_method_sync获取头部对象omap中的&quot;features&quot;key的内容*/
    ret = rbd_dev_v2_features(rbd_dev);
    if (ret)
        goto out_err;

    ...

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;224-rbd_obj_method_sync&quot;&gt;&lt;strong&gt;2.2.4. rbd_obj_method_sync&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;  通过前文分析，我们看到多个子函数中均使用了rbd_obj_method_sync函数来访问rados对象内容。该函数是对远程rados对象的同步调用接口，是对底层libceph接口(osd_client)的封装：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

/*
 * Synchronous osd object method call.  Returns the number of bytes
 * returned in the outbound buffer, or a negative error code.
 */
/*osd对象的同步方法调用，返回结果在inbound中。注：上文的注释有问题*/
static int rbd_obj_method_sync(struct rbd_device *rbd_dev,  /*当前RBD设备*/
                        const char *object_name,            /*访问对象名称*/
                        const char *class_name,             /*访问类名称*/
                        const char *method_name,            /*访问方法名称*/
                        const void *outbound,               /*输入参数内存位置*/
                        size_t outbound_size,               /*输入参数大小*/
                        void *inbound,                      /*返回结果内存位置*/
                        size_t inbound_size)                /*返回结果大小*/
{
    struct ceph_osd_client *osdc = &amp;amp;rbd_dev-&amp;gt;rbd_client-&amp;gt;client-&amp;gt;osdc; /*osd_client是底层libceph接口对象*/
    struct rbd_obj_request *obj_request;
    struct page **pages;
    u32 page_count;
    int ret;

    /*
     * Method calls are ultimately read operations.  The result
     * should placed into the inbound buffer provided.  They
     * also supply outbound data--parameters for the object
     * method.  Currently if this is present it will be a
     * snapshot id.
     */
    /*上面这段注释的意思是说针对对象的method call最终其实是对象的读操作。
      outbound表示method输入参数，当前只支持snapid；
      inbound表示返回结果。*/

    /*为返回结果inbound分配新的内存页， 为何要重新分配内存？*/
    page_count = (u32)calc_pages_for(0, inbound_size);
    pages = ceph_alloc_page_vector(page_count, GFP_KERNEL);
    if (IS_ERR(pages))
        return PTR_ERR(pages);

    ret = -ENOMEM;
    /*创建新的rbd_obj_request对象并初始化*/
    obj_request = rbd_obj_request_create(object_name, 0, inbound_size, OBJ_REQUEST_PAGES);
    if (!obj_request)
        goto out;

    obj_request-&amp;gt;pages = pages;
    obj_request-&amp;gt;page_count = page_count;

    /*创建rbd_obj_request在libceph对应的osd_request对象*/
    obj_request-&amp;gt;osd_req = rbd_osd_req_create(rbd_dev, false, obj_request);
    if (!obj_request-&amp;gt;osd_req)
        goto out;
    
    /*初始化osd_request中的op对象，它的类型为CEPH_OSD_OP_CALL，表示调用远端OSD中对象的方法*/
    osd_req_op_cls_init(obj_request-&amp;gt;osd_req, 0, CEPH_OSD_OP_CALL, class_name, method_name);
    if (outbound_size) {
        /*如果存在输入参数outbound，则为其分配新的内存页并复制其内容*/

        struct ceph_pagelist *pagelist;

        pagelist = kmalloc(sizeof (*pagelist), GFP_NOFS);
        if (!pagelist)
            goto out;

        ceph_pagelist_init(pagelist);
        ceph_pagelist_append(pagelist, outbound, outbound_size); /*将outbound中内容复制到pagelist*/
        osd_req_op_cls_request_data_pagelist(obj_request-&amp;gt;osd_req, 0,pagelist); /*将pagelist作为op的输入参数(request_data)*/
    }
    osd_req_op_cls_response_data_pages(obj_request-&amp;gt;osd_req, 0,
            obj_request-&amp;gt;pages, inbound_size, 0, false, false);/*将obj_request-&amp;gt;pages作为op的输出结果(response_data)*/
    rbd_osd_req_format_read(obj_request);

    /*将rbd_object_request对象提交给底层libceph发送给远端osd节点*/
    ret = rbd_obj_request_submit(osdc, obj_request);
    if (ret)
        goto out;

    /*等待远端osd节点返回响应*/
    ret = rbd_obj_request_wait(obj_request);
    if (ret)
        goto out;

    /*远端osd节点返回响应后，result中保存返回结果*/
    ret = obj_request-&amp;gt;result;
    if (ret &amp;lt; 0)
        goto out;

    rbd_assert(obj_request-&amp;gt;xferred &amp;lt; (u64)INT_MAX);
    ret = (int)obj_request-&amp;gt;xferred;
    /*根据远端osd返回的数据量xferred，将数据拷贝到inbound中*/
    ceph_copy_from_page_vector(pages, inbound, 0, obj_request-&amp;gt;xferred);
out:
    if (obj_request)
        rbd_obj_request_put(obj_request);
    else
        ceph_release_page_vector(pages, page_count);

return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;23-rbd_dev_device_setup&quot;&gt;&lt;strong&gt;2.3. rbd_dev_device_setup&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  基于之前获取的信息，我们可以通过内核块层提供的相关接口创建一个新的RBD块设备对象供应用使用：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/drivers/block/rbd.c:

static int rbd_dev_device_setup(struct rbd_device *rbd_dev)
{
    int ret;

    /*首先根据全局变量rbd_dev_id_max生成一个新的最大id号*/
    /* generate unique id: find highest unique id, add one */
    rbd_dev_id_get(rbd_dev);

    /*根据id号生成设备名称*/
    /* Fill in the device name, now that we have its id. */
    BUILD_BUG_ON(DEV_NAME_LEN &amp;lt; sizeof (RBD_DRV_NAME) + MAX_INT_FORMAT_WIDTH);
    sprintf(rbd_dev-&amp;gt;name, &quot;%s%d&quot;, RBD_DRV_NAME, rbd_dev-&amp;gt;dev_id);

    /* Get our block major device number. */

    /*通过块层接口register_blkdev注册一个新的块设备*/
    ret = register_blkdev(0, rbd_dev-&amp;gt;name);
    if (ret &amp;lt; 0)
        goto err_out_id;
    rbd_dev-&amp;gt;major = ret;

    /* Set up the blkdev mapping. */

    /*调用块层接口初始化gendisk对象，绑定其IO处理函数为rbd_request_fn(我们将以此为入口来分析IO流程)*/
    ret = rbd_init_disk(rbd_dev);
    if (ret)
        goto err_out_blkdev;

    /*快照相关，暂不关心*/
    ret = rbd_dev_mapping_set(rbd_dev);
    if (ret)
        goto err_out_disk;
    /*设备容量*/
    set_capacity(rbd_dev-&amp;gt;disk, rbd_dev-&amp;gt;mapping.size / SECTOR_SIZE);

    /*在sys目录下添加设备*/
    ret = rbd_bus_add_dev(rbd_dev);
    if (ret)
        goto err_out_mapping;

    /* Everything's ready.  Announce the disk to the world. */

    /*通过add_disk接口在系统中呈现一个新的块设备*/
    set_bit(RBD_DEV_FLAG_EXISTS, &amp;amp;rbd_dev-&amp;gt;flags);
    add_disk(rbd_dev-&amp;gt;disk);

    pr_info(&quot;%s: added with size 0x%llx\n&quot;, rbd_dev-&amp;gt;disk-&amp;gt;disk_name,
        (unsigned long long) rbd_dev-&amp;gt;mapping.size);

    return ret;

err_out_mapping:
    rbd_dev_mapping_clear(rbd_dev);
err_out_disk:
    rbd_free_disk(rbd_dev);
err_out_blkdev:
    unregister_blkdev(rbd_dev-&amp;gt;major, rbd_dev-&amp;gt;name);
err_out_id:
    rbd_dev_id_put(rbd_dev);
    rbd_dev_mapping_clear(rbd_dev);

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/RBD-client-2/&quot;&gt;【Rados Block Device】Client内核RBD驱动分析－设备映射流程&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/RBD-client-2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/RBD-client-2/</guid>
        
        <category>ceph</category>
        
        
      </item>
    
  </channel>
</rss>
