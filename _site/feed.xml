<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>吴斌</title>
    <description>欢迎来到我的技术博客~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 04 Jan 2018 11:16:35 +0800</pubDate>
    <lastBuildDate>Thu, 04 Jan 2018 11:16:35 +0800</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>进程管理之一：进程创建</title>
        <description>&lt;p&gt;  进程管理也是计算子系统(CPU&amp;amp;Memory)的核心功能，从本篇博文起，我们开始讨论进程管理。计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是进程什么是进程管理为什么需要进程管理&quot;&gt;什么是进程？什么是进程管理？为什么需要进程管理？&lt;/h3&gt;

&lt;p&gt;  从物理视角说上，进程是CPU上的一段逻辑过程，它的控制(代码段)和数据(数据段)存放于内存。回顾一下计算子系统开篇中描绘的系统结构图(如下图)，进程的执行要素包括CPU中的寄存器和内存段两个部分(虚拟内存段最终会映射到物理内存段)：寄存器代表进程的瞬时运行状态；代码段存储指令，控制进程执行逻辑；数据段存储进程的全局数据；堆栈段存储局部数据和动态数据。从功能视角说，进程是各种“功能”的实现实体，计算机为人们提供的诸如聊天、上网、看视频等各种功能都是通过进程实现的，因此进程有时也被叫作“任务“。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/cpu_low_level.jpg&quot; height=&quot;550&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  进程管理是指与进程相关的一系列动作，如创建、替换、终止、调度、通信等等。进程管理使得一个CPU可以执行若干进程，各进程分时复用CPU的物理资源；内存管理使得多个进程可以共享物理内存；基于上述两个核心功能，计算机系统可以实现多任务并行，大大提升系统运行效率，方使客户使用(想象一下，如果你的计算机一个时刻只能运行一个任务，那将是一种多么糟糕体验)。&lt;/p&gt;

&lt;h3 id=&quot;如何创建进程&quot;&gt;如何创建进程？&lt;/h3&gt;

&lt;p&gt;  进程创建就是新建一个进程，这是进程管理最基本的功能，也是进程生命周期的起点。下面我们就来看看进程创建在Linux内核中是如何实现的。&lt;/p&gt;

&lt;h4 id=&quot;forkvfork和clone&quot;&gt;&lt;strong&gt;fork、vfork和clone&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  从应用程序开发的层次上，我们应该知道创建进程(或线程，即轻量级进程)有fork、vfork和clone三种&lt;a href=&quot;https://rootw.github.io/2017/02/系统调用/&quot;&gt;系统调用&lt;/a&gt;：fork是创建进程标准做法，父子进程共享代码段，但拥有独立数据、堆栈段；vfork是轻量级进程创建方法，父子进程共享代码、数据和堆栈段，子进程运行期间父进程是睡眠的，当子进程结束后父进程才继续运行；clone则提供了更灵活的进程创建方式，可以通过clone_flags来控制创建过程，libpthread库提供的相关API即是通过clone系统调用实现的。大家可以在网上找一些这三种方式的示例代码，动手实验一下以加深理解。到了内核态，这三个系统调用最终都通过do_fork函数来实现其核心功能：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/fork.c:

SYSCALL_DEFINE0(fork)
{
    return do_fork(SIGCHLD, 0, 0, NULL, NULL);
}

SYSCALL_DEFINE0(vfork)
{
    return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, 
        0, NULL, NULL);
}

SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
                int __user *, parent_tidptr,
                int __user *, child_tidptr,
                int, tls_val)
{
    return do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;进程控制块struct-task_struct&quot;&gt;&lt;strong&gt;进程控制块：struct task_struct&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在深入分析do_fork之前，我们首先要明白内核对进程需要有一个抽象的数据表达，基于这种数据表达才能实现各种管理功能。我们将内核中表达进程的数据结构叫做进程控制块，在linux中则是struct task_struct。这里我不打算对task_struct中的各个字段进行逐一描述，因为难以表述清楚，大家可以结合后续的代码流程来深入理解各字段的含义，下面是一幅整体结构图，供参考：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process1_1.jpg&quot; height=&quot;500&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  另外，在早期的linux版本中，进程控制块是包含进程的内核态栈的(通常是8KB大小)。什么是内核态栈？每个进程都有用户态空间和内核态空间两个执行空间，出于安全隔离的考虑，两个空间使用独立的栈，因此内核栈就被安排在了进程控制块中，栈底在高地址端，从高地址往低地址扩展，而进程控制块其它数据则被放置在8K的低地址起始位置处。随着内核的发展，各种功能不断被加入，进程控制块的数据结构也在不断变大，因此就存在挤占内核栈的风险。所以高版本内核将进程控制块和内核栈进行了分离：内核栈的低地址端只保留基本的进程信息，并通过指针对向真正的进程控制块结构，如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/process1_2.jpg&quot; height=&quot;300&quot; width=&quot;450&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;深入do_fork&quot;&gt;&lt;strong&gt;深入do_fork&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  do_fork在传入参数clone_flags的控制下，基于当前进程复制了一个新进程，其大体流程是：先复制当前进程的进程控制块，然后再调度新进程进入运行态。代码框架如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/kernel/fork.c:

/*
 *  Ok, this is the main fork-routine.
 *
 * It copies the process, and if successful kick-starts
 * it and waits for it to finish using the VM if required.
 */
long do_fork(unsigned long clone_flags,
        unsigned long stack_start,
        unsigned long stack_size,
        int __user *parent_tidptr,
    int __user *child_tidptr)
{
    struct task_struct *p;
    long nr;

    ...

    /*基于当前进程的task_struct和clone_flags复制新进程*/
    p = copy_process(clone_flags, stack_start, stack_size,
                                child_tidptr, NULL, trace);
    /*
     * Do this prior waking up the new thread - the thread pointer
     * might get invalid after that point, if the thread exits quickly.
     */
    if (!IS_ERR(p)) {
        struct completion vfork;
        struct pid *pid;

        pid = get_task_pid(p, PIDTYPE_PID);
        nr = pid_vnr(pid);

        if (clone_flags &amp;amp; CLONE_PARENT_SETTID)
            put_user(nr, parent_tidptr);

        /*如果clone_flags中置了CLONE_VFORK标置，则需要初始化等待结构体*/
        if (clone_flags &amp;amp; CLONE_VFORK) {
            p-&amp;gt;vfork_done = &amp;amp;vfork;
            init_completion(&amp;amp;vfork);
            get_task_struct(p);
        }

        /*将新创建的进程加入调度队列*/
        wake_up_new_task(p);

        ...

        /*对于VFORK，当前进程(即父进程)需要等待子进程完成后才能继续运行*/
        if (clone_flags &amp;amp; CLONE_VFORK) {
            if (!wait_for_vfork_done(p, &amp;amp;vfork))
                ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
            }

        put_pid(pid);
    } else {
        nr = PTR_ERR(p);
    }
    return nr;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  补充一下关于clone_flags标记的注释说明，建议大家在使用到的代码位置处仔细阅读，以加深理解：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/uapi/linux/sched.h:

/*
 * cloning flags:
 */
#define CSIGNAL		0x000000ff	/* signal mask to be sent at exit */
#define CLONE_VM	0x00000100	/* set if VM shared between processes */
#define CLONE_FS	0x00000200	/* set if fs info shared between processes */
#define CLONE_FILES	0x00000400	/* set if open files shared between processes */
#define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
#define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
#define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
#define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
#define CLONE_THREAD	0x00010000	/* Same thread group? */
#define CLONE_NEWNS	0x00020000	/* New namespace group? */
#define CLONE_SYSVSEM	0x00040000	/* share system V SEM_UNDO semantics */
#define CLONE_SETTLS	0x00080000	/* create a new TLS for the child */
#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */
#define CLONE_CHILD_CLEARTID	0x00200000	/* clear the TID in the child */
#define CLONE_DETACHED		0x00400000	/* Unused, ignored */
#define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
#define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
/* 0x02000000 was previously the unused CLONE_STOPPED (Start in stopped state)
and is now available for re-use. */
#define CLONE_NEWUTS		0x04000000	/* New utsname group? */
#define CLONE_NEWIPC		0x08000000	/* New ipcs */
#define CLONE_NEWUSER		0x10000000	/* New user namespace */
#define CLONE_NEWPID		0x20000000	/* New pid namespace */
#define CLONE_NEWNET		0x40000000	/* New network namespace */
#define CLONE_IO		0x80000000	/* Clone io context */

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;零号进程与一号进程&quot;&gt;&lt;strong&gt;零号进程与一号进程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2018/01/进程创建/&quot;&gt;进程管理之一：进程创建&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jan 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>内存管理之六：初始化</title>
        <description>&lt;p&gt;  初始化过程往往是比较冗长且乏味的，如果一接触就开始学习这块内容会让人烦闷。在了解内存管理几个核心功能模块后，我们再回头看看内存管理的初始化过程，相信大家会有新的收获。计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;低级阶段-汇编实现&quot;&gt;低级阶段-汇编实现&lt;/h3&gt;

&lt;p&gt;  我们可以把整个内存初始化过程大体分成低级阶段和高级阶段两个过程。低级阶段主要是用低级汇编语言实现的，又可细分成三步：&lt;/p&gt;

&lt;p&gt;  首先是BIOS，它在计算机启动的最初阶段检测了物理内存的分布情况，并在x86实模式的高端地址处(1M内存以下)记录了这些内存分布信息(称为e820表)。e820表是一个数组，每一项记录了一段连续内存信息，包含起始地址、结束地址和内存类型。内存类型分为普通内存(RAM)、保留内存(RESERVED，如BIOS内存)、ACPI表空间等。此外，对于NUMA结构系统，BIOS还将产生ACPI的SRAT表，用来记录每个numa节点的内存分布。&lt;/p&gt;

&lt;p&gt;  接着BIOS通过引导GRUB，再由GRUB将内核实模式部分和保护模式部分加载进内存。之后GRUB跳转到内核实模式部分执行，此时内核通过int指令获取e820表信息，由此得知物理内存分布。&lt;/p&gt;

&lt;p&gt;  在低级阶段的最后，内核进入保护模式，设置了高端虚拟地址到物理地址的线性映射，并将栈空间设定在了0号进程(BSP启动核对应的IDLE进程)的栈空间，随后就进入了高级阶段。&lt;/p&gt;

&lt;h3 id=&quot;高级阶段-c语言实现&quot;&gt;高级阶段-C语言实现&lt;/h3&gt;

&lt;p&gt;  高级阶段的入口函数是start_kernel，与内存管理相关的部分主要setup_arch中。&lt;/p&gt;

&lt;p&gt;  首先，我们会看到一些以memblock_打头的函数，这是干嘛的？我们应该知道，完整的内存初始化过程完成之前，正常的内存申请和释放功能是没法使用的。但是内核初始化过程中也需要动态分配内存，这就产生了矛盾。内核的做法是在初始化阶段使用一个简便的内存管理方法，这就是memblock。它从e820表中获知内存的分布情况，并以简单的连续分配方法来管理内存。所以在内核初始化阶段，它使用memblock来进行内存的申请和释放。&lt;/p&gt;

&lt;p&gt;  接着在init_mem_mapping中，内核将direct mapping区线性映射到整个物理空间。如此一来，内核便可访问所有物理内存了。大家可以回顾下Linux 3.10/Documentation/x86/x86_64/mm.txt。&lt;/p&gt;

&lt;p&gt;  再接着在initmem_init中，通过读取ACPI的SRAT表获知NUMA信息，并将这些信息更新到memblock中，此时内核就得知了完整的内存分布信息：有多少段内存，每段内存分别属于哪个NUMA节点。这里内核会为每个节点创建struct pglist_data结构，用来记录内存分布信息。&lt;/p&gt;

&lt;p&gt;  随后进入了和分页相关的初始化过程paging_init。这里又涉及内核的sparse_memory特性，这又是什么鬼？内核在管理内存时，是需要分配独立的内存页来记录内存信息的，比如struct page数组。早期的内核是按最大物理内存量固定分配，对于小内存场景，这种方法问题不大。然而当前系统内存越来越大(x86_64最大支持2^46)，而且内存可能动态增加(热插内存条)时，固定分配的方法就不适用了。sparse memory则以更灵活的方式来分配管理内存，如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory6_1.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  内核将所有内存(最大2^46)划分成区(128M)，并通过一个数组mem_section来记录每个区的信息。对于数组mem_section，是按4K页为粒度来分配空间，本质可以视为一个两维数组。内核通过memblock记录的信息为可用内存动态分配管理空间，不可用的区间将置为空，或者将section_mem_map低位清零(代表对应的区不存在)。在此过程中，内核也会对struct page数组分配空间，并将地址记录到section_mem_map中。&lt;/p&gt;

&lt;p&gt;  完成sparse memory的初始化后，内核通过free_area_init_nodes来初始化内核NUMA节点的空闲内存信息。此时页管理系统没有任何空闲内存。那么空闲内存是怎么来的？这就回到start_kernel中，它先通过build_all_zonelists建立分配zone序列，再通过mem_init调用free_all_bootmem，这里会把memblock中的空闲内存释放到页管理系统中。此后，内核就可以使用正常的alloc_pages函数来分配页了。&lt;/p&gt;

&lt;p&gt;  在2017年的最后几天里，终于把内核中有关内存管理方面的基础内容分析完了，但整个内存管理涉及的知识面非常广，还包括：反向映射、大页内存管理、KSM、cgroup_memory等多个方面。要想真正精通内存管理，需要坚持长期学习，并不断总结与实践。2018，继续努力！&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/12/内存初始化/&quot;&gt;内存管理之六：内存初始化&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 29 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%88%9D%E5%A7%8B%E5%8C%96/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%88%9D%E5%A7%8B%E5%8C%96/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>内存管理之五：内存压缩</title>
        <description>&lt;p&gt;  本节将讨论内存压缩，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是内存压缩为什么需要它&quot;&gt;什么是内存压缩？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  内存压缩(memory compaction)是指把应用程序正在使用的物理页内容迁移(migrate)到其它物理页中，以释放原有物理页，从而方便空闲页的合并以形成大段连续空闲内存，有效避免内存碎片的产生。&lt;/p&gt;

&lt;p&gt;  前文介绍优化型伙伴算法时，我们已经看到空闲内存被分成不可移动、可回收、可移动几个类别，应用程序从可移动类别中获取内存页。连续的内存分配和释放如果产生碎片无法满足当前内存分配请求时，内核进入慢速分配流程触发内存压缩，进行碎片整理，从而最大限度满足分配需求。&lt;/p&gt;

&lt;h3 id=&quot;如何实现&quot;&gt;如何实现？&lt;/h3&gt;

&lt;p&gt;  从整体流程上看，内存分配函数alloc_pages在快速流程get_page_from_freelist无法满足时，会进入慢速流程__alloc_pages_slowpath。慢速流程中进行一系列尝试后如果仍然无法满足分配需求，则进入compact_zone内存压缩流程。&lt;/p&gt;

&lt;p&gt;  内存压缩是通过迁移实现，迁移就涉及源端和目的端，那么内存压缩过程所要考虑的第一个问题就是源端页从哪来？目的端页又从哪来？Linux内核采用的策略是把低地址的页移动到高地址，从而在低地址端形成大段连续内存。因此内核从低往高扫描内存区(zone)，收集需要迁移的源端内存页；另一方面从高往低扫描，获取迁移目的端的空闲页。迁移源端页的收集在isolate_migratepage函数中实现，目的端页的收集在isolate_freepages函数中实现，有兴趣的读者可以深入进一步分析。&lt;/p&gt;

&lt;p&gt;  迁移源端和目的端确认后，下面要考虑的就是迁移动作该如何完成。粗略来说，需要将源端页内容拷贝到目的页、将文件映射关系中的源端页替换成目的端页、解除源端页在页表中的映射并重新映射目的页。&lt;/p&gt;

&lt;p&gt;  下图展示了使用中的物理页的映射关系：进程的虚拟地址段(vma)通过mmap被映射到文件段；文件内容读取到物理内存后以缓存页方式被组织成文件映射树；页表将虚拟地址映射到文件缓存页，供MMU使用实现地址转换。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory5_1.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  基于上图，我们看看迁移过程：总体分两步，首先解映射源端页；其次是将映射树的源端页替换成目的页，包括内存页内容的拷贝。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory5_2.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory5_3.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  有的读者可能会问，怎么最后没有重新将虚拟地址映射到目的页呢？内核采用的是lazy的方式，当进程访问缺页时会完成重新映射。
详细代码在migrate_pages中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/migrate.c:

/*
 * migrate_pages - migrate the pages specified in a list, to the free pages
 *		   supplied as the target for the page migration
 *
 * @from:		The list of pages to be migrated.
 * @get_new_page:	The function used to allocate free pages to be used
 *			as the target of the page migration.
 * @private:		Private data to be passed on to get_new_page()
 * @mode:		The migration mode that specifies the constraints for
 *			page migration, if any.
 * @reason:		The reason for page migration.
 *
 * The function returns after 10 attempts or if no pages are movable any more
 * because the list has become empty or no retryable pages exist any more.
 * The caller should call putback_lru_pages() to return pages to the LRU
 * or free list only if ret != 0.
 *
 * Returns the number of pages that were not migrated, or an error code.
 */
int migrate_pages(struct list_head *from, new_page_t get_new_page,
                unsigned long private, enum migrate_mode mode, int reason)
{
    int retry = 1;
    int nr_failed = 0;
    int nr_succeeded = 0;
    int pass = 0;
    struct page *page;
    struct page *page2;
    int swapwrite = current-&amp;gt;flags &amp;amp; PF_SWAPWRITE;
    int rc;

    if (!swapwrite)
        current-&amp;gt;flags |= PF_SWAPWRITE;

    for(pass = 0; pass &amp;lt; 10 &amp;amp;&amp;amp; retry; pass++) {
        retry = 0;

        list_for_each_entry_safe(page, page2, from, lru) {
            cond_resched();

            /*解映射页表并移动内存页，内部将调用__unmap_and_move*/
            rc = unmap_and_move(get_new_page, private, page, pass &amp;gt; 2, mode);

            switch(rc) {
            case -ENOMEM:
                goto out;
            case -EAGAIN:
                retry++;
                break;
            case MIGRATEPAGE_SUCCESS:
                nr_succeeded++;
                break;
            default:
                /* Permanent failure */
                nr_failed++;
                break;
            }
        }
    }
    rc = nr_failed + retry;
out:
    if (nr_succeeded)
        count_vm_events(PGMIGRATE_SUCCESS, nr_succeeded);
    if (nr_failed)
        count_vm_events(PGMIGRATE_FAIL, nr_failed);
    trace_mm_migrate_pages(nr_succeeded, nr_failed, mode, reason);

    if (!swapwrite)
        current-&amp;gt;flags &amp;amp;= ~PF_SWAPWRITE;

    return rc;
}

static int __unmap_and_move(struct page *page, struct page *newpage,
                                int force, enum migrate_mode mode)
{
    int rc = -EAGAIN;
    int remap_swapcache = 1;
    struct mem_cgroup *mem;
    struct anon_vma *anon_vma = NULL;

    /*锁定迁移源端页*/
    if (!trylock_page(page)) {
        ...
    }

    if (PageWriteback(page)) {
        /*跳过或等待writeback页回刷完成*/
        ...
    }
    ...

    /* Establish migration ptes or remove ptes */
    try_to_unmap(page, TTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);

skip_unmap:
    if (!page_mapped(page))
        rc = move_to_new_page(newpage, page, remap_swapcache, mode);

    ...
out:
    return rc;
}

static int move_to_new_page(struct page *newpage, struct page *page,
                    int remap_swapcache, enum migrate_mode mode)
{
    struct address_space *mapping;
    int rc;

    /*
     * Block others from accessing the page when we get around to
     * establishing additional references. We are the only one
     * holding a reference to the new page at this point.
     */
    if (!trylock_page(newpage))
        BUG();

    /* Prepare mapping for the new page.*/
    newpage-&amp;gt;index = page-&amp;gt;index;
    newpage-&amp;gt;mapping = page-&amp;gt;mapping;
    if (PageSwapBacked(page))
        SetPageSwapBacked(newpage);
    
    mapping = page_mapping(page);
    /*调用底层文件系统接口实现页迁移*/
    if (!mapping)
        rc = migrate_page(mapping, newpage, page, mode);
    else if (mapping-&amp;gt;a_ops-&amp;gt;migratepage)
    /*
     * Most pages have a mapping and most filesystems provide a
     * migratepage callback. Anonymous pages are part of swap
     * space which also has its own migratepage callback. This
     * is the most common path for page migration.
     */
        rc = mapping-&amp;gt;a_ops-&amp;gt;migratepage(mapping, newpage, page, mode);
    else
        rc = fallback_migrate_page(mapping, newpage, page, mode);
    if (rc != MIGRATEPAGE_SUCCESS) {
        newpage-&amp;gt;mapping = NULL;
    } else {
        if (remap_swapcache)
            remove_migration_ptes(page, newpage);
        page-&amp;gt;mapping = NULL;
    }

    unlock_page(newpage);

    return rc;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，慢速分配中的核心的内存压缩流程已分析完成，更底层的细节涉及具体文件系统实现，大家可以继续深入分析。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/12/内存压缩/&quot;&gt;内存管理之五：内存压缩&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%8E%8B%E7%BC%A9/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/12/%E5%86%85%E5%AD%98%E5%8E%8B%E7%BC%A9/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>内存管理之四：内存交换</title>
        <description>&lt;p&gt;  本节将讨论内存交换，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是内存交换为什么需要它&quot;&gt;什么是内存交换？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  当系统内存使用压力较大时，内核会将访问率不好的匿名页暂时写到磁盘(换出)以释放这部分匿名页供系统使用；当换出的页再次被访问时，内核重新将其读入(换入)。&lt;/p&gt;

&lt;p&gt;  内存交换是回收匿名页的唯一手段。我们知道，应用程序使用的内存页分为文件映射页和匿名页两种。对于文件映射页，回收时可以将它的内容回刷到映射文件中。而对于匿名页(也包括私有的文件映射页)，默认情况下没有持久化文件和它对应。如果要回收这部分内存，必然要寻找可以暂时存放内容的存储对象。因此就出现了交换分区和内存交换的概念。&lt;/p&gt;

&lt;h3 id=&quot;如何实现&quot;&gt;如何实现？&lt;/h3&gt;

&lt;h4 id=&quot;交换分区&quot;&gt;&lt;strong&gt;交换分区&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  我们可以通过设置交换分区来打开内存交换功能，具体可参考swapon命令的使用方法。交换分区可以是一个物理磁盘分区，也可以是一个普通文件，在内核中只要是一个文件对象就可以。&lt;/p&gt;

&lt;p&gt;  交换分区以页为单位进行管理分配，内核通过一个引用数组记录交换分区中的每个页被进程的引用次数。每次换出同一页时，该页对应的引用数加一；换入时则相反，每换入一次就减一。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory4_0.jpg&quot; height=&quot;300&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;换出时机&quot;&gt;&lt;strong&gt;换出时机&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在内存回收快速分配流程中，如果选择回收匿名页，或者在慢速流程中唤醒kswapd内核服务进程回收匿名页时，都会将匿名页换出。&lt;/p&gt;

&lt;h4 id=&quot;换出流程&quot;&gt;&lt;strong&gt;换出流程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  在shrink_page_list函数中，我们关注其中和匿名页相关的部分就可以了解整个换出流程。代码片断参考&lt;a href=&quot;https://rootw.github.io/2017/10/内存回收/&quot;&gt;内存回收&lt;/a&gt;，更深入的细节需要进一步深入pageout函数进行分析。&lt;/p&gt;

&lt;p&gt;  整个过程可以参考下图：匿名页换出前(图a)，首先要把它加到交换分区文件对应的交换缓存(Swap Cache)中(图b)。交换缓存其实就是交换分区对应的文件缓存，和普通文件缓存一样，它也是通过一棵radix树进行数据组织。交换缓存把交换过程和文件系统关联了起来，我们可以通过文件系统抽象接口完成交换动作。另外，交换缓存也成为换出和换入过程需要使用的共享资源，通过锁机制可以有效达到同步效果。&lt;/p&gt;

&lt;p&gt;  接着内核便通过反向映射找到匿名页所有的映射页表，解除页表映射(图c)，并在页表中填入匿名页在交换分区中对应的位置信息。之所以在页表中填入匿名页在交换分区的存放位置，是便于在换入时重新读取页内容。&lt;/p&gt;

&lt;p&gt;  映射关系全部解除后，如果脏页内容已回刷完成，内核就可以将匿名页从缓存分区中移除并回收该页(图e)。图d表示解除页表映射后，进程B再次访问该匿名页的场景。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory4_1.jpg&quot; height=&quot;450&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;换入流程&quot;&gt;&lt;strong&gt;换入流程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  换入发生在被换出的匿名页再次被访问时，此时首先会进入页异常处理过程。内核通过判断页表内容确认需要进行换入操作时，将调用do_swap_page完成换入动作。&lt;/p&gt;

&lt;p&gt;  该函数整体思路比较明确：先查找交换缓存看是否存在期望的匿名页，如果存在则重新映射并更新页表；否则就分配新页加到交换缓存中，在读取页内容完成后即可映射给页表。每次发生换入操作时，交换分区对应页的引用计数减一，当发现交换分区中对应页的引用为零时，表示没有进程引用该页，便可将交换分区中的页回收。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/memory.c:

static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
                    unsigned long address, pte_t *page_table, pmd_t *pmd,
                    unsigned int flags, pte_t orig_pte)
{
    spinlock_t *ptl;
    struct page *page, *swapcache;
    swp_entry_t entry;
    pte_t pte;
    int locked;
    int exclusive = 0;
    int ret = 0;

    entry = pte_to_swp_entry(orig_pte); /*从页表项中找出换出页在交换分区中的位置*/
    
    ...

    page = lookup_swap_cache(entry); /*查找交换缓冲区*/
    if (!page) {
        /*如果交换缓冲区未缓存当前交换位置，则重新分配内存页并从交换分区中读入内容*/
        page = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, vma, address);
        ...
    } else if (PageHWPoison(page)) {
    ...
    }

    swapcache = page;
    locked = lock_page_or_retry(page, mm, flags);

    ...

    /*重新在页表中添加映射*/
    page_table = pte_offset_map_lock(mm, pmd, address, &amp;amp;ptl);
    ...
    pte = mk_pte(page, vma-&amp;gt;vm_page_prot);
    ...
    set_pte_at(mm, address, page_table, pte);
    if (page == swapcache)
        do_page_add_anon_rmap(page, vma, address, exclusive);
    else /* ksm created a completely new copy */
        page_add_new_anon_rmap(page, vma, address);
    ...
    swap_free(entry); /*减少当前交换位置的引用计数*/
    ...
    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，快速分配流程中涉及的回收和交换过程已分析完毕，后续我们将分析慢速分配流程中的内存压缩(迁移)功能。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/11/内存交换/&quot;&gt;内存管理之四：内存交换&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 03 Nov 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/11/%E5%86%85%E5%AD%98%E4%BA%A4%E6%8D%A2/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/11/%E5%86%85%E5%AD%98%E4%BA%A4%E6%8D%A2/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>内存管理之三：内存回收</title>
        <description>&lt;p&gt;  本节将讨论内存回收，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是内存回收为什么需要它&quot;&gt;什么是内存回收？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  内存分配过程中如果发现剩余内存量低于预定的水位线(代表内存使用紧张)，就会强制回收一部分使用频度不高的已分配内存，供后续分配使用。如此一来，好的方面是可最大限度满足系统内应用程序的内存分配请求，提升系统可用性。坏的方面是被回收页所属的应用可能再次访问该页，需要通过缺页处理再次分配映射页，从而带来应用性能的下降。一个优秀的内存回收算法需要在系统整体可用性和应用性能之间寻找合适的平衡点。&lt;/p&gt;

&lt;h3 id=&quot;如何实现&quot;&gt;如何实现？&lt;/h3&gt;

&lt;h4 id=&quot;1-四大链表&quot;&gt;&lt;strong&gt;1. 四大链表&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  内存回收需要思考的第一个问题是关于回收对象，即回收谁？Linux内核只能回收动态分配给应用程序的内存页，内核自身直接分配使用的页是不参与回收的(通过slab分配的内存可回收，这里我们不做深入讨论)。应用程序使用的内存页要么是通过文件映射的(如代码段)，要么是匿名映射的(如堆栈段)。因此内核将分配给应用程序的内存页分为两大类，即文件页和匿名页；同时根据内存页使用的频度又分为活跃页和不活跃
页。这样，内核针对NUMA节点的每个zone将已分配页放到四个LRU链表中：非活跃匿名页链表、活跃匿名页链表、非活跃文件页链表、活跃文件页链表。触发内存回收后，内核会将活跃页链表中使用频度低的页淘汰到非活跃页链表中，再从非活跃页链表中取出频度低的页直接回收。每个zone的四大链表定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/linux/mmzone.h:

/*
 * We do arithmetic on the LRU lists in various places in the code,
 * so it is important to keep the active lists LRU_ACTIVE higher in
 * the array than the corresponding inactive lists, and to keep
 * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
 *
 * This has to be kept in sync with the statistics in zone_stat_item
 * above and the descriptions in vmstat_text in mm/vmstat.c
 */
#define LRU_BASE 0
#define LRU_ACTIVE 1
#define LRU_FILE 2

enum lru_list {
    LRU_INACTIVE_ANON = LRU_BASE,
    LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
    LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
    LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
    LRU_UNEVICTABLE,
    NR_LRU_LISTS
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  了解四大链表结构后，可以思考内核是如何感知内存页的活跃程度的？内核是通过mark_page_accessed函数显式标记页的活跃程度。初始分配的匿名页被放置到活跃链表中，而文件页被放置到非活跃链表中，并由内核在后续操作过程中显式标记活跃程度：连续两次被访问后，该页将被移动到活跃页链表中。相关函数如下:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/swap.c:

/*
 * Mark a page as having seen activity.
 *
 * inactive,unreferenced	-&amp;gt;	inactive,referenced
 * inactive,referenced		-&amp;gt;	active,unreferenced
 * active,unreferenced		-&amp;gt;	active,referenced
 */
void mark_page_accessed(struct page *page)
{
    if (!PageActive(page) &amp;amp;&amp;amp; !PageUnevictable(page) &amp;amp;&amp;amp;
            PageReferenced(page) &amp;amp;&amp;amp; PageLRU(page)) {
        activate_page(page);
        ClearPageReferenced(page);
    } else if (!PageReferenced(page)) {
        SetPageReferenced(page);
    }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2-整体流程&quot;&gt;&lt;strong&gt;2. 整体流程&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  以下调用流程以页分配为出发点来跟踪内存回收shrink_zone：get_scan_count计算各个链表的回收比例，然后再通过shrink_list依次回收。下面我们进一步展开。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;alloc_pages
    -&amp;gt;get_page_from_freelist
        -&amp;gt;zone_reclaim
            -&amp;gt;shrink_zone
                -&amp;gt;shrink_lruvec
                    -&amp;gt;get_scan_count
                    -&amp;gt;shrink_list
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;21-回收比例&quot;&gt;&lt;strong&gt;2.1 回收比例&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  内存回收时有四大链表供选择，每次回收时都需要扫描所有链表吗？不是这样的，内核通过get_scan_count计算每个链表的扫描比例，比例越高回收的页可能就越多。这个函数详细的代码就不分析了，有点复杂，我们只需要知道最终nr数组会记录每个链表的扫描页数。&lt;/p&gt;

&lt;h4 id=&quot;22-活跃链表回收&quot;&gt;&lt;strong&gt;2.2 活跃链表回收&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  通过下面shrink_list的代码，我们可以清楚看到，针对活跃链表，如果非活跃页偏少则通过shrink_active_list将部分活跃页移动到非活跃页中，此时并不进行回收动作。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/vmscan.c:

static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
                            struct lruvec *lruvec, struct scan_control *sc)
{
    if (is_active_lru(lru)) {
        if (inactive_list_is_low(lruvec, lru))
            shrink_active_list(nr_to_scan, lruvec, sc, lru);
        return 0;
    }

    return shrink_inactive_list(nr_to_scan, lruvec, sc, lru);
}

static void shrink_active_list(unsigned long nr_to_scan,
                        struct lruvec *lruvec,
                        struct scan_control *sc,
                        enum lru_list lru)
{
    unsigned long nr_taken;
    unsigned long nr_scanned;
    unsigned long vm_flags;
    LIST_HEAD(l_hold);	/* The pages which were snipped off */
    LIST_HEAD(l_active);
    LIST_HEAD(l_inactive);
    struct page *page;
    unsigned long nr_rotated = 0;
    isolate_mode_t isolate_mode = 0;
    int file = is_file_lru(lru);
    struct zone *zone = lruvec_zone(lruvec);

    ...

    spin_lock_irq(&amp;amp;zone-&amp;gt;lru_lock);
    /*从活跃链表lruvec[lru]中移动部分页到l_hold中，nr_scanned记录移动页数*/
    nr_taken = isolate_lru_pages(nr_to_scan, lruvec, &amp;amp;l_hold,
                        &amp;amp;nr_scanned, sc, isolate_mode, lru);
    ...
    spin_unlock_irq(&amp;amp;zone-&amp;gt;lru_lock);

    while (!list_empty(&amp;amp;l_hold)) {
        /*针对l_hold中的每一页，偿试将其移动到非活跃链表中*/
        cond_resched();
        page = lru_to_page(&amp;amp;l_hold);
        list_del(&amp;amp;page-&amp;gt;lru);

        ...

        /*判断当前页page是否被访问，涉及反向映射，大家感兴趣可以深入分析*/
        if (page_referenced(page, 0, sc-&amp;gt;target_mem_cgroup, &amp;amp;vm_flags)) {
            nr_rotated += hpage_nr_pages(page);
            /*
             * Identify referenced, file-backed active pages and
             * give them one more trip around the active list. So
             * that executable code get better chances to stay in
             * memory under moderate memory pressure.  Anon pages
             * are not likely to be evicted by use-once streaming
             * IO, plus JVM can create lots of anon VM_EXEC pages,
             * so we ignore them here.
             */
            /*针对JVM场景，保留VM_EXEC段的文件页在活跃链表中*/
            if ((vm_flags &amp;amp; VM_EXEC) &amp;amp;&amp;amp; page_is_file_cache(page)) {
                list_add(&amp;amp;page-&amp;gt;lru, &amp;amp;l_active);
                continue;
            }
        }
        
        /*清除当前页的活跃标记，并放入临时链表l_inactive中，为后续移动作准备*/
        ClearPageActive(page);	/* we are de-activating */
        list_add(&amp;amp;page-&amp;gt;lru, &amp;amp;l_inactive);
    }

    /*
     * Move pages back to the lru list.
     */
    spin_lock_irq(&amp;amp;zone-&amp;gt;lru_lock);
    ...
    move_active_pages_to_lru(lruvec, &amp;amp;l_active, &amp;amp;l_hold, lru); /*批量将l_active中的页移回活跃链表*/
    move_active_pages_to_lru(lruvec, &amp;amp;l_inactive, &amp;amp;l_hold, lru - LRU_ACTIVE); /*批量将l_inactive中的页移到非活跃链表中*/
    __mod_zone_page_state(zone, NR_ISOLATED_ANON + file, -nr_taken);
    spin_unlock_irq(&amp;amp;zone-&amp;gt;lru_lock);

    free_hot_cold_page_list(&amp;amp;l_hold, 1); /*l_hold保留移动过程中已被释放的内存页，这里将其正式释放*/
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;23-非活跃链表回收&quot;&gt;&lt;strong&gt;2.3 非活跃链表回收&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  非活跃链表中的页是重点回收对象，核心功能在shrink_page_list函数中实现。该函数对非活跃链表尾部的若干页依次进行扫描：首先对当前扫描页加锁，避免扫描期间同时存在其它页操作；接着通过writeback标记判断当前扫描页是否正在被回写，如果是则跳过当前页去扫描下一页；接着判断当前页是否被访问过，如果页表的accessed位被置位则说明页被访问过，清除标记后任跳过当前页；页未被访问情况下，为匿名页添加交换分区映射(文件页必然有文件与之相应)，之后正式解除当前扫描页的页表映射；页表映射解除成功后，开始回刷脏页，待脏页回刷完成后最后将当前页从文件缓存映射或匿名映射中移除，之后便可释放当前页。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/vmscan.c:

/*
 * shrink_page_list() returns the number of reclaimed pages
 */
static unsigned long shrink_page_list(struct list_head *page_list,
                                struct zone *zone,
                                struct scan_control *sc,
                                enum ttu_flags ttu_flags,
                                unsigned long *ret_nr_dirty,
                                unsigned long *ret_nr_writeback,
                                bool force_reclaim)
{
/*参数说明：page_list记录待回收的非活跃页；zone代表当前被回收的zone；sc为回收控制变量；
          ttu_flags表示unmap解除页表映射操作标志；ret_nr_dirty表示返回给外层的脏页数；
          ret_nr_writeback表示返回给外层的正在回写的页数；force_relcaim代表强制回收*/

    LIST_HEAD(ret_pages); /*保存不可回收的页*/
    LIST_HEAD(free_pages); /*保存可回收的页*/
    int pgactivate = 0;
    unsigned long nr_dirty = 0;
    unsigned long nr_congested = 0;
    unsigned long nr_reclaimed = 0;
    unsigned long nr_writeback = 0;

    
    while (!list_empty(page_list)) { /*对page_list中待回收页依次进行回收*/
        struct address_space *mapping;
        struct page *page;
        int may_enter_fs;
        enum page_references references = PAGEREF_RECLAIM_CLEAN;

        cond_resched();

        page = lru_to_page(page_list);
        list_del(&amp;amp;page-&amp;gt;lru);

        /*偿试锁定当前页，如果无法锁定则保留当前页*/
        if (!trylock_page(page))
            goto keep;

        ...

        /*处理writeback页，该标记表示内存页正在被回刷落盘，但IO操作有可能还未完成*/
        if (PageWriteback(page)) {

            if (global_reclaim(sc) || !PageReclaim(page) || !may_enter_fs) {
                /*对于全局回收等场景，不对writeback页作回收，将其放回链表*/
                SetPageReclaim(page);
                nr_writeback++;
                goto keep_locked;
            }
            wait_on_page_writeback(page); /*其它情况下将等待页面回刷完成*/
        }

        /*对于非强制回收场景，需要检查当前待回收页的访问情况，确定访问不频繁才可回收*/
        if (!force_reclaim)
            references = page_check_references(page, sc);

        switch (references) {
        case PAGEREF_ACTIVATE:
            goto activate_locked;
        case PAGEREF_KEEP:
            goto keep_locked;
        case PAGEREF_RECLAIM:
        case PAGEREF_RECLAIM_CLEAN:
            ;/* try to reclaim the page below */
        }

        /*如果执行到这里，说明当前页是一个可回收的非writeback页*/

        /*
         * Anonymous process memory has backing store?
         * Try to allocate it some swap space here.
         */
        if (PageAnon(page) &amp;amp;&amp;amp; !PageSwapCache(page)) {
            /*针对匿名页，如果不在交换缓冲区中(交换分区对应的内存页缓存)，需要将它加入其中*/
            if (!(sc-&amp;gt;gfp_mask &amp;amp; __GFP_IO))
                goto keep_locked;
            if (!add_to_swap(page, page_list))
                goto activate_locked;
                may_enter_fs = 1;
        }

        mapping = page_mapping(page);

        /*执行到这里，说明当前页已建立后端映射关系：文件页对应文件，匿名页对应交换分区*/

        /*
         * The page is mapped into the page tables of one or more
         * processes. Try to unmap it here.
         */
        if (page_mapped(page) &amp;amp;&amp;amp; mapping) {
            /*开始解除页表中的映射关系并刷新TLB表*/
            switch (try_to_unmap(page, ttu_flags)) {
            case SWAP_FAIL:
                goto activate_locked;
            case SWAP_AGAIN:
                goto keep_locked;
            case SWAP_MLOCK:
                goto cull_mlocked;
            case SWAP_SUCCESS:
                ; /* try to free the page below */
            }
        }

        if (PageDirty(page)) {
            /*对于脏页，开始执行回刷动作*/
            nr_dirty++;

            ...

            /* Page is dirty, try to write it out here */
            switch (pageout(page, mapping, sc)) { /*调用底层文件系统接口进行回刷*/
            case PAGE_KEEP:
                nr_congested++;
                goto keep_locked;
            case PAGE_ACTIVATE:
                goto activate_locked;
            case PAGE_SUCCESS:
                /*回刷动作触发成功后，如果页面处在writeback状态，则不在本轮进行回收*/
                if (PageWriteback(page))
                    goto keep;
                if (PageDirty(page))
                    goto keep;

                /*
                 * A synchronous write - probably a ramdisk.  Go
                 * ahead and try to reclaim the page.
                 */
                /*对于ramdisk这类场景，回刷触发立刻就完成了，不会置writeback，可继续进行回收*/
                if (!trylock_page(page))
                    goto keep;
                if (PageDirty(page) || PageWriteback(page))
                    goto keep_locked;
                mapping = page_mapping(page);
            case PAGE_CLEAN:
                ; /* try to free the page below */
            }
        }

        ...

        /*从文件缓存或交换缓存中将当前页移除，彻底解除当前页的映射关系*/
        if (!mapping || !__remove_mapping(mapping, page))
            goto keep_locked;

        __clear_page_locked(page);
free_it:
        nr_reclaimed++;

        /*
         * Is there need to periodically free_page_list? It would
         * appear not as the counts should be low
         */
        list_add(&amp;amp;page-&amp;gt;lru, &amp;amp;free_pages); /*将当前页加入到待回收链表中*/
        continue;

cull_mlocked:
        if (PageSwapCache(page))
            try_to_free_swap(page);
        unlock_page(page);
        putback_lru_page(page);
        continue;

activate_locked:
        /* Not a candidate for swapping, so reclaim swap space. */
        if (PageSwapCache(page) &amp;amp;&amp;amp; vm_swap_full())
            try_to_free_swap(page);
        VM_BUG_ON(PageActive(page));
        SetPageActive(page);
        pgactivate++;
keep_locked:
        unlock_page(page);
keep:
        list_add(&amp;amp;page-&amp;gt;lru, &amp;amp;ret_pages);
        VM_BUG_ON(PageLRU(page) || PageUnevictable(page));
    } // end of while

    free_hot_cold_page_list(&amp;amp;free_pages, 1); /*正式回收free_pages链表中的所有页*/

    ...
    return nr_reclaimed;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;  至此，内存回收过程已基本分析完成，其中对于匿名页，回收过程会触发内存交换，有关内存交换的详细内容我们将在后续博文中给出。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/10/内存回收/&quot;&gt;内存管理之三：内存回收&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Oct 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/10/%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/10/%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>内存管理之二：内存分配</title>
        <description>&lt;p&gt;  本节将讨论内存分配，计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是内存分配为什么需要它有何技术难点&quot;&gt;什么是内存分配？为什么需要它？有何技术难点？&lt;/h3&gt;

&lt;p&gt;  前文分析页异常处理时，我们看到内核会为应用程序分配内存页，而应用程序本身不直接申请内存页(只会通过malloc在堆中动态申请内存)。此外，内核在处理系统调用或中断时，也有可能需要动态申请页。Linux内核实现内存页申请的接口是alloc_pages(gfp_mask, order)：其中，gfp_mask代表申请内存的方式(隐含内存用途)，控制内存分配的行为；order代表需要的连续内存页个数的阶；接口返回的是连续内存首页对应的struct page指针(还记得地址映射中介绍的virtual memory map么？)。&lt;/p&gt;

&lt;p&gt;  通过动态分配内存页，可以提升系统对内存资源的利用率。然而，在内存不断的申请和释放过程中，可以想象的一个问题就是碎片化，即剩余内存分布在各段非连续空间内，使得较大连续内存的申请无法得到满足。因此内存分配算法的主要目标就是避免碎片化。&lt;/p&gt;

&lt;p&gt;  有的同学可能会问，为什么一定需要连续的物理内存呢？页表机制可以把非连续的物理页映射到连续的虚拟地址空间，这样应用程序看到的不就是连续内存了吗？的确，对于应用程序来说，对连续内存页的需求不是很强烈，但是对于某些外设(例如DMA设备)来说，它们只能看到物理页，而不存在页表转换过程，此时内核只能分配连续物理页来满足设备的内存访问需求。所以连续的物理内存在内核看来是一种稀缺资源，内存分配时需要尽可能保证剩余内存的连续性。下面我们就来看看Linux内核中内存的分配算法。&lt;/p&gt;

&lt;h3 id=&quot;如何实现--优化型伙伴算法&quot;&gt;如何实现？- 优化型伙伴算法&lt;/h3&gt;

&lt;p&gt;  说到内存分配算法，不得不提一提耳熟能详、在各类教科书中不断被说明的伙伴算法(buddy algorithm)。连续内存被划分成各阶大小(1,2,4,8,…)的内存段，阶数相同且低地址段起始地址按阶对齐的相邻段被称为伙伴，如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory2_1.jpg&quot; height=&quot;300&quot; width=&quot;600&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  图中所示为连续八页内存中被分配出去一页时的状态：剩余内存被分为三段，分别为一个页、两个页和四个页。&lt;/p&gt;

&lt;p&gt;  伙伴算法的原理为：初始时，所有连续内存按最大阶(Linux中为10，代表最大连续页为1024页)划分成段。内存分配时按阶数从小到大的顺序寻找最先能满足当前分配大小的连续段，如果找到的段的阶大于分配需要的阶，则将找到的段拆分成低阶段，例如上图表示将从一个3阶内存段中分配一个0阶内存段(即一页)的情况。内存释放时，如果发现被释放页的伙伴页段均空闲，则将两个伙伴合并从一个大的连续段并继续尝试合并新段和它的伙伴，直到无法合并为止。因此只要相邻伙伴均被释放，内存总是能被合并成更大的页段，这就是伙伴算法名称的由来。&lt;/p&gt;

&lt;p&gt;  伙伴算法真的完美吗？它在分配内存页时确实在努力保证剩余内存的连续，即在小段连续内存能满足当前分配的情况下绝对不会去动用大段连续内存，这就防止了因分配不当产生碎片。然而，它却无法避免因连续分配和释放而产生的碎片，因为只有当相邻两个伙伴均空闲时才能进行合并，这意味着一个伙伴的占用将阻止内存合并的发生。&lt;/p&gt;

&lt;p&gt;  因伙伴页被长期占用而导致的碎片问题的确比分配产生的碎片要难处理得多。这不由让人联想到磁盘的“碎片整理”功能，也就是通过移动正在使用的空间来保证剩余空间的连续。那么我们是否可以将类似思路应用到内存管理中呢？&lt;/p&gt;

&lt;p&gt;  通过观察我们可以发现使用中的内存页也具有一定的移动性：对于分配给应用程序使用的页是可以移动的(movable)，因为通过页表的重映射可以在应用不感知的前提下实现页替换；对于分配给内核自身使用的页是不可移动的(unmovable)，因为内核以直接映射的方式访问该页；对于未被映射给应用的缓存页是可回收的(reclaimable)，因为回刷缓存后缓存页可直接释放。如果我们将不可移动或回收的页限定在一定的范围内，就可以保证剩余范围内的页是可移动的，那么我们就可以在剩余范围内通过移动内存页来实现“碎片整理”。Linux内核就是基于上述思路，将伙伴算法中的各阶空闲内存细分成不可移动、可回收和可移动三大类，分配时通过gfp_mask标志来控制从哪类内存中分配空间。对于可移动内存页，内核在内存不足时会进行内存压缩(compact)来尝试形成更大的连续内存。上述算法即是优化型伙伴算法。&lt;/p&gt;

&lt;p&gt;  下面我们结合代码来深入理解优化型伙伴算法。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/page_alloc.c:

/*
 * This is the 'heart' of the zoned buddy allocator.
 */
struct page *
__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
                        struct zonelist *zonelist, nodemask_t *nodemask)
{
/*在开始分析代码之前，先要理解NUMA这个概念：Non Uniform Memory Access，即非统一内存访问。
  我们前文举例的i440fx并不属于NUMA架构，NUMA架构通常会有多个计算节点，每个节点都有独立的内
  存，且每个节点在访问本地内存时的性能优于远端节点。linux内核通过ACPI中的SRAT表获知详细的
  NUMA信息。当然，我们可以将i440fx简单理解成只有一个节点的NUMA系统。*/
/*此外，内核会将整体物理内存空间划分成不同的区段(zone)：DMA段－16M地址空间以下；DMA32段－
  4G地址空间以下；Normal段－4G以上内存。因此每个NUMA节点可能包含多个zone。linux内核针对
  zone进行内存分配管理(伙伴算法)。*/

/*参数zonelist是根据gfp_mask计算得出的，以GFP_KERNEL为例，zonelist指向的通常就是当前进
  程所在NUMA节点包含的所有zone(内存分配策略为node first)。nodemask表示内存分配时需要过滤
  哪些节点，NULL表示不过滤任何节点。*/

    enum zone_type high_zoneidx = gfp_zone(gfp_mask); /*根据gfp_mask计算可分配的最
                                    大zone(Normal &amp;gt; DMA32 &amp;gt; DMA)，zone越小表示内存
                                    资源越稀缺。以GFP_KERNEL为例，这里计算可得normal*/
    struct zone *preferred_zone;
    struct page *page = NULL;
    int migratetype = allocflags_to_migratetype(gfp_mask); /*根据GFP中的__GFP_MOVABLE
                                    和__GFP_RECLAIMABLE标志计算待分配页的迁移类型，以
                                    GFP_KERNEL为例，这里计算得出MIGRAGE_UNMOVABLE*/
    
    ...

    /*先进行快速内存分配策略，大部分分配动作将在该函数中成功返回。如果空闲内存不足，快速分配会
      偿试内存回收*/
    page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask, order,
            zonelist, high_zoneidx, alloc_flags, preferred_zone, migratetype);
    if (unlikely(!page)) {
        ...
        /*如果快速分配失败，再进行慢速分配*/
        page = __alloc_pages_slowpath(gfp_mask, order,
                zonelist, high_zoneidx, nodemask, preferred_zone, migratetype);
    }

    ...

    return page;
}


/*下面我们补充一些内存分配标记相关的内容：*/
linux/include/linux/gfp.h:

/*
 * Action modifiers - doesn't change the zoning
 *
 * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt
 * _might_ fail.  This depends upon the particular VM implementation.
 *
 * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
 * cannot handle allocation failures.  This modifier is deprecated and no new
 * users should be added.
 *
 * __GFP_NORETRY: The VM implementation must not retry indefinitely.
 *
 * __GFP_MOVABLE: Flag that this page will be movable by the page migration
 * mechanism or reclaimed
 */
#define __GFP_WAIT	((__force gfp_t)___GFP_WAIT)	/* Can wait and reschedule? */
#define __GFP_HIGH	((__force gfp_t)___GFP_HIGH)	/* Should access emergency pools? */
#define __GFP_IO	((__force gfp_t)___GFP_IO)	/* Can start physical IO? */
#define __GFP_FS	((__force gfp_t)___GFP_FS)	/* Can call down to low-level FS? */
#define __GFP_COLD	((__force gfp_t)___GFP_COLD)	/* Cache-cold page required */
#define __GFP_NOWARN	((__force gfp_t)___GFP_NOWARN)	/* Suppress page allocation failure warning */
#define __GFP_REPEAT	((__force gfp_t)___GFP_REPEAT)	/* See above */
#define __GFP_NOFAIL	((__force gfp_t)___GFP_NOFAIL)	/* See above */
#define __GFP_NORETRY	((__force gfp_t)___GFP_NORETRY) /* See above */
#define __GFP_MEMALLOC	((__force gfp_t)___GFP_MEMALLOC)/* Allow access to emergency reserves */
#define __GFP_COMP	((__force gfp_t)___GFP_COMP)	/* Add compound page metadata */
#define __GFP_ZERO	((__force gfp_t)___GFP_ZERO)	/* Return zeroed page on success */
#define __GFP_NOMEMALLOC ((__force gfp_t)___GFP_NOMEMALLOC) /* Don't use emergency reserves.
                                                             * This takes precedence over the
                                                             * __GFP_MEMALLOC flag if both are
                                                             * set
                                                             */
#define __GFP_HARDWALL   ((__force gfp_t)___GFP_HARDWALL) /* Enforce hardwall cpuset memory allocs */
#define __GFP_THISNODE	((__force gfp_t)___GFP_THISNODE)/* No fallback, no policies */
#define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE) /* Page is reclaimable */
#define __GFP_NOTRACK	((__force gfp_t)___GFP_NOTRACK)  /* Don't track with kmemcheck */

#define __GFP_NO_KSWAPD	((__force gfp_t)___GFP_NO_KSWAPD)
#define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE) /* On behalf of other node */
#define __GFP_KMEMCG	((__force gfp_t)___GFP_KMEMCG) /* Allocation comes from a memcg-accounted resource */
#define __GFP_WRITE	((__force gfp_t)___GFP_WRITE)	/* Allocator intends to dirty page */

/*
* This may seem redundant, but it's a way of annotating false positives vs.
* allocations that simply cannot be supported (e.g. page tables).
*/
#define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)

#define __GFP_BITS_SHIFT 25	/* Room for N __GFP_FOO bits */
#define __GFP_BITS_MASK ((__force gfp_t)((1 &amp;lt;&amp;lt; __GFP_BITS_SHIFT) - 1))

/* This equals 0, but use constants in case they ever change */
#define GFP_NOWAIT	(GFP_ATOMIC &amp;amp; ~__GFP_HIGH)
/* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */
#define GFP_ATOMIC	(__GFP_HIGH)
#define GFP_NOIO	(__GFP_WAIT)
#define GFP_NOFS	(__GFP_WAIT | __GFP_IO)
#define GFP_KERNEL	(__GFP_WAIT | __GFP_IO | __GFP_FS)
#define GFP_TEMPORARY	(__GFP_WAIT | __GFP_IO | __GFP_FS | \
                        __GFP_RECLAIMABLE)
#define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)
#define GFP_HIGHUSER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | \
                        __GFP_HIGHMEM)
#define GFP_HIGHUSER_MOVABLE	(__GFP_WAIT | __GFP_IO | __GFP_FS | \
                        __GFP_HARDWALL | __GFP_HIGHMEM | \
                        __GFP_MOVABLE)
#define GFP_IOFS	(__GFP_IO | __GFP_FS)
#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \
                    __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \
                    __GFP_NO_KSWAPD)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  内存分配有一个快速分配流程和一个慢速分配流程，采用先快后慢的思路。快速分配根据zone内空闲内存量决定是否使用快速内存回收，如果空闲量足够的话，则直接采用优化型伙伴算法进行分配，否则先进行快速回收再尝试分配。快速分配一旦失败就会使用慢速分配的方式唤醒内存交换进程(kswapd)或触发内存压缩继续尝试分配。下面我们再深入看一下快速分配流程get_page_from_freelist：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/page_alloc.c:

/*
 * get_page_from_freelist goes through the zonelist trying to allocate
 * a page.
 */
static struct page *
get_page_from_freelist(gfp_t gfp_mask, nodemask_t *nodemask, unsigned int order,
                struct zonelist *zonelist, int high_zoneidx, int alloc_flags,
                struct zone *preferred_zone, int migratetype)
{
    struct zoneref *z;
    struct page *page = NULL;
    int classzone_idx;
    struct zone *zone;

    classzone_idx = zone_idx(preferred_zone);
zonelist_scan:
    /*
     * Scan zonelist, looking for a zone with enough free.
     * See also cpuset_zone_allowed() comment in kernel/cpuset.c.
     */
    /*下面的循环体开始遍历zonelist中的每个zone，最大为high_zoneidx，并过滤nodemask*/
    for_each_zone_zonelist_nodemask(zone, z, zonelist, high_zoneidx, nodemask) {
        ...
        if (!(alloc_flags &amp;amp; ALLOC_NO_WATERMARKS)) {
            unsigned long mark;
            int ret;

            /*内存管理初始化时，分为每个zone设定high、low、min三档水位线：空闲内存量大于high表示
              余量充足；空闲量小于high大于low表示有一定的内存压力，余量尚可；空闲量小于low大于min
              表示内存压力较大，余量不足；空闲量小于min，表示内存压力非常大，需要动用紧急内存*/

            /*快速分配时alloc_flags考量的是low水位线，只要当前zone的空闲内存量不低于low水位线，
              都会偿试通过伙伴算法进行分配，否则就进行内存回收*/
            mark = zone-&amp;gt;watermark[alloc_flags &amp;amp; ALLOC_WMARK_MASK];
            if (zone_watermark_ok(zone, order, mark, classzone_idx, alloc_flags))
                goto try_this_zone;
            ...

            ret = zone_reclaim(zone, gfp_mask, order); /*进行内存回收*/
            switch (ret) {
            case ZONE_RECLAIM_NOSCAN:
                /* did not scan */
                continue;
            case ZONE_RECLAIM_FULL:
                /* scanned but unreclaimable */
                continue;
            default:
                /* did we reclaim enough */
                /*回收部分内存后，如果水位线满足要求则偿试进行分配*/
                if (zone_watermark_ok(zone, order, mark, classzone_idx, alloc_flags))
                    goto try_this_zone;

            ...

            continue;
            }
        }

try_this_zone:

    /*正式进行内存分配*/
    page = buffered_rmqueue(preferred_zone, zone, order, gfp_mask, migratetype);
    if (page)
        break;
this_zone_full:
    ...
    }

    ...

    return page;
}


static inline
struct page *buffered_rmqueue(struct zone *preferred_zone,
                struct zone *zone, int order, gfp_t gfp_flags, int migratetype)
{
    unsigned long flags;
    struct page *page;

again:
    if (likely(order == 0)) {
        /*对于单页内存的分配，内核为cpu预留了一部分缓存空间；分配和释放进首先从缓存中进行。
          这样可以提升单页内存分配的效率，具体代码大家可以自行展开分析*/
        ...
    } else {
        spin_lock_irqsave(&amp;amp;zone-&amp;gt;lock, flags);
        /*偿试从当前zone中根据migratetype类型进行内存页分配，内部会调用__rmqueue_smallest*/
        page = __rmqueue(zone, order, migratetype);
        spin_unlock(&amp;amp;zone-&amp;gt;lock);
        ...
    }
    ...
    return page;
    ...
}

/*__rmqueue_smallest就是伙伴算法最核心的代码实现，逻辑比较清楚，大家可以仔细阅读*/
static inline
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order, int migratetype)
{
    unsigned int current_order;
    struct free_area * area;
    struct page *page;

    /*数据结构上，每个zone都有一个free_area数组，下标代表伙伴阶数，最大为10(MAX_ORDER为11)。
      在优化型伙伴算法中，freea_area每个元素再次按迁移性分为不可移动、可移动、可回收三个部分*/
    /* Find a page of the appropriate size in the preferred list */
    for (current_order = order; current_order &amp;lt; MAX_ORDER; ++current_order) { /*阶数从小到大*/
        area = &amp;amp;(zone-&amp;gt;free_area[current_order]);
        if (list_empty(&amp;amp;area-&amp;gt;free_list[migratetype])) /*判断是否符合分配要求*/
            continue;

        page = list_entry(area-&amp;gt;free_list[migratetype].next, struct page, lru); 
        list_del(&amp;amp;page-&amp;gt;lru); /*将符合要求的连续页从伙伴系统中移除*/
        rmv_page_order(page);
        area-&amp;gt;nr_free--;
        expand(zone, page, order, current_order, area, migratetype); /*将打散的空闲页添加到低阶链表中*/
        return page;
    }

    return NULL; /*如果找不到符合要求的空闲页，则返回空*/
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，内存分配中核心的伙伴算法分析完毕，后续博文我们将沿着内存回收线索深入往下分析。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/09/内存分配/&quot;&gt;内存管理之二：内存分配&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Sep 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/09/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/09/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>内存管理之一：地址映射</title>
        <description>&lt;p&gt;  地址映射是CPU核心和MMU共同完成的内存管理功能之一，本节将对此展开深入讨论。计算子系统相关内容目录&lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;点此进入&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;什么是地址映射为什么需要它&quot;&gt;什么是地址映射？为什么需要它？&lt;/h3&gt;

&lt;p&gt;  正如在&lt;a href=&quot;https://rootw.github.io/2017/02/计算机/&quot;&gt;计算机系统&lt;/a&gt;整体介绍中所说明的一样，MMU在CPU的配合下(通过页异常触发)，实现了线性地址到物理地址的动态映射，为正在CPU上运行的应用程序(进程)提供了一个独立的连续内存空间(线性地址空间，或称虚拟内存空间，其中放置了代码段、数据段和堆栈段)，屏蔽了地址分配、内存分配和内存回收等一系列复杂的系统行为，不仅提升了内存资源的利用效率，而且大大降低了应用开发难度，使程序猿可以更聚焦业务逻辑。结合CPU的进程管理功能，可以实现一个多任务并行系统，提升系统的可用性和性能。&lt;/p&gt;

&lt;h3 id=&quot;如何实现&quot;&gt;如何实现？&lt;/h3&gt;

&lt;h4 id=&quot;1线性地址&quot;&gt;&lt;strong&gt;1.线性地址&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  x86_64架构下Linux中每个应用程序(进程)可见的线性地址空间如下(注：分段机制在64位模式下已不产生实际作用)：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory1_1.jpg&quot; height=&quot;200&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  该架构支持48位线性地址(高16位仅做符号扩展，不参与地址转换)到40位物理地址(最多52位，由CPU实现决定)的映射。48位线性空间共256T，分为两个128T区间，分别分布在完整的64位空间的两端。其中，低128T为用户空间，映射用户程序代码、数据、堆栈和共享库，物理内存随着程序的运行由内核动态分配。而高128T则为内核空间：direct mapping区映射整个物理内存空间，便于内核访问所有物理内存；vmalloc space区间为内核调用vmalloc时使用的线性空间，物理内存动态分配且物理上不保证连续；virtual memory map是内核标识内存页信息的数组，供内存管理功能使用；kernel text &amp;amp; module区存放内核和模块的代码及数据。此外，也可以参考内核代码的说明：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/Documentation/x86/x86_64/mm.txt:

0000000000000000 - 00007fffffffffff (=47 bits) user space, different per mm
hole caused by [48:63] sign extension
ffff800000000000 - ffff80ffffffffff (=40 bits) guard hole
ffff880000000000 - ffffc7ffffffffff (=64 TB) direct mapping of all phys. memory
ffffc80000000000 - ffffc8ffffffffff (=40 bits) hole
ffffc90000000000 - ffffe8ffffffffff (=45 bits) vmalloc/ioremap space
ffffe90000000000 - ffffe9ffffffffff (=40 bits) hole
ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)
... unused hole ...
ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
... unused hole ...
ffffffff80000000 - ffffffffa0000000 (=512 MB)  kernel text mapping, from phys 0
ffffffffa0000000 - ffffffffff5fffff (=1525 MB) module mapping space
ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls
ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;2地址转换&quot;&gt;&lt;strong&gt;2.地址转换&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  MMU的线性地址转换是通过页表进行的，具体过程如下图所示(摘自intel程序员手册卷3)：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory1_3.jpg&quot; height=&quot;500&quot; width=&quot;750&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  其实最简单明了的方法是通过一个一维数组来记录映射关系:下标代表线性地址，数组元素内容代表物理地址。可是如此一来，用来表示映射关系的内存空间比被表示的物理空间还要大，显然这不是一个可行的方案。&lt;/p&gt;

&lt;p&gt;  工程师们采用了分段分级的思路来表示这种映射关系：先把线性空间以4K大小为单位进行划分(页)，然后再以大段连续空间进行转换，在每个大段空间内部再次划分成小段进行转换，直到段大小变为4K页大小。用以表示和段空间映射关系的结构称为页表，其大小也是一个页面。由于采用了分段的方法，页表空间大大减小；同时未映射的空间不必分配页表，这也进一步降低了页表占用空间。&lt;/p&gt;

&lt;p&gt;  x86_64架构下Linux用了四级页表来表示一个映射关系，依次为PGD、PUD、PMD、PT。每级页表4K大小，内部元素大小为8字节，高位指向了下一级页表的物理地址，低位表示页表属性(是否存在、读写权限、是否脏等等)。顶层页表PGD的物理地址存放在CPU的CR3寄存器中，供MMU访问。48位线性地址也相应地分成了五段：前四段，每段长9位，用来索引对应页表的元素；最后一段长12位，用来在页面中索引物理地址。各级页表的详细内容参考下表：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory1_4.jpg&quot; height=&quot;650&quot; width=&quot;800&quot; /&gt;  
&lt;/div&gt;

&lt;h4 id=&quot;3页异常处理&quot;&gt;&lt;strong&gt;3.页异常处理&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  一个进程初始运行时，对应的页表项大多都是空的。一旦MMU在地址转换过程中出现缺页或者读写权限问题时，MMU会触发页异常，打断CPU当前正在执行的程序，转而进行页异常处理(缺页会分配新页)。当页异常处理完毕后，CPU会重新执行引发缺页的指令，此时MMU便可正常完成地址转换。&lt;/p&gt;

&lt;p&gt;  下面，我们进一步深入分析整个页异常处理过程，关键流程如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/memory1_2.jpg&quot; height=&quot;600&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  CPU收到页异常后，首先进行的是上下文切换的硬件过程，该过程主要完成栈的切换(进入内核栈)、关键寄存器的保存和执行函数的切换(转入页异常处理函数page_fault)。有关中断和异常处理的详细分析请参考&lt;a href=&quot;https://rootw.github.io/2017/03/中断/&quot;&gt;中断分析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;  CPU被页异常中断后最先执行的是一段汇编代码(page_fault位于linux/arch/x86/kernel/entry_64.S，感兴趣的同学可以自行分析)，它完成了其他上下文寄存器的保存，并进入核心处理逻辑do_page_fault。&lt;/p&gt;

&lt;p&gt;  在理解MMU的工作原理之后，我想大家对缺页异常的核心处理逻辑应该很快能想明白，无非就是分配页、填充页内容、修改页表。然而，回顾一下前面线性地址章节描绘的地址空间分配图，我们会发现其中有代码、有数据、有堆和栈，不同类型的区段的对于页异常的处理逻辑是有区别的，例如代码段的页内容来自可执行文件，是只读类型的；数据段初始内容也来自可执行文件，但后续的修改不影响可执行执行文件；堆和栈的内容不来自任何文件，只在当前进程内部可见。因此，针对不同类型的内存区段需要有不同的处理方式。Linux内核以虚拟内存段(vma, virtual memory area)来表达不同程序区段，不同段可以具有不同的读写权限和属性；不在任何内存段的地址则认为是非法地址。有关内存段的数据结构如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/mm_types.h:

/*
 * This struct defines a memory VMM memory area. There is one of these
 * per VM-area/task.  A VM area is any part of the process virtual memory
 * space that has a special rule for the page-fault handlers (ie a shared
 * library, the executable area etc).
 */
struct vm_area_struct {
    /* The first cache line has the info for VMA tree walking. */

    unsigned long vm_start;		/* Our start address within vm_mm. */
    unsigned long vm_end;		/* The first byte after our end address within vm_mm. */

    /* linked list of VM areas per task, sorted by address */
    struct vm_area_struct *vm_next, *vm_prev;

    struct rb_node vm_rb;

    ...

    /* Second cache line starts here. */

    struct mm_struct *vm_mm;	/* The address space we belong to. */
    pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
    unsigned long vm_flags;		/* Flags, see mm.h. */

    ...

    /* Function pointers to deal with this struct. */
    const struct vm_operations_struct *vm_ops;

    /* Information about our backing store: */
    unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE units, *not* PAGE_CACHE_SIZE */
    struct file * vm_file;		/* File we map to (can be NULL). */
    void * vm_private_data;		/* was vm_pte (shared mem) */

    ...
};

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  所有vma会以链表形式统一到mm_struct中，该结构每个进程拥有一个，被进程控制块使用，描述了每个进程的有效内存区段和地址映射关系：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/include/mm_types.h:

struct mm_struct {
    struct vm_area_struct * mmap;		/* list of VMAs */
    struct rb_root mm_rb;
    struct vm_area_struct * mmap_cache;	/* last find_vma result */
#ifdef CONFIG_MMU
    unsigned long (*get_unmapped_area) (struct file *filp,
    unsigned long addr, unsigned long len,
    unsigned long pgoff, unsigned long flags);
    void (*unmap_area) (struct mm_struct *mm, unsigned long addr);
#endif
    unsigned long mmap_base;            /* base of mmap area */
    unsigned long mmap_legacy_base;     /* base of mmap area in bottom-up allocations */
    unsigned long task_size;            /* size of task vm space */
    unsigned long cached_hole_size; 	/* if non-zero, the largest hole below free_area_cache */
    unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
    unsigned long highest_vm_end;		/* highest vma end address */
    pgd_t * pgd;
    atomic_t mm_users;			/* How many users with user space? */
    atomic_t mm_count;			/* How many references to &quot;struct mm_struct&quot; (users count as 1) */
    int map_count;				/* number of VMAs */

    spinlock_t page_table_lock;		/* Protects page tables and some counters */
    struct rw_semaphore mmap_sem;

    struct list_head mmlist;        /* List of maybe swapped mm's.	These are globally strung
                                     * together off init_mm.mmlist, and are protected
                                     * by mmlist_lock
                                     */

    ...

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们回到do_page_fault函数，它通过读取CPU的CR2寄存器可以获知发生页异常的线性地址，并在当前进程对应的mm_struct中查找该线性地址对应的vma虚拟地址内存段，最后根据vma的属性来进一步处理页异常。当然，如果找不到线性地址对应的vma，内核就会认为发生了一次非法内存访问(让程序猿闻风丧胆的segfault由此产生)。代码片断分析如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/arch/x86/mm/fault.c:

/*
 * This routine handles page faults.  It determines the address,
 * and the problem, and then passes it off to one of the appropriate
 * routines.
 */
static void __kprobes
__do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
    struct vm_area_struct *vma;
    struct task_struct *tsk;
    unsigned long address;
    struct mm_struct *mm;
    int fault;
    unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;

    tsk = current;  /*获取当前进程*/
    mm = tsk-&amp;gt;mm;   /*当前进程对应的mm_struct*/

    /* Get the faulting address: */
    address = read_cr2();   /*x86架构下，页异常发生时，CR2寄存器中会记录发生异常的线性地址*/

    if (unlikely(fault_in_kernel_space(address))) {
        /*通过异常地址范围，判断异常地址是否在内核态。
          如果发生在内核态，通常是由于vmalloc导致的，这里会处理页表映射关系*/
        ...
        return;
    }

    ...

    /*如果执行到这里，说明页异常地址处在用户态范围*/
    /*如果代码段也在用户态，则打开中断，并记录标志；
      如果代码段在内核态，则根据页异常发生前的IF标记值来决定是否打开中断*/
    if (user_mode_vm(regs)) {
        local_irq_enable();
        error_code |= PF_USER;
        flags |= FAULT_FLAG_USER;
    } else {
        if (regs-&amp;gt;flags &amp;amp; X86_EFLAGS_IF)
            local_irq_enable();
    }

    ...

    /*
     * If we're in an interrupt, have no user context or are running
     * in an atomic region then we must not take the fault:
     */
    /*这里注意一点：页异常处理属于进程上下文，不是中断上下文，可睡眠*/
    if (unlikely(in_atomic() || !mm)) {
        bad_area_nosemaphore(regs, error_code, address);
        return;
    }

    if (error_code &amp;amp; PF_WRITE)
        flags |= FAULT_FLAG_WRITE;

    /*
     * When running in the kernel we expect faults to occur only to
     * addresses in user space.  All other faults represent errors in
     * the kernel and should generate an OOPS.  Unfortunately, in the
     * case of an erroneous fault occurring in a code path which already
     * holds mmap_sem we will deadlock attempting to validate the fault
     * against the address space.  Luckily the kernel only validly
     * references user space from well defined areas of code, which are
     * listed in the exceptions table.
     * ...
     */
    /*获取当前mm_struct的读信号量，避免后续处理过程中有其它流程修改mm_struct结构*/
    if (unlikely(!down_read_trylock(&amp;amp;mm-&amp;gt;mmap_sem))) {
        if ((error_code &amp;amp; PF_USER) == 0 &amp;amp;&amp;amp;
                !search_exception_tables(regs-&amp;gt;ip)) {
            bad_area_nosemaphore(regs, error_code, address);
            return;
        }
retry:
        down_read(&amp;amp;mm-&amp;gt;mmap_sem);
    } else {
    /*
     * The above down_read_trylock() might have succeeded in
     * which case we'll have missed the might_sleep() from
     * down_read():
     */
        might_sleep();
    }

    /*查找页异常地址对应的vma区段*/
    vma = find_vma(mm, address);
    if (unlikely(!vma)) {
        bad_area(regs, error_code, address);
        return;
    }
    /*如果页异常地址在合理的vma段地址范围内，则进行后续的异常处理*/
    if (likely(vma-&amp;gt;vm_start &amp;lt;= address))
        goto good_area;
    if (unlikely(!(vma-&amp;gt;vm_flags &amp;amp; VM_GROWSDOWN))) {
        /*如果页异常地址小于vma起始地址，但vma又不是往低地址方向增长(栈是往低地址方向增长的)，则出现错误*/
        bad_area(regs, error_code, address);
        return;
    }

    ...

    /*往低地址方向增长栈*/
    if (unlikely(expand_stack(vma, address))) {
        bad_area(regs, error_code, address);
        return;
    }

    /*
     * Ok, we have a good vm_area for this memory access, so
     * we can handle it..
     */
    /*如果执行到这里，后续便开始针对vma的属性进行不同的页异常处理*/
good_area:
    if (unlikely(access_error(error_code, vma))) {
        bad_area_access_error(regs, error_code, address);
        return;
    }

    /*
     * If for any reason at all we couldn't handle the fault,
     * make sure we exit gracefully rather than endlessly redo
     * the fault:
     */
    fault = handle_mm_fault(mm, vma, address, flags);

    ...

}


/*下面补充一些关于页异常错误码的内容，通常在发生段错误时我们在系统日志中可以看到错误码，
  通过错误码我们大致可以得知异常发生的原因*/
/*
 * Page fault error code bits:
 *
 *   bit 0 ==	 0: no page found	1: protection fault
 *   bit 1 ==	 0: read access		1: write access
 *   bit 2 ==	 0: kernel-mode access	1: user-mode access
 *   bit 3 ==				1: use of reserved bit detected
 *   bit 4 ==				1: fault was an instruction fetch
 */
enum x86_pf_error_code {
    PF_PROT     =       1 &amp;lt;&amp;lt; 0,
    PF_WRITE    =       1 &amp;lt;&amp;lt; 1,
    PF_USER     =       1 &amp;lt;&amp;lt; 2,
    PF_RSVD     =       1 &amp;lt;&amp;lt; 3,
    PF_INSTR    =       1 &amp;lt;&amp;lt; 4,
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  handle_mm_fault函数基于当前进程的mm_struct、异常地址所在vma和异常处理控制标志flags进行进一步异常处理。我们知道，页表共有四级，这里依次对各级页表进行处理：PGD是在进程创建的时候就分配好的，不需要动态分配；从PUD到PT，如果页表不存在会动态分配页并使页表指向新分配的页。页表处理完成后，进入handle_pte_fault处理最后的物理页。handle_mm_fault代码注解如下(这里暂不考虑大页等复杂特性)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/memory.c:

/*
 * By the time we get here, we already hold the mm semaphore
 */
static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
unsigned long address, unsigned int flags)
{
    pgd_t *pgd;
    pud_t *pud;
    pmd_t *pmd;
    pte_t *pte;

    ...

retry:
    pgd = pgd_offset(mm, address); /*直接获取当前进程中页异常线性地址在PGD表中对应的项，无须分配*/
    pud = pud_alloc(mm, pgd, address); /*获取PUD表中对应的项，如果PUD表不存在则动态分配*/
    if (!pud)
        return VM_FAULT_OOM;
    pmd = pmd_alloc(mm, pud, address); /*获取PMD表中对应的项，如果PMD不存在则动态分配*/
    if (!pmd)
        return VM_FAULT_OOM;

    ...
    if (unlikely(pmd_none(*pmd)) &amp;amp;&amp;amp;
        unlikely(__pte_alloc(mm, vma, pmd, address))) /*动态分配PTE*/
    return VM_FAULT_OOM;
    ...

    pte = pte_offset_map(pmd, address);

    /*各级页表分配完毕后，真正开始处理页异常*/
    return handle_pte_fault(mm, vma, address, pte, pmd, flags);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  题外话，页表操作的同步。通常情况下，一个确定的映射关系mm_struct只被一个进程引用，同一进程在确定时刻只会运行在一个核上，因此该进程的页异常处理也只在一个核上发生，此时不存在对进程页表做并发操作的可能。然而，如果一个进程有多个线程，那么这些线程将引用相同的mm_struct(代码段、数据段、堆空间完全相同，栈空间各不相同)，此时对mm_struct所涉及的各级页表操作时就需要考虑同步问题。&lt;/p&gt;

&lt;p&gt;  一种简单的方法是在开始页表操作前，对mm_struct先上一把大锁，待各级页表均操作完毕后再解锁。但存在的问题是页表操作过程中会涉及页分配，这是一个极其复杂的过程，可能还会睡眠，这样一来有可能出现成功加到锁的线程进入睡眠态后导致其他线程缺页却加不到锁的情况。即便不同线程访问的是不同内存段，但是却有可能出现因为一个线程的页异常处理不及时导致所有线程无法正常处理页异常的情况。&lt;/p&gt;

&lt;p&gt;  linux内核针对此种问题采用了最小化加锁范围的方法。每次操作页表前，如果页表项不存在则先分配页，然后加mm_struct锁。加锁成功后，如果发现页表项已经被赋值，说明有其他CPU先于当前CPU完成了页表分配，则释放先前分配页并解锁；如果未被赋值，则将分配页赋值给页表项，最后解锁。这种方法虽然会导致一些多余的页分配和释放动作，但加锁区间和持锁时间大大缩短，系统整体并发性大大提升。此外，对于最后一级PT页表的操作比前几级页表复杂性要高得多，因此内核对PT页表使用了一把独立的锁，进一步提升系统并行效率。代码注解如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
    pud_t *new = pud_alloc_one(mm, address); /*先偿试分配一页，该过程执行时间可能会比较长*/
    if (!new)
    return -ENOMEM;

    smp_wmb(); /* See comment in __pte_alloc */

    /*加锁判断原有页表项是否改变，如果发生改变说明有其它流程已成功分配页表，这里就释放之间分配的页*/
    spin_lock(&amp;amp;mm-&amp;gt;page_table_lock); 
    if (pgd_present(*pgd))		/* Another has populated it */
        pud_free(mm, new);
    else
        pgd_populate(mm, pgd, new);
    spin_unlock(&amp;amp;mm-&amp;gt;page_table_lock);
    return 0;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  我们再回到页异常处理主逻辑，接下来handle_pte_fault函数根据PT页表中异常地址对应的页表项进行不同处理：如果页表项PRESENT位未被置位，代表物理页不存在，需要进行缺页处理；否则，代表访问权限不够，需要调用do_wp_page进行写保护处理。在缺页的情况下，如果页表项不为零，说明前期把物理页交换到磁盘上了，而页表项纪录了交换页所在的磁盘位置信息，那么此时需要通过do_swap_page将交换页取回内存(内存交换将单独起一篇博文分析)；页表项为零则根据vma是否有文件对应进行不同处理，文件映射由do_linear_fault处理，匿名映射由do_anonymous_page处理。handle_pte_fault代码注解如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/memory.c:

int handle_pte_fault(struct mm_struct *mm,
            struct vm_area_struct *vma, unsigned long address,
            pte_t *pte, pmd_t *pmd, unsigned int flags)
{
    pte_t entry;
    spinlock_t *ptl;

    entry = *pte;
    if (!pte_present(entry)) { /*判断页是否不存在*/
        if (pte_none(entry)) { /*如果不仅不存在，而且页表项内容为零*/
            if (vma-&amp;gt;vm_ops) /*文件映射*/
                return do_linear_fault(mm, vma, address, pte, pmd, flags, entry);
            return do_anonymous_page(mm, vma, address, pte, pmd, flags); /*匿名映射*/
        }
        ...
        /*页不存在，但是非零，表示指向一个交换页，则执行换入操作*/
        return do_swap_page(mm, vma, address, pte, pmd, flags, entry);
    }

    ...

    /*处理防问权限异常，如写保护异常*/
    ptl = pte_lockptr(mm, pmd);
    spin_lock(ptl);
    if (unlikely(!pte_same(*pte, entry)))
        goto unlock;
    if (flags &amp;amp; FAULT_FLAG_WRITE) {
        if (!pte_write(entry))
            return do_wp_page(mm, vma, address, pte, pmd, ptl, entry);
        entry = pte_mkdirty(entry);
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  do_linear_fault函数处理线性文件映射内存段的缺页问题。对于读缺页，通过查找文件缓存页后直接采用缓存页作为映射页；对于写缺页，先查找文件缓存页，如果为共享内存段则直接采用缓存页映射，如果为私有内存段则分配新页、拷贝缓存页内容后再映射新页。代码注解如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;linux/mm/memory.c:

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
                        unsigned long address, pte_t *page_table, pmd_t *pmd,
                        unsigned int flags, pte_t orig_pte)
{
    pgoff_t pgoff = (((address &amp;amp; PAGE_MASK)
                    - vma-&amp;gt;vm_start) &amp;gt;&amp;gt; PAGE_SHIFT) + vma-&amp;gt;vm_pgoff; /*计算异常地址对应内容在文件中的偏移*/

    return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
}


static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
                    unsigned long address, pmd_t *pmd,
                    pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
{
    pte_t *page_table;
    spinlock_t *ptl;
    struct page *page;
    struct page *cow_page;
    pte_t entry;
    int anon = 0;
    struct page *dirty_page = NULL;
    struct vm_fault vmf;
    int ret;


    /*对于写操作，如果页异常地址所在vma段的属性是私有的，即没有设置VM_SHARED标记，
      则需要分配一个匿名页并复制文件中的内容*/
    if ((flags &amp;amp; FAULT_FLAG_WRITE) &amp;amp;&amp;amp; !(vma-&amp;gt;vm_flags &amp;amp; VM_SHARED)) {

        if (unlikely(anon_vma_prepare(vma)))
            return VM_FAULT_OOM;

        cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
        if (!cow_page)
            return VM_FAULT_OOM;

        ...
    } else
        cow_page = NULL;

    vmf.virtual_address = (void __user *)(address &amp;amp; PAGE_MASK);
    vmf.pgoff = pgoff;
    vmf.flags = flags;
    vmf.page = NULL;

    /*通过文件系统中的fault操作，在vmf.page中返回页异常地址对应文件内容的缓存页*/
    ret = vma-&amp;gt;vm_ops-&amp;gt;fault(vma, &amp;amp;vmf);
    ...

    page = vmf.page;
    if (flags &amp;amp; FAULT_FLAG_WRITE) {
        if (!(vma-&amp;gt;vm_flags &amp;amp; VM_SHARED)) {
            page = cow_page;
            anon = 1;
            copy_user_highpage(page, vmf.page, address, vma); /*复制缓存页中的内容到匿名页*/
            __SetPageUptodate(page);
        } else {
            ...
        }
    }

    page_table = pte_offset_map_lock(mm, pmd, address, &amp;amp;ptl);

    if (likely(pte_same(*page_table, orig_pte))) { /*通常都会进入该分支，是一种高效的页表访问方式*/
        entry = mk_pte(page, vma-&amp;gt;vm_page_prot); /*将页地址和基本属性填入页表项*/
        if (flags &amp;amp; FAULT_FLAG_WRITE)
            entry = maybe_mkwrite(pte_mkdirty(entry), vma); /*设置页表项写权限*/
        if (anon) { /*如果是匿名页，则添加反向匿名映射*/
            inc_mm_counter_fast(mm, MM_ANONPAGES);
            page_add_new_anon_rmap(page, vma, address);
        } else { /*如果是文件映射，则添加反向文件映射*/
            inc_mm_counter_fast(mm, MM_FILEPAGES);
            page_add_file_rmap(page);
            if (flags &amp;amp; FAULT_FLAG_WRITE) {
                dirty_page = page;
                get_page(dirty_page);
            }
        }
        set_pte_at(mm, address, page_table, entry); /*最终修改页表项内容*/

        ...
    } else {
        ...
    }

    pte_unmap_unlock(page_table, ptl);

    ...
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  do_anonymous_page函数处理匿名映射内存段的缺页问题。由于匿名映射没有对应文件，这里直接分配新页进行映射。代码注解如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
                        unsigned long address, pte_t *page_table, pmd_t *pmd,
                        unsigned int flags)
{
    struct page *page;
    spinlock_t *ptl;
    pte_t entry;

    ...

    /* Allocate our own private page. */
    if (unlikely(anon_vma_prepare(vma)))
        goto oom;
    page = alloc_zeroed_user_highpage_movable(vma, address);
    if (!page)
        goto oom;
    /*
     * The memory barrier inside __SetPageUptodate makes sure that
     * preceeding stores to the page contents become visible before
     * the set_pte_at() write.
     */
    __SetPageUptodate(page);

    entry = mk_pte(page, vma-&amp;gt;vm_page_prot);
    if (vma-&amp;gt;vm_flags &amp;amp; VM_WRITE)
        entry = pte_mkwrite(pte_mkdirty(entry));

    page_table = pte_offset_map_lock(mm, pmd, address, &amp;amp;ptl);
    ...

    inc_mm_counter_fast(mm, MM_ANONPAGES);
    page_add_new_anon_rmap(page, vma, address);
setpte:
    set_pte_at(mm, address, page_table, entry);
    ...

unlock:
    pte_unmap_unlock(page_table, ptl);
    return 0;
...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  do_wp_page函数处理写保护，即针对没有写权限的映射页触发了写请求。这里的处理思路和匿名页处理有些类似，也是分配新页后拷贝原有页的内容，之后解除原有页的映射之后再映射新页。&lt;/p&gt;

&lt;p&gt;  至此，MMU和CPU的内存地址映射功能已经整体分析完毕。CPU在页异常过程中多次涉及内存页分配，而内存分配又牵扯到内存回收和交换，这些都内存管理中不可缺少的部分，后续将对这些部分进行深入分析。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/08/地址映射/&quot;&gt;内存管理之一：地址映射&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/08/%E5%9C%B0%E5%9D%80%E6%98%A0%E5%B0%84/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/%E5%9C%B0%E5%9D%80%E6%98%A0%E5%B0%84/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>CPU中断处理</title>
        <description>&lt;p&gt;  从计算机系统内部看，中断无时无刻不在，这篇博文就和大家一起探讨中断的原理，并以x86_64平台上的linux 3.10内核为例来分析底层实现细节。&lt;/p&gt;

&lt;h3 id=&quot;什么是中断&quot;&gt;什么是中断？&lt;/h3&gt;

&lt;p&gt;  中断是一个系统过程，是计算系统中外部设备向CPU(或CPU之间)通知事件发生的一种机制。这种说法也许有些偏底层，可以从上层更直观的角度来理解：想象你正面对你的个人电脑，当你按下一个键盘按键时，你就触发了一个中断，随后屏幕上会出现你期望的字母；或者当你移动鼠标时，你也会触会一个中断，随后光标会随鼠标的移动而移动。严格意义上说，中断只是一个系统过程(系统调用过程如果抛开其核心处理函数的执行，也是一个系统过程)，是系统的一个部分，而不是一个完整系统，因为它不具备完整系统所应有的&lt;strong&gt;功能&lt;/strong&gt;、&lt;strong&gt;性能&lt;/strong&gt;、&lt;strong&gt;可靠性&lt;/strong&gt;、&lt;strong&gt;可扩展性&lt;/strong&gt;、&lt;strong&gt;安全性&lt;/strong&gt;、&lt;strong&gt;兼容性&lt;/strong&gt;、&lt;strong&gt;可维护性&lt;/strong&gt;等各方面的属性。比如，通过按动键盘，你以中断的方式向计算系统发送命令，但真正执行命令并返回结果的是计算机系统而不是按键动作本身。&lt;/p&gt;

&lt;h3 id=&quot;为什么需要中断&quot;&gt;为什么需要中断？&lt;/h3&gt;

&lt;p&gt;  计算机是个“死脑筋”，从打开电源键开始，就算你不向她发送任何指令，她也会按设定的程序开始忙禄，大多时候都在执行一个叫做IDEL的无聊程序。如何让她听命于你呢？两种方法，要么让她随时可被打断，去做你想让她做的事，随后再去干原来被打断的事；要么让她不停地一直问你想让她做什么，其它的事啥也不干，这样你一发话，她可以立刻响应你的命令。第一种方式，便是中断(interrupt)，第二种方式叫轮询(polling)。在大多数场景下，中断都是一种更为高效的(从完成任务数来看)通知方式，因为计算机干了更多有意义的事，而不是一直在“傻问”；轮询时，如果你想让计算机干得活不是很多，那么计算机大多数问询得到结果都是“谢谢，我不需要你做什么”，这就白白浪费了她的宝贵精力，但是每次你想让计算机做事时，她总是先主动地询问你，之后便立刻进入工作状态了，这比下达命令之后才慢吞吞开始行动的中断方式更为高效(从任务响应时间来看)。因此中断和轮询各有优点，各有各的适用场景：中断适用大多数设备通知场景，而在处理时延敏感场景下(如高性能网络转发)，轮询表现得更好。&lt;/p&gt;

&lt;h3 id=&quot;如何实现中断&quot;&gt;如何实现中断？&lt;/h3&gt;

&lt;p&gt;  下面我们深入系统内部，更细致地理解中断过程。前期的博文介绍过intel i440fx体系的基本组成，这里我们做些简化，只看和中断过程相关的几个部分，并按中断发生后的时序对中断过程作一个概括性的描述：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/images/posts/i440fx/interrupt.jpg&quot; height=&quot;400&quot; width=&quot;500&quot; /&gt;  
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;首先，我们可以看到在南桥芯片上集成了一个称为IOAPIC的部件，它共有24根中断引脚可以接收来自外部设备(如键盘、鼠标)的中断请求；当外设触发中断请求后，IOAPIC芯片会根据设备驱动初始化时设定的内容(一个内存地址Address和一个数据Data)向总线发送中断信息(将Data值写入Address地址代表的内存)，如图中绿色线条所示，直观地理解，&lt;strong&gt;这个Address指明了接收本次中断的具体CPU，而Data代表中断向量号(粗略地讲，可以认为这是不同中断相互区别的一个整数值，0~255)&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;其次，对于PCI设备而言，有两种中断触发方式：一种是通过intx中断引脚触发(最终通过IOAPIC发送中断信号，如图中虚线所示)；另一种是MSI/MSI-X方式(Message Signal Interrupt)，PCI设备通过驱动初始化时设定的内容直接向总线发送中断，如图中蓝线所示，其发送原理类似IOAPIC，由于外部设备中断过程并非本文讨论的重点，有兴趣的同学可以查阅PCI规范中相关内容来获得更深入的理解。&lt;/li&gt;
  &lt;li&gt;此外，CPU之间可以通过写ICR寄存器发送IPI(Inter Processors Interrupt)中断来进行核间通信，如图中粉色线条所示。&lt;/li&gt;
  &lt;li&gt;最后，每个CPU逻辑核都有一个称为LAPIC的子部件，它负责接收总线上的中断信息，当确认是发送给本地逻辑核时，便会引发本地CPU的中断过程：CPU会对保存当前正在执行任务的状态信息，之后会根据中断信息找到具体的中断处理逻辑函数；在完成中断处理函数后，CPU便会恢复先前保存的任务状态，继续处理原先的作务。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cpu如何处理中断&quot;&gt;CPU如何处理中断？&lt;/h3&gt;

&lt;p&gt;  介绍完系统内部整体的中断过程后，我们把焦点放到CPU上，更深入地从代码级别看看它是如何处理中断的。&lt;/p&gt;

&lt;h4 id=&quot;1硬件自动完成动作&quot;&gt;&lt;strong&gt;1､硬件自动完成动作&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  x86_64 CPU在中断发生时会执行一系列硬件动作，完成执行上下文的切换，这些都是硬件自动完成的，不受软件控制，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/i440fx/context_switch.jpg&quot; height=&quot;800&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;  图中上半部分表示中断发生前寄存器状态(这里以用户态上下文状态举例)：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CS(代码段寄存器)和rip(指令指针寄存器)指向了当前正在执行的用户态指令的位置(绿色线条所示)；&lt;/li&gt;
  &lt;li&gt;SS(堆栈段寄存器)和rsp(栈指针寄存器)指向了当前用户态堆栈的栈项位置；&lt;/li&gt;
  &lt;li&gt;rflags(标志寄存器)中的IF位为1，表示允许中断发生；&lt;/li&gt;
  &lt;li&gt;TR(Task Register)任务寄存器指向当前任务的任务状态段TSS(Task State Segment)，其中的rsp0域指向了内核态栈顶位置(内核特权级为0)；&lt;/li&gt;
  &lt;li&gt;IDTR(Interrupt Descriptor Table Register)指向了全局中断描述符表(Interrupt Descriptor Table)，表中共有256个中断描述符(Interrupt Descriptor)，每个描述符指向一个中断处理函数入口，中断描述符在表中的索引(下标)称为中断向量(Interrupt Vector)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  中断发生后(只能发生在指令边界，不能打断单条指令的执行)，寄存器状态将发生变化，如上图下半部分所示：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;对于运行在用户态的程序，中断发生后需要切换到内核态执行中断处理函数，出于安全的考虑，堆栈也需要切换到内核态(注意，每个进程在内核态都有一个独立的栈空间，3.10内核中有16K大小，栈项指针保存在TSS)；&lt;/li&gt;
  &lt;li&gt;切换到内核态栈后，CPU自动将用户态SS、rsp、rflags、CS、rip压入栈中(从上到下，栈顶在下，栈底在上)；&lt;/li&gt;
  &lt;li&gt;CPU根据中断向量，取出中断描述符表中对应的中断描述符，将CS:rip指向中断描述符中的函数入口地址；&lt;/li&gt;
  &lt;li&gt;对于类型为Interrupt Gate的中断描述符，rflags中的IF标置位将被清零，表示CPU此时开始不响应外部中断。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  细心的同学可能会问如果程序正好在执行系统调用进入内核态，那中断的硬件过程是怎样的？除了不用进行栈切换外，其它的过程和上面的一样，因为系统调用已经完成了栈切换的动作。&lt;/p&gt;

&lt;h4 id=&quot;2中断描述符表&quot;&gt;&lt;strong&gt;2､中断描述符表&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  硬件自动完成动作的最后是根据中断描述表中的内容找到中断处理函数入口，下面我们看看3.10内核里的中断描述符表的相关实现，其初始流程大致为start_kernel-&amp;gt;init_IRQ-&amp;gt;native_init_IRQ，其核心片断如下所示：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/irqinit.c:

void __init native_init_IRQ(void)
{
    int i;
    
    ...

    /*
     * Cover the whole vector space, no vector can escape
     * us. (some of these will be overridden and become
     * 'special' SMP interrupts)
     */
    i = FIRST_EXTERNAL_VECTOR;
    for_each_clear_bit_from(i, used_vectors, NR_VECTORS) {
        /* IA32_SYSCALL_VECTOR could be used in trap_init already. */
        set_intr_gate(i, interrupt[i - FIRST_EXTERNAL_VECTOR]);
    }

    ...

}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  FIRST_EXTERNAL_VECTOR为32，NR_VECTOR为256，开头的这段注释的意思是说这里会给从32到256的所有中断向量注册处理函数，从下面的代码看出处理函数在全局数组interrupt中。那么就有两个问题：为什么从32开始？为什么一开始就能把中断处理函数全部注册好，此时驱动程序都没初始化，具体的中断处理逻辑难道不是在驱动代码中实现的吗？第一个问题比较好回答，其实0~31的向量是intel预留给&lt;strong&gt;异常&lt;/strong&gt;使用的，这是CPU用来处理内部问题的一种方式，如除零、缺页等等。第二个问题目前确实比较难回答，我们就带着这个问题看看interrupt数组的定义吧：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

/*
 * Build the entry stubs and pointer table with some assembler magic.
 * We pack 7 stubs into a single 32-byte chunk, which will fit in a
 * single cache line on all modern x86 implementations.
 */
    .section .init.rodata,&quot;a&quot;
ENTRY(interrupt)
    .section .entry.text
    .p2align 5
    .p2align CONFIG_X86_L1_CACHE_SHIFT
ENTRY(irq_entries_start)
vector=FIRST_EXTERNAL_VECTOR
.rept (NR_VECTORS-FIRST_EXTERNAL_VECTOR+6)/7
    .balign 32
    .rept	7
    .if vector &amp;lt; NR_VECTORS
1:	pushq_cfi $(~vector+0x80)	/* Note: always in signed byte range */
            .if ((vector-FIRST_EXTERNAL_VECTOR)%7) &amp;lt;&amp;gt; 6
    jmp 2f
            .endif
    .previous
    .quad 1b
    .section .entry.text
    vector=vector+1
    .endif
    .endr
2:	jmp common_interrupt
.endr
END(irq_entries_start)

.previous
END(interrupt)
.previous
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这段汇编代码确实比较晦涩，它把32到256的中断按7个一组划成一个个大组，每个大组的内存占用空间大小在32个字节内，这样这些组块可以被CPU缓存到内部缓存中，以加速对这些内存的访问，显然这是一个性能优化手段。每个大组内包含了7个中断的桩(stub)函数和每个中断的处理函数入口地址，其内存结构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/i440fx/array.jpg&quot; height=&quot;500&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;  每个中断处理函数的入口地址以XXX_表示，桩函数包含两条指令，一条push指令和一条jmp指令。前6个中断桩函数的jmp指令都会跳转到最后一个桩函数的jmp指令位置，而该指令最终跳转到common_interrupt位置处继续执行。在每个桩函数的最后(组内的最后一个桩函数是在jmp指令前)放置了当前处理函数的入口地址，最终这些地址会组成全局interrupt数组。&lt;/p&gt;

&lt;h4 id=&quot;3公共入口函数common_interrupt&quot;&gt;&lt;strong&gt;3､公共入口函数common_interrupt&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  从上节的介绍中可以看出，中断发生后，CPU会执行中断描述符表所指向的各个中断的桩函数(如上图中XXX_32表示32号向量所对应的中断处理函数入口)，而所有桩函数在将中断向量压入栈后(会做符号化处理)，最终会跳转到common_interrupt函数，这个函数就成了所有中断的公共入口：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

/*
 * Interrupt entry/exit.
 *
 * Interrupt entry points save only callee clobbered registers in fast path.
 *
 * Entry runs with interrupts off.
 */

/* 0(%rsp): ~(interrupt number) */
.macro interrupt func
    /* reserve pt_regs for scratch regs and rbp */
    subq $ORIG_RAX-RBP, %rsp
    SAVE_ARGS_IRQ
    call \func
.endm

/*
 * Interrupt entry/exit should be protected against kprobes
 */
.pushsection .kprobes.text, &quot;ax&quot;
/*
 * The interrupt stubs push (~vector+0x80) onto the stack and
 * then jump to common_interrupt.
 */
.p2align CONFIG_X86_L1_CACHE_SHIFT
common_interrupt:
    addq $-0x80,(%rsp)		/* Adjust vector to [-256,-1] range */
    interrupt do_IRQ
    /* 0(%rsp): old_rsp-ARGOFFSET */
ret_from_intr:
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这里的interrupt代表一个宏，而不是之前讨论的interrupt全局数组。common_interrupt的工作过程就是将栈顶的向量号转化成负数(-256,-1)，然后通过SAVE_ARGS_IRQ宏保存必要的寄要器，最后调用C语言函数do_IRQ来处理中断。SAVE_ARGS_IRQ宏定义如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

/* save partial stack frame */
.macro SAVE_ARGS_IRQ
    cld
    /* start from rbp in pt_regs and jump over */
    movq_cfi rdi, (RDI-RBP)
    movq_cfi rsi, (RSI-RBP)
    movq_cfi rdx, (RDX-RBP)
    movq_cfi rcx, (RCX-RBP)
    movq_cfi rax, (RAX-RBP)
    movq_cfi  r8,  (R8-RBP)
    movq_cfi  r9,  (R9-RBP)
    movq_cfi r10, (R10-RBP)
    movq_cfi r11, (R11-RBP)

    /* Save rbp so that we can unwind from get_irq_regs() */
    movq_cfi rbp, 0

    /* Save previous stack value */
    movq %rsp, %rsi

    leaq -RBP(%rsp),%rdi	/* arg1 for handler */
    testl $3, CS-RBP(%rsi)
    je 1f
    SWAPGS
    /*
     * irq_count is used to check if a CPU is already on an interrupt stack
     * or not. While this is essentially redundant with preempt_count it is
     * a little cheaper to use a separate counter in the PDA (short of
     * moving irq_enter into assembly, which would be too much work)
     */
1:	incl PER_CPU_VAR(irq_count)
    cmovzq PER_CPU_VAR(irq_stack_ptr),%rsp

    /* Store previous stack value */
    pushq %rsi
    TRACE_IRQS_OFF
.endm
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/include/asm/calling.h:

/*
 * 64-bit system call stack frame layout defines and helpers,
 * for assembly code:
 */

#define R15		  0
#define R14		  8
#define R13		 16
#define R12		 24
#define RBP		 32
#define RBX		 40

/* arguments: interrupts/non tracing syscalls only save up to here: */
#define R11		 48
#define R10		 56
#define R9		 64
#define R8		 72
#define RAX		 80
#define RCX		 88
#define RDX		 96
#define RSI		104
#define RDI		112
#define ORIG_RAX	120       /* + error_code */
/* end of arguments */

/* cpu exception frame or undefined in case of fast syscall: */
#define RIP		128
#define CS		136
#define EFLAGS		144
#define RSP		152
#define SS		160
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  为什么只保存rdi~r11寄存器？这就涉及gcc编译方面的知识了，对于一个C函数来说，调用者如果在rdi~r11寄存器中保存了有用的信息，那调用者就需要在执行该C函数的调用前保存这些寄存器，因为C函数执行的过程中有可能会修改这些寄存器且不对这些寄存器做保存；而对于rbx,rbp,r12-r15这些寄存器，调用者如果在其中保存了有用的信息，在C函数调用返回后，这些寄存器的值不会发生改变，因为如果C函数内部会使用这些寄存器，它会保存旧的值并在函数返回前恢复这些寄存器旧有的值。&lt;/p&gt;

&lt;p&gt;  终于来到了C语言函数do_IRQ:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/irq.c:

/*
 * do_IRQ handles all normal device IRQ's (the special
 * SMP cross-CPU interrupts have their own specific
 * handlers).
 */
unsigned int __irq_entry do_IRQ(struct pt_regs *regs)
{
    struct pt_regs *old_regs = set_irq_regs(regs);

    /* high bit used in ret_from_ code  */
    unsigned vector = ~regs-&amp;gt;orig_ax;
    unsigned irq;

    irq_enter();
    exit_idle();

    irq = __this_cpu_read(vector_irq[vector]);

    if (!handle_irq(irq, regs)) {
        ...
    }

    irq_exit();

    set_irq_regs(old_regs);
    return 1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  上述函数的参数regs对应寄存器rdi(可以回顾下x86_64寄存器传参规则)，它是在SAVE_ARGS_IRQ宏中赋值的，指向了栈顶保存的r15寄存器。我理解此时栈顶有可能并没有保存r15寄存器的值，就看do_IRQ函数汇编后需不需要使用r15，但是其实do_IRQ只需要通过regs找到偏移为orig_ax的值(保存了向量号)就行，并不会去访问regs-&amp;gt;r15，所以并不影响程序的正确性。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;do_IRQ函数先保存了旧的栈帧结构指针，并在函数返回前恢复了旧的栈帧结构指针(目前还不是太理解在x86中的作用)；&lt;/li&gt;
  &lt;li&gt;通过regs中的orig_ax取出中断向量号，这里会将负数再次转成正数；&lt;/li&gt;
  &lt;li&gt;执行irq_enter表明正式进入中断上下文，如将当前进程的preempt_count计数增加；exit_idle表明CPU将退出空闲状态，这里均不作展开；&lt;/li&gt;
  &lt;li&gt;通过percpu变量将中断向量转换成irq号，并根据irq号处理中断；&lt;/li&gt;
  &lt;li&gt;执行riq_exit表明退出中断上下文，并恢复旧的栈帖结构指针；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  这里最让人困惑的是&lt;strong&gt;irq&lt;/strong&gt;号，它和中断向量是什么关系？转化关系为什么又是percpu类型的？为了回答这些问题，我们的思路暂时切出中断发生后的过程，来了解一些中断管理类的概念和初始化动作。&lt;/p&gt;

&lt;p&gt;  smp系统出现之前，系统中不同的外部中断完全可以用中断向量来区分，但在smp系统中，CPU核数增加导致中断处理也变得复杂，每个CPU都可以处理不同的中断，如果还用全局性的中断向量来区分中断，所能表示的中断数目太少。那是否可以给每个CPU都设立独立的中断描述符表？不行，这样会大大增加内核实现的复杂性，它采用了一种变通的方式：所有外部中断通过irq号来区分，&lt;strong&gt;不同的中断(即不同的irq)可以使用相同的中断向量，只要这些中断被分配到不同的核上&lt;/strong&gt;，例如在我的系统中查看中断信息得到如下结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/i440fx/example.jpg&quot; height=&quot;680&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;p&gt;  第一列中的数字即代表中断irq号，如0号irq代表ISA总线上的全局PIT时钟中断；通常来说0-15号irq对应传统ISA中断；16—39号开始分配给IOAPIC(i440fx中只有一个IOAPIC，占用24个irq)；再往后的irq分配给MSI/MSI-X(i440fx中从16+24=40号开始)。上图中我们看不到系统给每个中断分配的中断向量，假设系统初始化时给irq 0分配了0号核的32号向量，给irq 1分配了1号核的32号向量，那么0号核的percpu数组vector_irq的32号元素就指向irq 0，而1号核的percpu数组vecotr_irq的32号元素指向irq 1，如此一来，虽然0号核和1号核收到的中断向量都是32，但是do_IRQ可以通过percpu的vector_irq找到不同的irq，并通handle_irq执行真正的中断处理逻辑。这就是percpu的vectro_irq的神奇作用，也回答了前篇所提出的&lt;strong&gt;为什么在驱动初始化前就可以给所有中断向量注册处理函数：&lt;/strong&gt;中断描述符表中所指的函数只是一个伪入口(即桩函数)，而非实际的处理函数；实际的处理函数是在驱动初始化时，在为设备申请了irq号之后，通过request_irq(irq, function…)注册给不同的irq的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;这里还可以再思考一个问题：系统中最多可处理的中断是多少个？是256么？&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;  我们再切回中断的处理过程，在理解irq、中断向量、CPU核之间的关系后，可以看到handle_irq即是对每个中断进行实质性处理的核心函数，最终会调用request_irq函数注册的中断处理逻辑。下面我们就来分析一下handle_irq的实现逻辑。&lt;/p&gt;

&lt;h4 id=&quot;4中断处理逻辑handle_irq&quot;&gt;&lt;strong&gt;4､中断处理逻辑handle_irq&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  该函数整体逻辑比较简单：先将irq转换成&lt;code class=&quot;highlighter-rouge&quot;&gt;struct irq_desc&lt;/code&gt;结构，然后调用的generic_handle_irq_desc函数。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/irq_64.c:

bool handle_irq(unsigned irq, struct pt_regs *regs)
{
    struct irq_desc *desc;

    ...

    desc = irq_to_desc(irq);
    if (unlikely(!desc))
        return false;

    generic_handle_irq_desc(irq, desc);
    return true;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  irq_desc结构体将包含与中断相关的所有关键信息，内核中将所有中断的irq_desc结构组织成一棵树的结构：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include/linux/irqdesc.h:

/**
 * struct irq_desc - interrupt descriptor
 * @irq_data:		per irq and chip data passed down to chip functions
 * @kstat_irqs:		irq stats per cpu
 * @handle_irq:		highlevel irq-events handler
 * @preflow_handler:	handler called before the flow handler (currently used by sparc)
 * @action:		the irq action chain
 * @status:		status information
 * @core_internal_state__do_not_mess_with_it: core internal status information
 * @depth:		disable-depth, for nested irq_disable() calls
 * @wake_depth:		enable depth, for multiple irq_set_irq_wake() callers
 * @irq_count:		stats field to detect stalled irqs
 * @last_unhandled:	aging timer for unhandled count
 * @irqs_unhandled:	stats field for spurious unhandled interrupts
 * @threads_handled:	stats field for deferred spurious detection of threaded handlers
 * @threads_handled_last: comparator field for deferred spurious detection of theraded handlers
 * @lock:		locking for SMP
 * @affinity_hint:	hint to user space for preferred irq affinity
 * @affinity_notify:	context for notification of affinity changes
 * @pending_mask:	pending rebalanced interrupts
 * @threads_oneshot:	bitfield to handle shared oneshot threads
 * @threads_active:	number of irqaction threads currently running
 * @wait_for_threads:	wait queue for sync_irq to wait for threaded handlers
 * @dir:		/proc/irq/ procfs entry
 * @name:		flow handler name for /proc/interrupts output
 */
struct irq_desc {
    struct irq_data		irq_data;
    unsigned int __percpu	*kstat_irqs;
    irq_flow_handler_t	handle_irq;

    struct irqaction	*action;	/* IRQ action list */
    unsigned int		status_use_accessors;
    unsigned int		core_internal_state__do_not_mess_with_it;
    unsigned int		depth;		/* nested irq disables */
    unsigned int		wake_depth;	/* nested wake enables */
    unsigned int		irq_count;	/* For detecting broken IRQs */
    unsigned long		last_unhandled;	/* Aging timer for unhandled count */
    unsigned int		irqs_unhandled;
    atomic_t		threads_handled;
    int			threads_handled_last;
    raw_spinlock_t		lock;
    struct cpumask		*percpu_enabled;
#ifdef CONFIG_SMP
    const struct cpumask	*affinity_hint;
    struct irq_affinity_notify *affinity_notify;
#ifdef CONFIG_GENERIC_PENDING_IRQ
    cpumask_var_t		pending_mask;
#endif
#endif
    unsigned long		threads_oneshot;
    atomic_t		threads_active;
    wait_queue_head_t       wait_for_threads;
#ifdef CONFIG_PROC_FS
    struct proc_dir_entry	*dir;
#endif
    int			parent_irq;
    struct module		*owner;
    const char		*name;
} ____cacheline_internodealigned_in_smp;

...

/*
 * Architectures call this to let the generic IRQ layer
 * handle an interrupt. If the descriptor is attached to an
 * irqchip-style controller then we call the -&amp;gt;handle_irq() handler,
 * and it calls __do_IRQ() if it's attached to an irqtype-style controller.
 */
static inline void generic_handle_irq_desc(unsigned int irq, struct irq_desc *desc)
{
    desc-&amp;gt;handle_irq(irq, desc);
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  对于上述代码片断，我不再多作解释，大家可以对照代码注释仔细理解。这里的generic_handle_irq_desc函数通过内联的方式会调用每个中断对应的handle_irq函数。可能很多同学会把这里的handle_irq理解成就是用户(驱动程序)通过request_irq注册的中断处理函数。其实不然，这里的handle_irq仍然是一段通用的中断处理逻辑，用来实现对不同中断模式的处理和中断流控功能。这些通用的处理函数主要有三类：handle_level_irq、handle_edge_irq、handle_fasteoi_irq。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kernel/irq/chip.c:

/**
 *	handle_level_irq - Level type irq handler
 *	@irq:	the interrupt number
 *	@desc:	the interrupt description structure for this irq
 *
 *	Level type interrupts are active as long as the hardware line has
 *	the active level. This may require to mask the interrupt and unmask
 *	it after the associated handler has acknowledged the device, so the
 *	interrupt line is back to inactive.
 */
void
handle_level_irq(unsigned int irq, struct irq_desc *desc)
{
    ...
}

/**
 *	handle_fasteoi_irq - irq handler for transparent controllers
 *	@irq:	the interrupt number
 *	@desc:	the interrupt description structure for this irq
 *
 *	Only a single callback will be issued to the chip: an -&amp;gt;eoi()
 *	call when the interrupt has been serviced. This enables support
 *	for modern forms of interrupt handlers, which handle the flow
 *	details in hardware, transparently.
 */
void
handle_fasteoi_irq(unsigned int irq, struct irq_desc *desc)
{
    ...
}

/**
 *	handle_edge_irq - edge type IRQ handler
 *	@irq:	the interrupt number
 *	@desc:	the interrupt description structure for this irq
 *
 *	Interrupt occures on the falling and/or rising edge of a hardware
 *	signal. The occurrence is latched into the irq controller hardware
 *	and must be acked in order to be reenabled. After the ack another
 *	interrupt can happen on the same source even before the first one
 *	is handled by the associated event handler. If this happens it
 *	might be necessary to disable (mask) the interrupt depending on the
 *	controller hardware. This requires to reenable the interrupt inside
 *	of the loop which handles the interrupts which have arrived while
 *	the handler was running. If all pending interrupts are handled, the
 *	loop is left.
 */
void
handle_edge_irq(unsigned int irq, struct irq_desc *desc)
{
    ...
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这三类函数主要是针对不同物理电气特性的中断和中断控制器(如IO-APIC、支持MSI的PCI设备等)做不同的处理。有兴趣的同学可以结合intel IO-APIC说明和PCI规范来仔细理解里面的实现过程。&lt;/p&gt;

&lt;p&gt;  最后，这几类函数都会调用handle_irq_event，它会调用irq_desc中action的handler，这个函数指针，才是用户通过request_irq注册的中断处理函数。到这一步，才真正调用到实际的中断处理逻辑。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kernel/irq/handler.c:

irqreturn_t
handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
{
    irqreturn_t retval = IRQ_NONE;
    unsigned int flags = 0, irq = desc-&amp;gt;irq_data.irq;

    do {
        irqreturn_t res;

        res = action-&amp;gt;handler(irq, action-&amp;gt;dev_id);

        ...

        switch (res) {
        case IRQ_WAKE_THREAD:
            /*
             * Catch drivers which return WAKE_THREAD but
             * did not set up a thread function
             */
            if (unlikely(!action-&amp;gt;thread_fn)) {
                warn_no_thread(irq, action);
                break;
            }

            irq_wake_thread(desc, action);

            /* Fall through to add to randomness */
        case IRQ_HANDLED:
            flags |= action-&amp;gt;flags;
            break;

        default:
            break;
        }

        retval |= res;
        action = action-&amp;gt;next;
    } while (action);

    add_interrupt_randomness(irq, flags);

    if (!noirqdebug)
        note_interrupt(irq, desc, retval);
    return retval;
}

irqreturn_t handle_irq_event(struct irq_desc *desc)
{
    struct irqaction *action = desc-&amp;gt;action;
    irqreturn_t ret;

    desc-&amp;gt;istate &amp;amp;= ~IRQS_PENDING;
    irqd_set(&amp;amp;desc-&amp;gt;irq_data, IRQD_IRQ_INPROGRESS);
    raw_spin_unlock(&amp;amp;desc-&amp;gt;lock);

    ret = handle_irq_event_percpu(desc, action);

    raw_spin_lock(&amp;amp;desc-&amp;gt;lock);
    irqd_clear(&amp;amp;desc-&amp;gt;irq_data, IRQD_IRQ_INPROGRESS);
    return ret;
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;5中断返回ret_from_intr&quot;&gt;&lt;strong&gt;5､中断返回ret_from_intr&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;  当irq_desc的action处理完毕之后，中断处理过程将逐步返回到ret_from_intr：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

ret_from_intr:
    DISABLE_INTERRUPTS(CLBR_NONE)
    TRACE_IRQS_OFF
    decl PER_CPU_VAR(irq_count)

    /* Restore saved previous stack */
    popq %rsi
    leaq ARGOFFSET-RBP(%rsi), %rsp

exit_intr:
    GET_THREAD_INFO(%rcx)
    testl $3,CS-ARGOFFSET(%rsp)
    je retint_kernel

/* Interrupt came from user space */
/*
 * Has a correct top of stack, but a partial stack frame
 * %rcx: thread info. Interrupts off.
 */
retint_with_reschedule:
    movl $_TIF_WORK_MASK,%edi
retint_check:
    LOCKDEP_SYS_EXIT_IRQ
    movl TI_flags(%rcx),%edx
    andl %edi,%edx
    jnz  retint_careful

retint_swapgs:		/* return to user-space */
    /*
     * The iretq could re-enable interrupts:
     */
    DISABLE_INTERRUPTS(CLBR_ANY)
    TRACE_IRQS_IRETQ
    SWAPGS
    jmp restore_args

retint_restore_args:	/* return to kernel space */
    DISABLE_INTERRUPTS(CLBR_ANY)
    /*
     * The iretq could re-enable interrupts:
     */
    TRACE_IRQS_IRETQ
restore_args:
    RESTORE_ARGS 1,8,1

irq_return:
    INTERRUPT_RETURN
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  上述过程首先判断中断发生时是在用户态还是在内核态，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;如果是在内核态，就跳转到retint_kernel执行，这里会根据内核是否打开抢占进行不同的处理：如果内核不可抢占，那就恢复寄存器后返回到被中断的上下文继续执行；如果是可抢占的，那就可以进行调度。&lt;/li&gt;
  &lt;li&gt;如果是在用户态，就进行调度及信号相关的判断和处理；处理完成并恢复寄存器后，便通过iretq指令返回被中断的上下文继续执行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  至此，CPU上中断处理的整个系统过程完美结束，这真是一个漫长的旅途:-&amp;gt;&lt;/p&gt;

&lt;h3 id=&quot;中断处理如何优化软中断&quot;&gt;中断处理如何优化？－软中断&lt;/h3&gt;

&lt;p&gt;  通过前文的分析，我们看到中断的处理过程已经比较复杂了，即便如此，系统工程师们仍努力在思考如何改进中断的处理。一个显著的问题就是，如果CPU每次都是等整个中断处理逻辑执行完毕之后再开始响应下一个中断，那后续中断处理的实时性就会受影响，而且长时间处于中断上下文也会影响时钟和任务调度。于是，内核工程师想了一个办法：把中断的处理分成两部分：一部分是立刻要做的(通常是和硬件相关的部分)，并且只有等这部分做完了才能响应下一个中断，这部分通常处理时间很短，我们称这部分为上半部；另一部分是可以晚些时候处理(偏上层逻辑的部分)，并且在处理这部分工作的时候是可以响应一下个中断的，这部分通常处理时间较长，我们称之为下半部。软中断就是下半部的一种实现方式，它大大提升了中断处理的实时性。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/03/中断/&quot;&gt;CPU中断处理&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 18 Mar 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/03/%E4%B8%AD%E6%96%AD/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/%E4%B8%AD%E6%96%AD/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>系统调用</title>
        <description>&lt;p&gt;  计算子系统的诸多功能大多是通过系统调用来实现的，而且对于应用程序员来说，函数调用可能再熟悉不过了，但是对于系统调用这类特殊的函数调用，可能就局限在使用层面，而不会过多地去做深入研究。因此，这篇博文就从系统调用的角度来深入理解计算子系统。&lt;/p&gt;

&lt;h3 id=&quot;什么是系统调用&quot;&gt;什么是系统调用？&lt;/h3&gt;

&lt;p&gt;  在计算子系统中，当CPU执行进程时，系统调用是经常发生的一个过程。它是操作系统内核为应用程序提供的一组功能接口(API)，通过这组接口应用程序可以实现一系列全局性的系统功能，如创建新的进程(进程是系统全局性的资源，受内核统一调度和管理)、访问文件系统(文件系统也是系统全局性资源，可供多个应用程序共同使用)、访问网络接口设备(网卡是系统全局性资源，同样可被多个应用程序共享)。抛开内部实现功能的核心逻辑，系统调用是一个产生系统权限变化的&lt;strong&gt;特殊系统过程&lt;/strong&gt;。站在上层用户的视角来说，当你打开一个文件、点开一个新的应用窗口或者发送一个网络消息时都会涉及到系统调用。&lt;/p&gt;

&lt;h3 id=&quot;为什么需要系统调用&quot;&gt;为什么需要系统调用？&lt;/h3&gt;

&lt;p&gt;  前期的博文在介绍计算系统时说过，通用计算系统的优点在于可通过软件的部署实现功能的不断扩展。这里就引入一系列问题：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;运行在同一个计算系统上的不同应用程序有时需要实现相同的功能，是否需要各自都实现一套代码？&lt;/li&gt;
  &lt;li&gt;系统性的功能该如何实现？&lt;/li&gt;
  &lt;li&gt;如果某一个应用程序恶意破坏系统资源状态，该如何做防护？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  对于不同应用程序需要实现相同功能的问题，大家可能都会想到通过&lt;strong&gt;函数库&lt;/strong&gt;的方式对相同功能进行抽取和复用，但这里需要注意一点：不同应用程序即便使用相同的库函数，函数内部所使用进程级全局对象在不同进程间是相互隔离的，并不会相互影响。&lt;/p&gt;

&lt;p&gt;  那么对于系统级的全局资源的操作该如何实现？比如两个应用进程都想访问存储设备，如果只是通过函数库的方式实现了对存储设备的访问功能，那么两个应用进程就有可能破环彼此在存储设备上的数据，因为两个进程逻辑上是隔离的，都认为自己是以独占的方式在使用存储设备。正是为了实现对系统全局资源的统一访问和操作，系统工程师们创造一个被所有进程所共享的代码空间和数据空间(这就是被我们被为&lt;strong&gt;内核&lt;/strong&gt;的东西)。内核不仅代码空间被所有进程所共享，而且任意进程修改了数据空间中的数据后，其它进程都可以感知到它的修改。这样所有涉及系统全局资源的操作都可以放到内核中来实现，因此内核是一个涵盖进程、内存、磁盘、网卡等全局资源操作的复杂软件系统。&lt;/p&gt;

&lt;p&gt;  内核既然如此重要，而又被所有进程所共同改变，如果有恶意进程刻意破坏内核怎么办？硬件工程师给出了他们的解决方案：将CPU的执行空间划分为不同的&lt;strong&gt;等级&lt;/strong&gt;(比如x86中共分0到3,四个等级)，内核被放在最高的等级、应用程序独有的代码和数据被放在比较低的等级(如何linux在x86中将内核放在0级，将应用代码和数据放在3级)，高级别的代码可以访问低级别的代码和数据，而低级别的代码不允计访问高级别的代码和数据；同时提供若干特殊指令允许特权级切换到指定的代码位置执行已设定好的代码功能，这些代码功能就是系统调用，是内核为应用程序提供的安全访问系统功能的函数入口。&lt;/p&gt;

&lt;h3 id=&quot;如何实现系统调用&quot;&gt;如何实现系统调用？&lt;/h3&gt;

&lt;p&gt;  下面我们将深入系统内部，从寄存器和指令过程的层次来理解整体系统调用过程。&lt;/p&gt;

&lt;p&gt;  以文件访问为例，首先从应用层开始，为实现对文件的读取操作，一个C语言应用程序通常是这样的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main()
{
    int fd = -1;
    char buff[1024] = {0};
    
    fd = open(&quot;XXX&quot;, O_RDWR); //执行打开文件的系统调用
    ...
    read(fd, buff, 1024); //执行读取文件的系统调用
    ...
    
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  C语言应用程序中的系统调用最终会由glibc库实现，在glibc库中这些系统调用是用汇编语言完成的(原因是涉及特殊的特权级切换指令的调用)。当然，我们也可以直接通过汇编指令来实现系统调用：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main()
{
    int fd = -1;
    char buff[1024] = {0};

    //调用open系统调用，其系统调用号为2，第一个参数放在rdi寄存器中，代表打开的文件名
    //第二个参数放在rsi寄存器中，代表文件打开方式(这里以读写方式打开文件)

    asm(&quot;mov %2, %%rax;
         syscall;&quot;
        :&quot;=a&quot;(fd)
        :&quot;D&quot;(FILENAME), &quot;S&quot;(O_RDWR)
    );

    ...

    //调用read系统调用，其系统调用号为0，第一个参数rdi代表之前打开的文件句柄号
    //第二个参数rsi代表数据存储内存起始地址，第三个参数rdx代表读取的最大长度

    asm(&quot;mov %0, %%rax;
         syscall;&quot;
        :&quot;=a&quot;
        :&quot;D&quot;(fd), &quot;S&quot;(buff), &quot;d&quot;(1024)
    );
    ...

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  所有系统调用通过特权切换后都将跳转到相同的函数地址，因此为区别不同的系统调用功能，内核将所有的系统调用功能实现函数组成一个数组，并通过数组下标来索引具体的系统调用功能实现函数，这个下标就是系统调用号，如open系统调用的调用号为2，read系统调用的调用号为0。具体代码如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/syscalls/syscall_64.tbl:

#
# 64-bit system call numbers and entry vectors
#
# The format is:
# &amp;lt;number&amp;gt; &amp;lt;abi&amp;gt; &amp;lt;name&amp;gt; &amp;lt;entry point&amp;gt;
#
# The abi is &quot;common&quot;, &quot;64&quot; or &quot;x32&quot; for this file.
#
0	common	read			sys_read
1	common	write			sys_write
2	common	open			sys_open
3	common	close			sys_close
...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  有的读者可能对C语言内嵌汇编的语法不太熟悉(可参考有关内嵌AT&amp;amp;T汇编语法的学习资料)，这里再简要介绍下前面系统调用的指令过程：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;首先，将希望调用的系统调用功能的系统调用号放入rax寄存器中；&lt;/li&gt;
  &lt;li&gt;接着，通过给寄存器赋值来进行参数传递，最多可传递6个参数，依次为rdi、rsi、rdx、r10、r8、r9；&lt;/li&gt;
  &lt;li&gt;最后，执行syscall指令进行特权级切换和执行跳转；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  syscall指令是x86_64架构下引入的轻量级特权切换指令(相对于x86_32架构下的&lt;strong&gt;int 0x80&lt;/strong&gt;指令)，其主要功功能是：(1)将当前函数执行地址(rip寄存器的值)保存到rcx中；(2)将当前标志寄存器rflag的值保存到r11寄存器中；(3)通过修改rip跳转到MSR_LSTAR寄存器指向的内核函数入口；(4)根据MSR_SYSCALL_MASK寄存器修改rflag寄存器。可见相比x86_32架构，syscall指令执行的动作要少得多，因此它的执行速度更快。&lt;/p&gt;

&lt;p&gt;  至此我们终于要涉足内核了，那么系统调用入口函数是什么？答案就在内核启动过程中系统调用的初始化流程中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/cpu/common.c:

void syscall_init(void)
{
    /*
     * LSTAR and STAR live in a bit strange symbiosis.
     * They both write to the same internal register. STAR allows to
     * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.
     */
    wrmsrl(MSR_STAR,  ((u64)__USER32_CS)&amp;lt;&amp;lt;48  | ((u64)__KERNEL_CS)&amp;lt;&amp;lt;32);
    wrmsrl(MSR_LSTAR, system_call);
    wrmsrl(MSR_CSTAR, ignore_sysret);
    
    ...

    /* Flags to clear on syscall */
    wrmsrl(MSR_SYSCALL_MASK,
        X86_EFLAGS_TF|X86_EFLAGS_DF|X86_EFLAGS_IF|
        X86_EFLAGS_IOPL|X86_EFLAGS_AC|X86_EFLAGS_NT);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  这段代码说明在linux-3.10系统中，执行syscall指令后，当前进程会切换到内核态并将开始执行system_call函数；同时rflag标志寄存器的中断标志位(X86_EFLAGS_IF)会清零，这使得当前CPU不会响应普通中断，即不会被普通中断打断执行逻辑(但会被不可屏蔽中断NMI打断)。下面我们就来看system_call函数的实现，该函数在linux-3.10/arch/x86/kernel/entry_64.S中，这是一段底层代码，因此是用汇编语言编写的，在正式分析函数功能前，建议大家先看看函数的注释：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

/*
 * System call entry. Up to 6 arguments in registers are supported.
 *
 * SYSCALL does not save anything on the stack and does not change the
 * stack pointer.  However, it does mask the flags register for us, so
 * CLD and CLAC are not needed.
 */

/*
 * Register setup:
 * rax  system call number
 * rdi  arg0
 * rcx  return address for syscall/sysret, C arg3
 * rsi  arg1
 * rdx  arg2
 * r10  arg3 	(--&amp;gt; moved to rcx for C)
 * r8   arg4
 * r9   arg5
 * r11  eflags for syscall/sysret, temporary for C
 * r12-r15,rbp,rbx saved by C code, not touched.
 *
 * Interrupts are off on entry.
 * Only called from user space.
 *
 * XXX	if we had a free scratch register we could save the RSP into the stack frame
 *      and report it properly in ps. Unfortunately we haven't.
 *
 * When user can change the frames always force IRET. That is because
 * it deals with uncanonical addresses better. SYSRET has trouble
 * with them due to bugs in both AMD and Intel CPUs.
 */

ENTRY(system_call)
    CFI_STARTPROC	simple
    CFI_SIGNAL_FRAME
    CFI_DEF_CFA	rsp,KERNEL_STACK_OFFSET
    CFI_REGISTER	rip,rcx
    /*CFI_REGISTER	rflags,r11*/
    SWAPGS_UNSAFE_STACK
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  从这里的注释中我们可以看出如下要点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;syscall指令不会在栈在保存中作何信息，也不会切换栈指针，即修改rsp寄存器；因此，熟悉x86_32架构下int 0x80系统调用原理的同学注意了，这里是不同的；&lt;/li&gt;
  &lt;li&gt;系统调用最多传递6个参数，依次放在rdi、rsi、rdx、r10、r8、r9寄存器中；这里有一个扩展的知识点，&lt;strong&gt;在x86_64下普通C语言函数也可以通过寄存器传参数的，前6个参数的顺序是rdi、rsi、rdx、rcx、r8、r9寄存器&lt;/strong&gt;，好奇的读者可能会问那系统调用的传参为何不跟普通C函数保持一致呢？回顾一下syscall指令的执行过程，大家可能就会发现此时rcx已经存放了系统调用结束后的返回地址，因此不能用来传参了；&lt;/li&gt;
  &lt;li&gt;进入system_call函数的时候，CPU是不响应外部中断的；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  system_call函数开始的几个CFI开头的宏和函数追踪相关，通过展开后是空的，因此不作关心。SWAPGS_UNSAFE_STACK用来切换gs寄存器，我们继续往下分析：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

GLOBAL(system_call_after_swapgs)

    movq	%rsp,PER_CPU_VAR(old_rsp)
    movq	PER_CPU_VAR(kernel_stack),%rsp
/*
 * No need to follow this irqs off/on section - it's straight
 * and short:
 */
    ENABLE_INTERRUPTS(CLBR_NONE)
    SAVE_ARGS 8,0
    movq  %rax,ORIG_RAX-ARGOFFSET(%rsp)
    movq  %rcx,RIP-ARGOFFSET(%rsp)
    CFI_REL_OFFSET rip,RIP-ARGOFFSET
    testl $_TIF_WORK_SYSCALL_ENTRY,TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET)
    jnz tracesys
system_call_fastpath:
#if __SYSCALL_MASK == ~0
    cmpq $__NR_syscall_max,%rax
#else
    andl $__SYSCALL_MASK,%eax
    cmpl $__NR_syscall_max,%eax
#endif
    ja badsys
    movq %r10,%rcx
    call *sys_call_table(,%rax,8)  # XXX:	 rip relative
    movq %rax,RAX-ARGOFFSET(%rsp)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  GLOBAL定义了一个全局函数system_call_after_swapgs，随后两条mov指令是在关中断的前提下执行的，因此不会被打断，它们的指令过程是：先将当前rsp寄存器(指向用户态栈空间)保存到内核中per cpu变量old_rsp中(即每个CPU访问不同的old_rsp变量)，接着将当前CPU的kernel_stack值(指向当前进程内核栈且预留了ss、rsp、rflags、cs、rip的40个字节的空间)赋给rsp寄存器，即完成了栈的切换。将栈指针切换到内核态之后，即完成了基本执行环境的准备，随后通过ENABLE_INTERRUPT宏(本质为执行sti指令)打开当前CPU的中断，后续的执行过程中CPU又可以响应外部中断了。接着SAVE_ARGS宏开始保存rdi到r11寄存器的值到栈中，其实现在linux-3.10/arch/x86/include/asm/calling.h中：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/include/asm/calling.h:

#define R15		  0
#define R14		  8
#define R13		 16
#define R12		 24
#define RBP		 32
#define RBX		 40

/* arguments: interrupts/non tracing syscalls only save up to here: */
#define R11		 48
#define R10		 56
#define R9		 64
#define R8		 72
#define RAX		 80
#define RCX		 88
#define RDX		 96
#define RSI		104
#define RDI		112
#define ORIG_RAX	120       /* + error_code */
/* end of arguments */

/* cpu exception frame or undefined in case of fast syscall: */
#define RIP		128
#define CS		136
#define EFLAGS		144
#define RSP		152
#define SS		160

#define ARGOFFSET	R11
#define SWFRAME		ORIG_RAX

.macro SAVE_ARGS addskip=0, save_rcx=1, save_r891011=1
    subq  $9*8+\addskip, %rsp
    CFI_ADJUST_CFA_OFFSET	9*8+\addskip
    movq_cfi rdi, 8*8
    movq_cfi rsi, 7*8
    movq_cfi rdx, 6*8

.if \save_rcx
    movq_cfi rcx, 5*8
.endif

    movq_cfi rax, 4*8

.if \save_r891011
    movq_cfi r8,  3*8
    movq_cfi r9,  2*8
    movq_cfi r10, 1*8
    movq_cfi r11, 0*8
.endif

.endm
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  回到system_call主逻辑，保存完rdi到r11的寄存器和rax寄存器后，紧随的mov指令把rcx(前面讲过多次，此时rcx保存的是用户态返回地址)也保存到栈中。随后的一段逻辑用来判断是否需要对系统调用进行追踪及rax中的系统调用号是否超过设定的最大值__NR_syscall_max，如果无须追踪且系统调用号没有超过最大值，则通过call指令调用sys_call_table数组中由系统调用号索引的具体处理函数(如系统调用号为0，则调用sys_read函数)。当然，在调用sys_函数前需要将系统调用的第4个参数从r10中复制到rcx中，以确保sys_函数能获取正确的参数。当sys_函数调用完成后，rax将保存其返回值，这里也会将返回值压入栈中。到这步系统调用核心功能已经完成，但是在返回到用户空间前，内核还会做一些常规性的事务，如检查信号、检查当前进程是否需要被调度等，最后才是从栈中恢复之前保存的诸多寄存器、切换栈指针到用户态、通过sysret指令返回到用户空间(syscall的下一条指令)继续执行。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arch/x86/kernel/entry_64.S:

/*
* Syscall return path ending with SYSRET (fast path)
* Has incomplete stack frame and undefined top of stack.
*/
ret_from_sys_call:
    movl $_TIF_ALLWORK_MASK,%edi
    /* edi:	flagmask */
sysret_check:
    LOCKDEP_SYS_EXIT
    DISABLE_INTERRUPTS(CLBR_NONE)
    TRACE_IRQS_OFF
    movl TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET),%edx
    andl %edi,%edx
    jnz  sysret_careful
    CFI_REMEMBER_STATE
/*
 * sysretq will re-enable interrupts:
 */
    TRACE_IRQS_ON
    movq RIP-ARGOFFSET(%rsp),%rcx
    CFI_REGISTER	rip,rcx
    RESTORE_ARGS 1,-ARG_SKIP,0
    /*CFI_REGISTER	rflags,r11*/
    movq	PER_CPU_VAR(old_rsp), %rsp
    USERGS_SYSRET64
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;  至此，linux-3.10内核在x86_64上完整系统调用过程中已分析完毕，过程中分析的部分内容涉及C函数编译和汇编，建议结合网上现有的一些资料来加深理解。enjoy hacking the system~&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/02/系统调用/&quot;&gt;系统调用&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/02/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/02/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
      <item>
        <title>计算子系统-开篇与目录</title>
        <description>&lt;p&gt;  计算子系统是计算机内部最为复杂的系统，如&lt;a href=&quot;https://rootw.github.io/2017/02/计算机/&quot;&gt;计算机&lt;/a&gt;所述，它的系统过程包含应用和内核两部分，其中内核又包含了内存管理(地址转换、空间分配、空间回收等)、进程执行(完成数据计算、外设控制等核心功能，以及系统调用切换等系统过程)、进程管理(创建、替换、终止、调度、通信)、中断处理、内核同步等一系列功能。因此我们需要从不同的功能视角来观察和理解它们，才能形成更加全面的认识。因此后续博文将从内存管理、进程管理、系统调用、中断处理等多个方面来分析该系统，建议的阅读目录顺序如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/08/地址映射/&quot;&gt;内存管理之一：地址映射&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/09/内存分配/&quot;&gt;内存管理之二：内存分配&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/10/内存回收/&quot;&gt;内存管理之三：内存回收&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/11/内存交换/&quot;&gt;内存管理之四：内存交换&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/12/内存压缩/&quot;&gt;内存管理之五：内存压缩&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/12/内存初始化/&quot;&gt;内存管理之六：内存初始化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2018/01/进程创建/&quot;&gt;进程管理之一：进程创建&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/02/系统调用/&quot;&gt;系统调用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootw.github.io/2017/03/中断/&quot;&gt;中断处理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  针对功能的深入分析，其实也是对计算子系统物理工作过程的深入理解。功能分析时，我们可以借助高级语言来表达分析过程(这里我们主要以C语言并结合linux 3.10内核来理解核心子系统的实现)。然而，当分析深入到一定程度时，高级语言可能无法细致地表达某些系统过程，此时我们就需要通过汇编语言(AT&amp;amp;T语法)来描述系统内部的寄存器及其操作过程。本质上来说，高级语言所描述的功能最终也是转化成汇编来实现的，因此掌握汇编语言是深入理解系统一个必不可少的技能。这里我们先对CPU内部寄存器及内存布局设计做一个总体描绘，它们的内部结构如下图所示：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/posts/i440fx/cpu_low_level.jpg&quot; height=&quot;550&quot; width=&quot;400&quot; /&gt;  
&lt;/div&gt;

&lt;p&gt;  对于AT&amp;amp;T汇编语法和intel x86_64架构原理，网上应该有不少资源供大家学习，这里暂不作全面介绍，后续分析时如果涉及将会补充一些说明。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
转载请注明：&lt;a href=&quot;https://rootw.github.io&quot;&gt;吴斌的博客&lt;/a&gt; » &lt;a href=&quot;https://rootw.github.io/2017/02/计算子系统/&quot;&gt;计算子系统-开篇与目录&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 11 Feb 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/02/%E8%AE%A1%E7%AE%97%E5%AD%90%E7%B3%BB%E7%BB%9F/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/02/%E8%AE%A1%E7%AE%97%E5%AD%90%E7%B3%BB%E7%BB%9F/</guid>
        
        <category>自顶向下分析计算机系统</category>
        
        
      </item>
    
  </channel>
</rss>
